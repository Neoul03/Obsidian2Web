<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[NetSys_Obsidian]]></title><description><![CDATA[Obsidian digital garden]]></description><link>http://github.com/dylang/node-rss</link><image><url>lib/media/favicon.png</url><title>NetSys_Obsidian</title><link></link></image><generator>Webpage HTML Export plugin for Obsidian</generator><lastBuildDate>Thu, 12 Dec 2024 10:27:44 GMT</lastBuildDate><atom:link href="lib/rss.xml" rel="self" type="application/rss+xml"/><pubDate>Thu, 12 Dec 2024 10:26:46 GMT</pubDate><ttl>60</ttl><dc:creator></dc:creator><item><title><![CDATA[__devres_alloc_node()]]></title><description><![CDATA[ 
 <br>/**
 * __devres_alloc_node - Allocate device resource data
 * @release: Release function devres will be associated with
 * @size: Allocation size
 * @gfp: Allocation flags
 * @nid: NUMA node
 * @name: Name of the resource
 *
 * Allocate devres of @size bytes.  The allocated area is zeroed, then
 * associated with @release.  The returned pointer can be passed to
 * other devres_*() functions.
 *
 * RETURNS:
 * Pointer to allocated devres on success, NULL on failure.
 */
void *__devres_alloc_node(dr_release_t release, size_t size, gfp_t gfp, int nid,
			  const char *name)
{
	struct devres *dr;

	dr = alloc_dr(release, size, gfp | __GFP_ZERO, nid); //[[alloc_dr()]]
	if (unlikely(!dr))
		return NULL;
	set_node_dbginfo(&amp;dr-&gt;node, name, size);
	return dr-&gt;data;
}
EXPORT_SYMBOL_GPL(__devres_alloc_node);
<br><a data-href="alloc_dr()" href="encyclopedia-of-networksystem/function/drivers-base/alloc_dr().html" class="internal-link" target="_self" rel="noopener nofollow">alloc_dr()</a>]]></description><link>encyclopedia-of-networksystem/function/drivers-base/__devres_alloc_node().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-base/__devres_alloc_node().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[alloc_dr()]]></title><description><![CDATA[ 
 <br>static __always_inline struct devres * alloc_dr(dr_release_t release,
						size_t size, gfp_t gfp, int nid)
{
	size_t tot_size;
	struct devres *dr;

	if (!check_dr_size(size, &amp;tot_size))
		return NULL;

	dr = kmalloc_node_track_caller(tot_size, gfp, nid); // [[kmalloc_node_track_caller()]]
	if (unlikely(!dr))
		return NULL;

	/* No need to clear memory twice */
	if (!(gfp &amp; __GFP_ZERO))
		memset(dr, 0, offsetof(struct devres, data));

	INIT_LIST_HEAD(&amp;dr-&gt;node.entry);
	dr-&gt;node.release = release;
	return dr;
}
<br><a data-href="kmalloc_node_track_caller()" href="encyclopedia-of-networksystem/function/include-linux/kmalloc_node_track_caller().html" class="internal-link" target="_self" rel="noopener nofollow">kmalloc_node_track_caller()</a>]]></description><link>encyclopedia-of-networksystem/function/drivers-base/alloc_dr().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-base/alloc_dr().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[devm_kmalloc()]]></title><description><![CDATA[ 
 <br>/**
 * devm_kmalloc - Resource-managed kmalloc
 * @dev: Device to allocate memory for
 * @size: Allocation size
 * @gfp: Allocation gfp flags
 *
 * Managed kmalloc.  Memory allocated with this function is
 * automatically freed on driver detach.  Like all other devres
 * resources, guaranteed alignment is unsigned long long.
 *
 * RETURNS:
 * Pointer to allocated memory on success, NULL on failure.
 */
void *devm_kmalloc(struct device *dev, size_t size, gfp_t gfp)
{
	struct devres *dr;

	if (unlikely(!size))
		return ZERO_SIZE_PTR;

	/* use raw alloc_dr for kmalloc caller tracing */
	dr = alloc_dr(devm_kmalloc_release, size, gfp, dev_to_node(dev)); // [[alloc_dr()]]
	if (unlikely(!dr))
		return NULL;

	/*
	 * This is named devm_kzalloc_release for historical reasons
	 * The initial implementation did not support kmalloc, only kzalloc
	 */
	set_node_dbginfo(&amp;dr-&gt;node, "devm_kzalloc_release", size);
	devres_add(dev, dr-&gt;data);
	return dr-&gt;data;
}
EXPORT_SYMBOL_GPL(devm_kmalloc);
<br><a data-href="alloc_dr()" href="encyclopedia-of-networksystem/function/drivers-base/alloc_dr().html" class="internal-link" target="_self" rel="noopener nofollow">alloc_dr()</a>]]></description><link>encyclopedia-of-networksystem/function/drivers-base/devm_kmalloc().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-base/devm_kmalloc().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[__ice_q_vector_set_napi_queues()]]></title><description><![CDATA[ 
 <br>/**
 * __ice_q_vector_set_napi_queues - Map queue[s] associated with the napi
 * @q_vector: q_vector pointer
 * @locked: is the rtnl_lock already held
 *
 * Associate the q_vector napi with all the queue[s] on the vector.
 * Caller indicates the lock status.
 */
void __ice_q_vector_set_napi_queues(struct ice_q_vector *q_vector, bool locked)
{
	struct ice_rx_ring *rx_ring;
	struct ice_tx_ring *tx_ring;

	ice_for_each_rx_ring(rx_ring, q_vector-&gt;rx)
		__ice_queue_set_napi(q_vector-&gt;vsi-&gt;netdev, rx_ring-&gt;q_index,
				     NETDEV_QUEUE_TYPE_RX, &amp;q_vector-&gt;napi,
				     locked);

	ice_for_each_tx_ring(tx_ring, q_vector-&gt;tx)
		__ice_queue_set_napi(q_vector-&gt;vsi-&gt;netdev, tx_ring-&gt;q_index,
				     NETDEV_QUEUE_TYPE_TX, &amp;q_vector-&gt;napi,
				     locked);
	/* Also set the interrupt number for the NAPI */
	netif_napi_set_irq(&amp;q_vector-&gt;napi, q_vector-&gt;irq.virq);
}
<br>
napi와 연관된 큐들을 매핑하는 함수. 여기서 net_device에 멤버로 있는 netdev_rx_queue와 netdev_queue에다가 해당 napi를 할당하게 된다. 그런데 이것은 기존의 ice_rx_ring과는 또 다른 Rx_queue이다.
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/__ice_q_vector_set_napi_queues().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/__ice_q_vector_set_napi_queues().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[__ice_xmit_xdp_ring()]]></title><description><![CDATA[ 
 <br>/**
 * __ice_xmit_xdp_ring - submit frame to XDP ring for transmission
 * @xdp: XDP buffer to be placed onto Tx descriptors
 * @xdp_ring: XDP ring for transmission
 * @frame: whether this comes from .ndo_xdp_xmit()
 */
int __ice_xmit_xdp_ring(struct xdp_buff *xdp, struct ice_tx_ring *xdp_ring,
			bool frame)
{
	struct skb_shared_info *sinfo = NULL;
	u32 size = xdp-&gt;data_end - xdp-&gt;data;
	struct device *dev = xdp_ring-&gt;dev;
	u32 ntu = xdp_ring-&gt;next_to_use;
	struct ice_tx_desc *tx_desc;
	struct ice_tx_buf *tx_head;
	struct ice_tx_buf *tx_buf;
	u32 cnt = xdp_ring-&gt;count;
	void *data = xdp-&gt;data;
	u32 nr_frags = 0;
	u32 free_space;
	u32 frag = 0;

	free_space = ICE_DESC_UNUSED(xdp_ring);
	if (free_space &lt; ICE_RING_QUARTER(xdp_ring))
		free_space += ice_clean_xdp_irq(xdp_ring);

	if (unlikely(!free_space))
		goto busy;

	if (unlikely(xdp_buff_has_frags(xdp))) {
		sinfo = xdp_get_shared_info_from_buff(xdp);
		nr_frags = sinfo-&gt;nr_frags;
		if (free_space &lt; nr_frags + 1)
			goto busy;
	}

	tx_desc = ICE_TX_DESC(xdp_ring, ntu);
	tx_head = &amp;xdp_ring-&gt;tx_buf[ntu];
	tx_buf = tx_head;

	for (;;) {
		dma_addr_t dma;

		dma = dma_map_single(dev, data, size, DMA_TO_DEVICE);
		if (dma_mapping_error(dev, dma))
			goto dma_unmap;

		/* record length, and DMA address */
		dma_unmap_len_set(tx_buf, len, size);
		dma_unmap_addr_set(tx_buf, dma, dma);

		if (frame) {
			tx_buf-&gt;type = ICE_TX_BUF_FRAG;
		} else {
			tx_buf-&gt;type = ICE_TX_BUF_XDP_TX;
			tx_buf-&gt;raw_buf = data;
		}

		tx_desc-&gt;buf_addr = cpu_to_le64(dma);
		tx_desc-&gt;cmd_type_offset_bsz = ice_build_ctob(0, 0, size, 0);

		ntu++;
		if (ntu == cnt)
			ntu = 0;

		if (frag == nr_frags)
			break;

		tx_desc = ICE_TX_DESC(xdp_ring, ntu);
		tx_buf = &amp;xdp_ring-&gt;tx_buf[ntu];

		data = skb_frag_address(&amp;sinfo-&gt;frags[frag]);
		size = skb_frag_size(&amp;sinfo-&gt;frags[frag]);
		frag++;
	}

	/* store info about bytecount and frag count in first desc */
	tx_head-&gt;bytecount = xdp_get_buff_len(xdp);
	tx_head-&gt;nr_frags = nr_frags;

	if (frame) {
		tx_head-&gt;type = ICE_TX_BUF_XDP_XMIT;
		tx_head-&gt;xdpf = xdp-&gt;data_hard_start;
	}

	/* update last descriptor from a frame with EOP */
	tx_desc-&gt;cmd_type_offset_bsz |=
		cpu_to_le64(ICE_TX_DESC_CMD_EOP &lt;&lt; ICE_TXD_QW1_CMD_S);

	xdp_ring-&gt;xdp_tx_active++;
	xdp_ring-&gt;next_to_use = ntu;

	return ICE_XDP_TX;

dma_unmap:
	for (;;) {
		tx_buf = &amp;xdp_ring-&gt;tx_buf[ntu];
		dma_unmap_page(dev, dma_unmap_addr(tx_buf, dma),
			       dma_unmap_len(tx_buf, len), DMA_TO_DEVICE);
		dma_unmap_len_set(tx_buf, len, 0);
		if (tx_buf == tx_head)
			break;

		if (!ntu)
			ntu += cnt;
		ntu--;
	}
	return ICE_XDP_CONSUMED;

busy:
	xdp_ring-&gt;ring_stats-&gt;tx_stats.tx_busy++;

	return ICE_XDP_CONSUMED;
}
<br>
Frame을 XDP ring으로 제출하여 전송을 할수 있도록 한다. ice_run_xdp 자체가 tx든 rx든 통합되어 실행되므로, bottom up path에서는 본 코드는 실행되지 않을 것이라고 생각한다. 여기서부터는 본격적으로 XDP이다.
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/__ice_xmit_xdp_ring().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/__ice_xmit_xdp_ring().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_add_xdp_frag()]]></title><description><![CDATA[ 
 <br>/**
 * ice_add_xdp_frag - Add contents of Rx buffer to xdp buf as a frag
 * @rx_ring: Rx descriptor ring to transact packets on
 * @xdp: xdp buff to place the data into
 * @rx_buf: buffer containing page to add
 * @size: packet length from rx_desc
 *
 * This function will add the data contained in rx_buf-&gt;page to the xdp buf.
 * It will just attach the page as a frag.
 */
static int
ice_add_xdp_frag(struct ice_rx_ring *rx_ring, struct xdp_buff *xdp,
		 struct ice_rx_buf *rx_buf, const unsigned int size)
{
	struct skb_shared_info *sinfo = xdp_get_shared_info_from_buff(xdp);

	if (!size)
		return 0;

	if (!xdp_buff_has_frags(xdp)) {
		sinfo-&gt;nr_frags = 0;
		sinfo-&gt;xdp_frags_size = 0;
		xdp_buff_set_frags_flag(xdp);
	}

	if (unlikely(sinfo-&gt;nr_frags == MAX_SKB_FRAGS)) {
		ice_set_rx_bufs_act(xdp, rx_ring, ICE_XDP_CONSUMED);
		return -ENOMEM;
	}

	__skb_fill_page_desc_noacc(sinfo, sinfo-&gt;nr_frags++, rx_buf-&gt;page,
				   rx_buf-&gt;page_offset, size);
	sinfo-&gt;xdp_frags_size += size;
	/* remember frag count before XDP prog execution; bpf_xdp_adjust_tail()
	 * can pop off frags but driver has to handle it on its own
	 */
	rx_ring-&gt;nr_frags = sinfo-&gt;nr_frags;

	if (page_is_pfmemalloc(rx_buf-&gt;page))
		xdp_buff_set_frag_pfmemalloc(xdp);

	return 0;
}
<br>
xdp buf에다가 frag로써 Rx buffer의 컨텐츠를 추가하게 된다.
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_add_xdp_frag().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_add_xdp_frag().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_alloc_irq()]]></title><description><![CDATA[ 
 <br>/**
 * ice_alloc_irq - Allocate new interrupt vector
 * @pf: board private structure
 * @dyn_only: force dynamic allocation of the interrupt
 *
 * Allocate new interrupt vector for a given owner id.
 * return struct msi_map with interrupt details and track
 * allocated interrupt appropriately.
 *
 * This function reserves new irq entry from the irq_tracker.
 * if according to the tracker information all interrupts that
 * were allocated with ice_pci_alloc_irq_vectors are already used
 * and dynamically allocated interrupts are supported then new
 * interrupt will be allocated with pci_msix_alloc_irq_at.
 *
 * Some callers may only support dynamically allocated interrupts.
 * This is indicated with dyn_only flag.
 *
 * On failure, return map with negative .index. The caller
 * is expected to check returned map index.
 *
 */
struct msi_map ice_alloc_irq(struct ice_pf *pf, bool dyn_only)
{
	int sriov_base_vector = pf-&gt;sriov_base_vector;
	struct msi_map map = { .index = -ENOENT };
	struct device *dev = ice_pf_to_dev(pf);
	struct ice_irq_entry *entry;

	entry = ice_get_irq_res(pf, dyn_only); // [[ice_get_irq_res()]]
	if (!entry)
		return map;

	/* fail if we're about to violate SRIOV vectors space */
	if (sriov_base_vector &amp;&amp; entry-&gt;index &gt;= sriov_base_vector)
		goto exit_free_res;

	if (pci_msix_can_alloc_dyn(pf-&gt;pdev) &amp;&amp; entry-&gt;dynamic) {
		map = pci_msix_alloc_irq_at(pf-&gt;pdev, entry-&gt;index, NULL);
		if (map.index &lt; 0)
			goto exit_free_res;
		dev_dbg(dev, "allocated new irq at index %d\n", map.index);
	} else {
		map.index = entry-&gt;index;
		map.virq = pci_irq_vector(pf-&gt;pdev, map.index);
	}

	return map;

exit_free_res:
	dev_err(dev, "Could not allocate irq at idx %d\n", entry-&gt;index);
	ice_free_irq_res(pf, entry-&gt;index);
	return map;
}
<br><a data-href="ice_get_irq_res()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_get_irq_res().html" class="internal-link" target="_self" rel="noopener nofollow">ice_get_irq_res()</a><br>return이 msi_map임.<br>dyn_alloc에 따라 pci_msix_alloc_irq_at() 혹은 pci_irq_vector() 등으로 msi_map을 구성함.<br>⇒ index는 msi 인덱스이고, virq는 연관된 리눅스 인터럽트 번호임. virq 번호는 vsi의 base address에 저장된 irq로부터 index만큼 떨어져 있다고 보면 됨. 연속적인 인터럽트 번호를 가지는 것임.]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_alloc_irq().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_alloc_irq().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_alloc_rx_bufs()]]></title><description><![CDATA[ 
 <br>/**
 * ice_alloc_rx_bufs - Replace used receive buffers
 * @rx_ring: ring to place buffers on
 * @cleaned_count: number of buffers to replace
 *
 * Returns false if all allocations were successful, true if any fail. Returning
 * true signals to the caller that we didn't replace cleaned_count buffers and
 * there is more work to do.
 *
 * First, try to clean "cleaned_count" Rx buffers. Then refill the cleaned Rx
 * buffers. Then bump tail at most one time. Grouping like this lets us avoid
 * multiple tail writes per call.
 */
bool ice_alloc_rx_bufs(struct ice_rx_ring *rx_ring, unsigned int cleaned_count)
{
	union ice_32b_rx_flex_desc *rx_desc;
	u16 ntu = rx_ring-&gt;next_to_use;
	struct ice_rx_buf *bi;

	/* do nothing if no valid netdev defined */
	if ((!rx_ring-&gt;netdev &amp;&amp; rx_ring-&gt;vsi-&gt;type != ICE_VSI_CTRL) ||
	    !cleaned_count)
		return false;

	/* get the Rx descriptor and buffer based on next_to_use */
	rx_desc = ICE_RX_DESC(rx_ring, ntu);
	bi = &amp;rx_ring-&gt;rx_buf[ntu];

	do {
		/* if we fail here, we have work remaining */
		if (!ice_alloc_mapped_page(rx_ring, bi))
			break;

		/* sync the buffer for use by the device */
		dma_sync_single_range_for_device(rx_ring-&gt;dev, bi-&gt;dma,
						 bi-&gt;page_offset,
						 rx_ring-&gt;rx_buf_len,
						 DMA_FROM_DEVICE);

		/* Refresh the desc even if buffer_addrs didn't change
		 * because each write-back erases this info.
		 */
		rx_desc-&gt;read.pkt_addr = cpu_to_le64(bi-&gt;dma + bi-&gt;page_offset);

		rx_desc++;
		bi++;
		ntu++;
		if (unlikely(ntu == rx_ring-&gt;count)) {
			rx_desc = ICE_RX_DESC(rx_ring, 0);
			bi = rx_ring-&gt;rx_buf;
			ntu = 0;
		}

		/* clear the status bits for the next_to_use descriptor */
		rx_desc-&gt;wb.status_error0 = 0;

		cleaned_count--;
	} while (cleaned_count);

	if (rx_ring-&gt;next_to_use != ntu)
		ice_release_rx_desc(rx_ring, ntu);

	return !!cleaned_count;
}
<br>
사용된 수신 버퍼를 대체한다. 우선 사용된 Rx Buffer를 제거하고, 새로운 깨끗한 Rx buffer를 보충하게 된다.
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_alloc_rx_bufs().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_alloc_rx_bufs().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_alloc_vsis()]]></title><description><![CDATA[ 
 <br>static int ice_alloc_vsis(struct ice_pf *pf)
{
	struct device *dev = ice_pf_to_dev(pf);

	pf-&gt;num_alloc_vsi = pf-&gt;hw.func_caps.guar_num_vsi;
	if (!pf-&gt;num_alloc_vsi)
		return -EIO;

	if (pf-&gt;num_alloc_vsi &gt; UDP_TUNNEL_NIC_MAX_SHARING_DEVICES) {
		dev_warn(dev,
			 "limiting the VSI count due to UDP tunnel limitation %d &gt; %d\n",
			 pf-&gt;num_alloc_vsi, UDP_TUNNEL_NIC_MAX_SHARING_DEVICES);
		pf-&gt;num_alloc_vsi = UDP_TUNNEL_NIC_MAX_SHARING_DEVICES;
	}

	pf-&gt;vsi = devm_kcalloc(dev, pf-&gt;num_alloc_vsi, sizeof(*pf-&gt;vsi),
			       GFP_KERNEL);
	if (!pf-&gt;vsi)
		return -ENOMEM;

	pf-&gt;vsi_stats = devm_kcalloc(dev, pf-&gt;num_alloc_vsi,
				     sizeof(*pf-&gt;vsi_stats), GFP_KERNEL);
	if (!pf-&gt;vsi_stats) {
		devm_kfree(dev, pf-&gt;vsi);
		return -ENOMEM;
	}

	return 0;
}
<br>
vsi들을 devm_kcalloc()으로 할당해주고 있음.
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_alloc_vsis().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_alloc_vsis().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_allocate_pf()]]></title><description><![CDATA[ 
 <br>struct ice_pf *ice_allocate_pf(struct device *dev)
{
	struct devlink *devlink;

	devlink = devlink_alloc(&amp;ice_devlink_ops, sizeof(struct ice_pf), dev);
	if (!devlink)
		return NULL;

	/* Add an action to teardown the devlink when unwinding the driver */
	if (devm_add_action_or_reset(dev, ice_devlink_free, devlink))
		return NULL;

	return devlink_priv(devlink);
}
<br>
pf를 할당함. 
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_allocate_pf().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_allocate_pf().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_build_skb()]]></title><description><![CDATA[ 
 <br>/**
 * ice_build_skb - Build skb around an existing buffer
 * @rx_ring: Rx descriptor ring to transact packets on
 * @xdp: xdp_buff pointing to the data
 *
 * This function builds an skb around an existing XDP buffer, taking care
 * to set up the skb correctly and avoid any memcpy overhead. Driver has
 * already combined frags (if any) to skb_shared_info.
 */
static struct sk_buff *
ice_build_skb(struct ice_rx_ring *rx_ring, struct xdp_buff *xdp)
{
	u8 metasize = xdp-&gt;data - xdp-&gt;data_meta;
	struct skb_shared_info *sinfo = NULL;
	unsigned int nr_frags;
	struct sk_buff *skb;

	if (unlikely(xdp_buff_has_frags(xdp))) {
		sinfo = xdp_get_shared_info_from_buff(xdp);
		nr_frags = sinfo-&gt;nr_frags;
	}

	/* Prefetch first cache line of first page. If xdp-&gt;data_meta
	 * is unused, this points exactly as xdp-&gt;data, otherwise we
	 * likely have a consumer accessing first few bytes of meta
	 * data, and then actual data.
	 */
	net_prefetch(xdp-&gt;data_meta);
	/* build an skb around the page buffer */
	skb = napi_build_skb(xdp-&gt;data_hard_start, xdp-&gt;frame_sz); // [[napi_build_skb()]]
	if (unlikely(!skb))
		return NULL;

	/* must to record Rx queue, otherwise OS features such as
	 * symmetric queue won't work
	 */
	skb_record_rx_queue(skb, rx_ring-&gt;q_index);

	/* update pointers within the skb to store the data */
	skb_reserve(skb, xdp-&gt;data - xdp-&gt;data_hard_start);
	__skb_put(skb, xdp-&gt;data_end - xdp-&gt;data); // [[__skb_put()]]
	if (metasize)
		skb_metadata_set(skb, metasize);

	if (unlikely(xdp_buff_has_frags(xdp)))
		xdp_update_skb_shared_info(skb, nr_frags,
					   sinfo-&gt;xdp_frags_size,
					   nr_frags * xdp-&gt;frame_sz,
					   xdp_buff_is_frag_pfmemalloc(xdp));

	return skb;
}
<br><a data-href="napi_build_skb()" href="encyclopedia-of-networksystem/function/net-core/napi_build_skb().html" class="internal-link" target="_self" rel="noopener nofollow">napi_build_skb()</a><br>
<a data-href="__skb_put()" href="encyclopedia-of-networksystem/function/include-linux/__skb_put().html" class="internal-link" target="_self" rel="noopener nofollow">__skb_put()</a><br>
존재하는 XDP 버퍼에다가 skb 버퍼를 만드는 함수이다. build를 할 수있는지 위의 flag에 따라 본 함수가 실행되거나 아래의 ice_construct_skb가 실행되게 된다.
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_build_skb().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_build_skb().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_clean_rx_irq()]]></title><description><![CDATA[ 
 <br>/**
 * ice_clean_rx_irq - Clean completed descriptors from Rx ring - bounce buf
 * @rx_ring: Rx descriptor ring to transact packets on
 * @budget: Total limit on number of packets to process
 *
 * This function provides a "bounce buffer" approach to Rx interrupt
 * processing. The advantage to this is that on systems that have
 * expensive overhead for IOMMU access this provides a means of avoiding
 * it by maintaining the mapping of the page to the system.
 *
 * Returns amount of work completed
 */
int ice_clean_rx_irq(struct ice_rx_ring *rx_ring, int budget)
{
	unsigned int total_rx_bytes = 0, total_rx_pkts = 0;
	unsigned int offset = rx_ring-&gt;rx_offset;
	struct xdp_buff *xdp = &amp;rx_ring-&gt;xdp;
	u32 cached_ntc = rx_ring-&gt;first_desc;
	struct ice_tx_ring *xdp_ring = NULL;
	struct bpf_prog *xdp_prog = NULL;
	u32 ntc = rx_ring-&gt;next_to_clean;
	u32 cnt = rx_ring-&gt;count;
	u32 xdp_xmit = 0;
	u32 cached_ntu;
	bool failure;
	u32 first;

	/* Frame size depend on rx_ring setup when PAGE_SIZE=4K */
#if (PAGE_SIZE &lt; 8192)
	xdp-&gt;frame_sz = ice_rx_frame_truesize(rx_ring, 0);
#endif

	xdp_prog = READ_ONCE(rx_ring-&gt;xdp_prog);
	if (xdp_prog) {
		xdp_ring = rx_ring-&gt;xdp_ring;
		cached_ntu = xdp_ring-&gt;next_to_use;
	}

	/* start the loop to process Rx packets bounded by 'budget' */
	while (likely(total_rx_pkts &lt; (unsigned int)budget)) {
		union ice_32b_rx_flex_desc *rx_desc;
		struct ice_rx_buf *rx_buf;
		struct sk_buff *skb;
		unsigned int size;
		u16 stat_err_bits;
		u16 vlan_tci;

		/* get the Rx desc from Rx ring based on 'next_to_clean' */
		rx_desc = ICE_RX_DESC(rx_ring, ntc);

		/* status_error_len will always be zero for unused descriptors
		 * because it's cleared in cleanup, and overlaps with hdr_addr
		 * which is always zero because packet split isn't used, if the
		 * hardware wrote DD then it will be non-zero
		 */
		stat_err_bits = BIT(ICE_RX_FLEX_DESC_STATUS0_DD_S);
		if (!ice_test_staterr(rx_desc-&gt;wb.status_error0, stat_err_bits))
			break;

		/* This memory barrier is needed to keep us from reading
		 * any other fields out of the rx_desc until we know the
		 * DD bit is set.
		 */
		dma_rmb();

		ice_trace(clean_rx_irq, rx_ring, rx_desc);
		if (rx_desc-&gt;wb.rxdid == FDIR_DESC_RXDID || !rx_ring-&gt;netdev) {
			struct ice_vsi *ctrl_vsi = rx_ring-&gt;vsi;

			if (rx_desc-&gt;wb.rxdid == FDIR_DESC_RXDID &amp;&amp;
			    ctrl_vsi-&gt;vf)
				ice_vc_fdir_irq_handler(ctrl_vsi, rx_desc);
			if (++ntc == cnt)
				ntc = 0;
			rx_ring-&gt;first_desc = ntc;
			continue;
		}

		size = le16_to_cpu(rx_desc-&gt;wb.pkt_len) &amp;
			ICE_RX_FLX_DESC_PKT_LEN_M;

		/* retrieve a buffer from the ring */
		rx_buf = ice_get_rx_buf(rx_ring, size, ntc); // [[ice_get_rx_buf()]]

		if (!xdp-&gt;data) {
			void *hard_start;

			hard_start = page_address(rx_buf-&gt;page) + rx_buf-&gt;page_offset -
				     offset;
			xdp_prepare_buff(xdp, hard_start, offset, size, !!offset); // [[xdp_prepare_buff()]]
#if (PAGE_SIZE &gt; 4096)
			/* At larger PAGE_SIZE, frame_sz depend on len size */
			xdp-&gt;frame_sz = ice_rx_frame_truesize(rx_ring, size);
#endif
			xdp_buff_clear_frags_flag(xdp);
		} else if (ice_add_xdp_frag(rx_ring, xdp, rx_buf, size)) { // [[ice_add_xdp_frag()]]
			break;
		}
		if (++ntc == cnt)
			ntc = 0;

		/* skip if it is NOP desc */
		if (ice_is_non_eop(rx_ring, rx_desc))
			continue;

		ice_run_xdp(rx_ring, xdp, xdp_prog, xdp_ring, rx_buf, rx_desc); // [[ice_run_xdp()]]
		if (rx_buf-&gt;act == ICE_XDP_PASS)
			goto construct_skb;
		total_rx_bytes += xdp_get_buff_len(xdp);
		total_rx_pkts++;

		xdp-&gt;data = NULL;
		rx_ring-&gt;first_desc = ntc;
		rx_ring-&gt;nr_frags = 0;
		continue;
construct_skb:
		if (likely(ice_ring_uses_build_skb(rx_ring))) // [[ice_ring_uses_build_skb()]]
			skb = ice_build_skb(rx_ring, xdp); // [[ice_build_skb()]]
		else
			skb = ice_construct_skb(rx_ring, xdp); // [[ice_construct_skb()]]
		/* exit if we failed to retrieve a buffer */
		if (!skb) {
			rx_ring-&gt;ring_stats-&gt;rx_stats.alloc_page_failed++;
			rx_buf-&gt;act = ICE_XDP_CONSUMED;
			if (unlikely(xdp_buff_has_frags(xdp)))
				ice_set_rx_bufs_act(xdp, rx_ring,
						    ICE_XDP_CONSUMED);
			xdp-&gt;data = NULL;
			rx_ring-&gt;first_desc = ntc;
			rx_ring-&gt;nr_frags = 0;
			break;
		}
		xdp-&gt;data = NULL;
		rx_ring-&gt;first_desc = ntc;
		rx_ring-&gt;nr_frags = 0;

		stat_err_bits = BIT(ICE_RX_FLEX_DESC_STATUS0_RXE_S);
		if (unlikely(ice_test_staterr(rx_desc-&gt;wb.status_error0,
					      stat_err_bits))) {
			dev_kfree_skb_any(skb);
			continue;
		}

		vlan_tci = ice_get_vlan_tci(rx_desc);

		/* pad the skb if needed, to make a valid ethernet frame */
		if (eth_skb_pad(skb))
			continue;

		/* probably a little skewed due to removing CRC */
		total_rx_bytes += skb-&gt;len;

		/* populate checksum, VLAN, and protocol */
		ice_process_skb_fields(rx_ring, rx_desc, skb); // [[ice_process_skb_fields()]]

		ice_trace(clean_rx_irq_indicate, rx_ring, rx_desc, skb);
		/* send completed skb up the stack */
		ice_receive_skb(rx_ring, skb, vlan_tci); // [[ice_receive_skb()]]

		/* update budget accounting */
		total_rx_pkts++;
	}

	first = rx_ring-&gt;first_desc;
	while (cached_ntc != first) {
		struct ice_rx_buf *buf = &amp;rx_ring-&gt;rx_buf[cached_ntc];

		if (buf-&gt;act &amp; (ICE_XDP_TX | ICE_XDP_REDIR)) {
			ice_rx_buf_adjust_pg_offset(buf, xdp-&gt;frame_sz);
			xdp_xmit |= buf-&gt;act;
		} else if (buf-&gt;act &amp; ICE_XDP_CONSUMED) {
			buf-&gt;pagecnt_bias++;
		} else if (buf-&gt;act == ICE_XDP_PASS) {
			ice_rx_buf_adjust_pg_offset(buf, xdp-&gt;frame_sz);
		}

		ice_put_rx_buf(rx_ring, buf); // [[ice_put_rx_buf()]]
		if (++cached_ntc &gt;= cnt)
			cached_ntc = 0;
	}
	rx_ring-&gt;next_to_clean = ntc;
	/* return up to cleaned_count buffers to hardware */
	failure = ice_alloc_rx_bufs(rx_ring, ICE_RX_DESC_UNUSED(rx_ring)); // [[ice_alloc_rx_bufs()]]

	if (xdp_xmit)
		ice_finalize_xdp_rx(xdp_ring, xdp_xmit, cached_ntu); // [[ice_finalize_xdp_rx()]]

	if (rx_ring-&gt;ring_stats)
		ice_update_rx_ring_stats(rx_ring, total_rx_pkts,
					 total_rx_bytes); // [[ice_update_rx_ring_stats()]]

	/* guarantee a trip back through this routine if there was a failure */
	return failure ? budget : (int)total_rx_pkts;
}
<br><a data-href="ice_get_rx_buf()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_get_rx_buf().html" class="internal-link" target="_self" rel="noopener nofollow">ice_get_rx_buf()</a><br>
<a data-href="xdp_prepare_buff()" href="encyclopedia-of-networksystem/function/include-net/xdp_prepare_buff().html" class="internal-link" target="_self" rel="noopener nofollow">xdp_prepare_buff()</a><br>
<a data-href="ice_add_xdp_frag()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_add_xdp_frag().html" class="internal-link" target="_self" rel="noopener nofollow">ice_add_xdp_frag()</a><br>
<a data-href="ice_run_xdp()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_run_xdp().html" class="internal-link" target="_self" rel="noopener nofollow">ice_run_xdp()</a><br>
<a data-href="ice_ring_uses_build_skb()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_ring_uses_build_skb().html" class="internal-link" target="_self" rel="noopener nofollow">ice_ring_uses_build_skb()</a><br>
<a data-href="ice_build_skb()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_build_skb().html" class="internal-link" target="_self" rel="noopener nofollow">ice_build_skb()</a><br>
<a data-href="ice_construct_skb()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_construct_skb().html" class="internal-link" target="_self" rel="noopener nofollow">ice_construct_skb()</a><br>
<a data-href="ice_process_skb_fields()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_process_skb_fields().html" class="internal-link" target="_self" rel="noopener nofollow">ice_process_skb_fields()</a><br>
<a data-href="ice_receive_skb()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_receive_skb().html" class="internal-link" target="_self" rel="noopener nofollow">ice_receive_skb()</a><br>
<a data-href="ice_put_rx_buf()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_put_rx_buf().html" class="internal-link" target="_self" rel="noopener nofollow">ice_put_rx_buf()</a><br>
<a data-href="ice_alloc_rx_bufs()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_alloc_rx_bufs().html" class="internal-link" target="_self" rel="noopener nofollow">ice_alloc_rx_bufs()</a><br>
<a data-href="ice_finalize_xdp_rx()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_finalize_xdp_rx().html" class="internal-link" target="_self" rel="noopener nofollow">ice_finalize_xdp_rx()</a><br>
<a data-href="ice_update_rx_ring_stats()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_update_rx_ring_stats().html" class="internal-link" target="_self" rel="noopener nofollow">ice_update_rx_ring_stats()</a><br>
xdp를 무조건 셋팅하는 것으로 보인다. 우선 없다고 가정하고 보면 if(!xdp→data)를 통해 확인하여 ice_add_xdp_frag() 함수를 실행할 것이다. 그러고 나서 ice_run_xdp()를 실행한다. 이후에 보면 construct_skb:라는 label이 존재한다. 이는 네트워크 스택에 링버퍼의 내용을 전달하는 뜻으로, ICE_XDP_PASS값을 rx_buf→act에서 가질 때 실행되게 된다. 그게 아니라면 중간의 continue; 때문에 construct_skb: 라벨 아래의 코드들은 실행되지 않게 된다. construct_skb: 라벨 아래에서는 전통적인 네트워크 스택이 진행되게 된다.
중간에 ICE_RX_DESC() 매크로를 통하여 rx_ring의 ntc 인덱스에 있는 desc를 해당하는 구조체 타입으로 캐스팅하여 가져오게 된다.
또한 만약 xdp-&gt;data가 null값이라면, 즉 xdp가 가르키고 있는 data 영역이 존재하지 않는다면, 직접 rx_buf-&gt; page로 접근하여 해당 page_address를 hard_start로 하여 data 영역의 head 부분을 가르키게 한다. 이후 xdp_prepare_buff 함수를 호출하여 xdp_buff를 사용할 수 있도록 설정한다.
budget을 다 사용하였을 경우 while문을 빠져나오게 되는데, 여기는 gro를 통해 전부 끝났을 경우이다.
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_clean_rx_irq().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_clean_rx_irq().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_clean_tx_irq()]]></title><description><![CDATA[ 
 <br>/**
 * ice_clean_tx_irq - Reclaim resources after transmit completes
 * @tx_ring: Tx ring to clean
 * @napi_budget: Used to determine if we are in netpoll
 *
 * Returns true if there's any budget left (e.g. the clean is finished)
 */
static bool ice_clean_tx_irq(struct ice_tx_ring *tx_ring, int napi_budget)
{
	unsigned int total_bytes = 0, total_pkts = 0;
	unsigned int budget = ICE_DFLT_IRQ_WORK;
	struct ice_vsi *vsi = tx_ring-&gt;vsi;
	s16 i = tx_ring-&gt;next_to_clean;
	struct ice_tx_desc *tx_desc;
	struct ice_tx_buf *tx_buf;

	/* get the bql data ready */
	netdev_txq_bql_complete_prefetchw(txring_txq(tx_ring));

	tx_buf = &amp;tx_ring-&gt;tx_buf[i];
	tx_desc = ICE_TX_DESC(tx_ring, i);
	i -= tx_ring-&gt;count;

	prefetch(&amp;vsi-&gt;state);

	do {
		struct ice_tx_desc *eop_desc = tx_buf-&gt;next_to_watch;

		/* if next_to_watch is not set then there is no work pending */
		if (!eop_desc)
			break;

		/* follow the guidelines of other drivers */
		prefetchw(&amp;tx_buf-&gt;skb-&gt;users);

		smp_rmb();	/* prevent any other reads prior to eop_desc */

		ice_trace(clean_tx_irq, tx_ring, tx_desc, tx_buf);
		/* if the descriptor isn't done, no work yet to do */
		if (!(eop_desc-&gt;cmd_type_offset_bsz &amp;
		      cpu_to_le64(ICE_TX_DESC_DTYPE_DESC_DONE)))
			break;

		/* clear next_to_watch to prevent false hangs */
		tx_buf-&gt;next_to_watch = NULL;

		/* update the statistics for this packet */
		total_bytes += tx_buf-&gt;bytecount;
		total_pkts += tx_buf-&gt;gso_segs;

		/* free the skb */
		napi_consume_skb(tx_buf-&gt;skb, napi_budget);

		/* unmap skb header data */
		dma_unmap_single(tx_ring-&gt;dev,
				 dma_unmap_addr(tx_buf, dma),
				 dma_unmap_len(tx_buf, len),
				 DMA_TO_DEVICE);

		/* clear tx_buf data */
		tx_buf-&gt;type = ICE_TX_BUF_EMPTY;
		dma_unmap_len_set(tx_buf, len, 0);

		/* unmap remaining buffers */
		while (tx_desc != eop_desc) {
			ice_trace(clean_tx_irq_unmap, tx_ring, tx_desc, tx_buf);
			tx_buf++;
			tx_desc++;
			i++;
			if (unlikely(!i)) {
				i -= tx_ring-&gt;count;
				tx_buf = tx_ring-&gt;tx_buf;
				tx_desc = ICE_TX_DESC(tx_ring, 0);
			}

			/* unmap any remaining paged data */
			if (dma_unmap_len(tx_buf, len)) {
				dma_unmap_page(tx_ring-&gt;dev,
					       dma_unmap_addr(tx_buf, dma),
					       dma_unmap_len(tx_buf, len),
					       DMA_TO_DEVICE);
				dma_unmap_len_set(tx_buf, len, 0);
			}
		}
		ice_trace(clean_tx_irq_unmap_eop, tx_ring, tx_desc, tx_buf);

		/* move us one more past the eop_desc for start of next pkt */
		tx_buf++;
		tx_desc++;
		i++;
		if (unlikely(!i)) {
			i -= tx_ring-&gt;count;
			tx_buf = tx_ring-&gt;tx_buf;
			tx_desc = ICE_TX_DESC(tx_ring, 0);
		}

		prefetch(tx_desc);

		/* update budget accounting */
		budget--;
	} while (likely(budget));

	i += tx_ring-&gt;count;
	tx_ring-&gt;next_to_clean = i;

	ice_update_tx_ring_stats(tx_ring, total_pkts, total_bytes);
	netdev_tx_completed_queue(txring_txq(tx_ring), total_pkts, total_bytes);

#define TX_WAKE_THRESHOLD ((s16)(DESC_NEEDED * 2))
	if (unlikely(total_pkts &amp;&amp; netif_carrier_ok(tx_ring-&gt;netdev) &amp;&amp;
		     (ICE_DESC_UNUSED(tx_ring) &gt;= TX_WAKE_THRESHOLD))) {
		/* Make sure that anybody stopping the queue after this
		 * sees the new next_to_clean.
		 */
		smp_mb();
		if (netif_tx_queue_stopped(txring_txq(tx_ring)) &amp;&amp;
		    !test_bit(ICE_VSI_DOWN, vsi-&gt;state)) {
			netif_tx_wake_queue(txring_txq(tx_ring));
			++tx_ring-&gt;ring_stats-&gt;tx_stats.restart_q;
		}
	}

	return !!budget;
}
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_clean_tx_irq().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_clean_tx_irq().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_construct_skb()]]></title><description><![CDATA[ 
 <br>/**
 * ice_construct_skb - Allocate skb and populate it
 * @rx_ring: Rx descriptor ring to transact packets on
 * @xdp: xdp_buff pointing to the data
 *
 * This function allocates an skb. It then populates it with the page
 * data from the current receive descriptor, taking care to set up the
 * skb correctly.
 */
static struct sk_buff *
ice_construct_skb(struct ice_rx_ring *rx_ring, struct xdp_buff *xdp)
{
	unsigned int size = xdp-&gt;data_end - xdp-&gt;data;
	struct skb_shared_info *sinfo = NULL;
	struct ice_rx_buf *rx_buf;
	unsigned int nr_frags = 0;
	unsigned int headlen;
	struct sk_buff *skb;

	/* prefetch first cache line of first page */
	net_prefetch(xdp-&gt;data);

	if (unlikely(xdp_buff_has_frags(xdp))) {
		sinfo = xdp_get_shared_info_from_buff(xdp);
		nr_frags = sinfo-&gt;nr_frags;
	}

	/* allocate a skb to store the frags */
	skb = __napi_alloc_skb(&amp;rx_ring-&gt;q_vector-&gt;napi, ICE_RX_HDR_SIZE,
			       GFP_ATOMIC | __GFP_NOWARN); // [[__napi_alloc_skb()]]
	if (unlikely(!skb))
		return NULL;

	rx_buf = &amp;rx_ring-&gt;rx_buf[rx_ring-&gt;first_desc];
	skb_record_rx_queue(skb, rx_ring-&gt;q_index); // [[skb_record_rx_queue()]]
	/* Determine available headroom for copy */
	headlen = size;
	if (headlen &gt; ICE_RX_HDR_SIZE)
		headlen = eth_get_headlen(skb-&gt;dev, xdp-&gt;data, ICE_RX_HDR_SIZE);

	/* align pull length to size of long to optimize memcpy performance */
	memcpy(__skb_put(skb, headlen), xdp-&gt;data, ALIGN(headlen,
							 sizeof(long)));

	/* if we exhaust the linear part then add what is left as a frag */
	size -= headlen;
	if (size) {
		/* besides adding here a partial frag, we are going to add
		 * frags from xdp_buff, make sure there is enough space for
		 * them
		 */
		if (unlikely(nr_frags &gt;= MAX_SKB_FRAGS - 1)) {
			dev_kfree_skb(skb);
			return NULL;
		}
		skb_add_rx_frag(skb, 0, rx_buf-&gt;page,
				rx_buf-&gt;page_offset + headlen, size,
				xdp-&gt;frame_sz);
	} else {
		/* buffer is unused, change the act that should be taken later
		 * on; data was copied onto skb's linear part so there's no
		 * need for adjusting page offset and we can reuse this buffer
		 * as-is
		 */
		rx_buf-&gt;act = ICE_SKB_CONSUMED;
	}

	if (unlikely(xdp_buff_has_frags(xdp))) {
		struct skb_shared_info *skinfo = skb_shinfo(skb);

		memcpy(&amp;skinfo-&gt;frags[skinfo-&gt;nr_frags], &amp;sinfo-&gt;frags[0],
		       sizeof(skb_frag_t) * nr_frags);

		xdp_update_skb_shared_info(skb, skinfo-&gt;nr_frags + nr_frags,
					   sinfo-&gt;xdp_frags_size,
					   nr_frags * xdp-&gt;frame_sz,
					   xdp_buff_is_frag_pfmemalloc(xdp));
	}

	return skb;
}
<br><a data-href="__napi_alloc_skb()" href="encyclopedia-of-networksystem/function/net-core/__napi_alloc_skb().html" class="internal-link" target="_self" rel="noopener nofollow">__napi_alloc_skb()</a><br>
<a data-href="skb_record_rx_queue()" href="encyclopedia-of-networksystem/function/include-linux/skb_record_rx_queue().html" class="internal-link" target="_self" rel="noopener nofollow">skb_record_rx_queue()</a><br>
첫 번째 차이점은 napi_build_skb vs __napi_alloc_skb 실행이다. 이후 skb_record_rx_queue를 실행하게 된다.
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_construct_skb().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_construct_skb().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_enable_interrupt()]]></title><description><![CDATA[ 
 <br>/**
 * ice_enable_interrupt - re-enable MSI-X interrupt
 * @q_vector: the vector associated with the interrupt to enable
 *
 * If the VSI is down, the interrupt will not be re-enabled. Also,
 * when enabling the interrupt always reset the wb_on_itr to false
 * and trigger a software interrupt to clean out internal state.
 */
static void ice_enable_interrupt(struct ice_q_vector *q_vector)
{
	struct ice_vsi *vsi = q_vector-&gt;vsi;
	bool wb_en = q_vector-&gt;wb_on_itr;
	u32 itr_val;

	if (test_bit(ICE_DOWN, vsi-&gt;state))
		return;

	/* trigger an ITR delayed software interrupt when exiting busy poll, to
	 * make sure to catch any pending cleanups that might have been missed
	 * due to interrupt state transition. If busy poll or poll isn't
	 * enabled, then don't update ITR, and just enable the interrupt.
	 */
	if (!wb_en) {
		itr_val = ice_buildreg_itr(ICE_ITR_NONE, 0);
	} else {
		q_vector-&gt;wb_on_itr = false;

		/* do two things here with a single write. Set up the third ITR
		 * index to be used for software interrupt moderation, and then
		 * trigger a software interrupt with a rate limit of 20K on
		 * software interrupts, this will help avoid high interrupt
		 * loads due to frequently polling and exiting polling.
		 */
		itr_val = ice_buildreg_itr(ICE_IDX_ITR2, ICE_ITR_20K);
		itr_val |= GLINT_DYN_CTL_SWINT_TRIG_M |
			   ICE_IDX_ITR2 &lt;&lt; GLINT_DYN_CTL_SW_ITR_INDX_S |
			   GLINT_DYN_CTL_SW_ITR_INDX_ENA_M;
	}
	wr32(&amp;vsi-&gt;back-&gt;hw, GLINT_DYN_CTL(q_vector-&gt;reg_idx), itr_val);
}

]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_enable_interrupt().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_enable_interrupt().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_finalize_xdp_rx()]]></title><description><![CDATA[ 
 <br>/**
 * ice_finalize_xdp_rx - Bump XDP Tx tail and/or flush redirect map
 * @xdp_ring: XDP ring
 * @xdp_res: Result of the receive batch
 * @first_idx: index to write from caller
 *
 * This function bumps XDP Tx tail and/or flush redirect map, and
 * should be called when a batch of packets has been processed in the
 * napi loop.
 */
void ice_finalize_xdp_rx(struct ice_tx_ring *xdp_ring, unsigned int xdp_res,
			 u32 first_idx)
{
	struct ice_tx_buf *tx_buf = &amp;xdp_ring-&gt;tx_buf[first_idx];

	if (xdp_res &amp; ICE_XDP_REDIR)
		xdp_do_flush();

	if (xdp_res &amp; ICE_XDP_TX) {
		if (static_branch_unlikely(&amp;ice_xdp_locking_key))
			spin_lock(&amp;xdp_ring-&gt;tx_lock);
		/* store index of descriptor with RS bit set in the first
		 * ice_tx_buf of given NAPI batch
		 */
		tx_buf-&gt;rs_idx = ice_set_rs_bit(xdp_ring);
		ice_xdp_ring_update_tail(xdp_ring);
		if (static_branch_unlikely(&amp;ice_xdp_locking_key))
			spin_unlock(&amp;xdp_ring-&gt;tx_lock);
	}
}
<br>
Tx일 때만 사용되므로 집중해서 볼 필요는 없다.
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_finalize_xdp_rx().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_finalize_xdp_rx().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_get_irq_res()]]></title><description><![CDATA[ 
 <br>/**
 * ice_get_irq_res - get an interrupt resource
 * @pf: board private structure
 * @dyn_only: force entry to be dynamically allocated
 *
 * Allocate new irq entry in the free slot of the tracker. Since xarray
 * is used, always allocate new entry at the lowest possible index. Set
 * proper allocation limit for maximum tracker entries.
 *
 * Returns allocated irq entry or NULL on failure.
 */
static struct ice_irq_entry *ice_get_irq_res(struct ice_pf *pf, bool dyn_only)
{
	struct xa_limit limit = { .max = pf-&gt;irq_tracker.num_entries,
				  .min = 0 };
	unsigned int num_static = pf-&gt;irq_tracker.num_static;
	struct ice_irq_entry *entry;
	unsigned int index;
	int ret;

	entry = kzalloc(sizeof(*entry), GFP_KERNEL);
	if (!entry)
		return NULL;

	/* skip preallocated entries if the caller says so */
	if (dyn_only)
		limit.min = num_static;

	ret = xa_alloc(&amp;pf-&gt;irq_tracker.entries, &amp;index, entry, limit,
		       GFP_KERNEL);

	if (ret) {
		kfree(entry);
		entry = NULL;
	} else {
		entry-&gt;index = index;
		entry-&gt;dynamic = index &gt;= num_static;
	}

	return entry;
}
<br>
irq_tracker에서 lowest possible index를 할당해줌. (msi-x에서 할당하는 Irq Resource)
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_get_irq_res().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_get_irq_res().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_get_rx_buf()]]></title><description><![CDATA[ 
 <br>/**
 * ice_get_rx_buf - Fetch Rx buffer and synchronize data for use
 * @rx_ring: Rx descriptor ring to transact packets on
 * @size: size of buffer to add to skb
 * @ntc: index of next to clean element
 *
 * This function will pull an Rx buffer from the ring and synchronize it
 * for use by the CPU.
 */
static struct ice_rx_buf *
ice_get_rx_buf(struct ice_rx_ring *rx_ring, const unsigned int size,
	       const unsigned int ntc)
{
	struct ice_rx_buf *rx_buf;

	rx_buf = &amp;rx_ring-&gt;rx_buf[ntc];
	rx_buf-&gt;pgcnt =
#if (PAGE_SIZE &lt; 8192)
		page_count(rx_buf-&gt;page);
#else
		0;
#endif
	prefetchw(rx_buf-&gt;page);

	if (!size)
		return rx_buf;
	/* we are reusing so sync this buffer for CPU use */
	dma_sync_single_range_for_cpu(rx_ring-&gt;dev, rx_buf-&gt;dma,
				      rx_buf-&gt;page_offset, size,
				      DMA_FROM_DEVICE);

	/* We have pulled a buffer for use, so decrement pagecnt_bias */
	rx_buf-&gt;pagecnt_bias--;

	return rx_buf;
}
<br>
Rx Buffer를 사용하기 위해 가져와서 동기화시키는 함수이다. prefetchw(rx_buf→page)함수를 사용한다. 그후 dma_sync_single_range_for_cpu() 함수를 통해 싱크를 진행한다.
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_get_rx_buf().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_get_rx_buf().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_init_dev()]]></title><description><![CDATA[ 
 <br>static int ice_init_dev(struct ice_pf *pf)
{
	struct device *dev = ice_pf_to_dev(pf);
	struct ice_hw *hw = &amp;pf-&gt;hw;
	int err;

	err = ice_init_hw(hw);
	if (err) {
		dev_err(dev, "ice_init_hw failed: %d\n", err);
		return err;
	}

	/* Some cards require longer initialization times
	 * due to necessity of loading FW from an external source.
	 * This can take even half a minute.
	 */
	if (ice_is_pf_c827(hw)) {
		err = ice_wait_for_fw(hw, 30000);
		if (err) {
			dev_err(dev, "ice_wait_for_fw timed out");
			return err;
		}
	}

	ice_init_feature_support(pf); // [[ice_init_feature_support()]]

	ice_request_fw(pf); // [[ice_request_fw()]]

	/* if ice_request_fw fails, ICE_FLAG_ADV_FEATURES bit won't be
	 * set in pf-&gt;state, which will cause ice_is_safe_mode to return
	 * true
	 */
	if (ice_is_safe_mode(pf)) {
		/* we already got function/device capabilities but these don't
		 * reflect what the driver needs to do in safe mode. Instead of
		 * adding conditional logic everywhere to ignore these
		 * device/function capabilities, override them.
		 */
		ice_set_safe_mode_caps(hw);
	}

	err = ice_init_pf(pf); // [[ice_init_pf()]]
	if (err) {
		dev_err(dev, "ice_init_pf failed: %d\n", err);
		goto err_init_pf;
	}

	pf-&gt;hw.udp_tunnel_nic.set_port = ice_udp_tunnel_set_port;
	pf-&gt;hw.udp_tunnel_nic.unset_port = ice_udp_tunnel_unset_port;
	pf-&gt;hw.udp_tunnel_nic.flags = UDP_TUNNEL_NIC_INFO_MAY_SLEEP;
	pf-&gt;hw.udp_tunnel_nic.shared = &amp;pf-&gt;hw.udp_tunnel_shared;
	if (pf-&gt;hw.tnl.valid_count[TNL_VXLAN]) {
		pf-&gt;hw.udp_tunnel_nic.tables[0].n_entries =
			pf-&gt;hw.tnl.valid_count[TNL_VXLAN];
		pf-&gt;hw.udp_tunnel_nic.tables[0].tunnel_types =
			UDP_TUNNEL_TYPE_VXLAN;
	}
	if (pf-&gt;hw.tnl.valid_count[TNL_GENEVE]) {
		pf-&gt;hw.udp_tunnel_nic.tables[1].n_entries =
			pf-&gt;hw.tnl.valid_count[TNL_GENEVE];
		pf-&gt;hw.udp_tunnel_nic.tables[1].tunnel_types =
			UDP_TUNNEL_TYPE_GENEVE;
	}

	err = ice_init_interrupt_scheme(pf); // [[ice_init_interrupt_scheme()]]
	if (err) {
		dev_err(dev, "ice_init_interrupt_scheme failed: %d\n", err);
		err = -EIO;
		goto err_init_interrupt_scheme;
	}

	/* In case of MSIX we are going to setup the misc vector right here
	 * to handle admin queue events etc. In case of legacy and MSI
	 * the misc functionality and queue processing is combined in
	 * the same vector and that gets setup at open.
	 */
	err = ice_req_irq_msix_misc(pf); // [[ice_req_irq_msix_misc()]]
	if (err) {
		dev_err(dev, "setup of misc vector failed: %d\n", err);
		goto err_req_irq_msix_misc;
	}

	return 0;

err_req_irq_msix_misc:
	ice_clear_interrupt_scheme(pf);
err_init_interrupt_scheme:
	ice_deinit_pf(pf);
err_init_pf:
	ice_deinit_hw(hw);
	return err;
}
<br><a data-href="ice_init_feature_support()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_init_feature_support().html" class="internal-link" target="_self" rel="noopener nofollow">ice_init_feature_support()</a><br>
<a data-href="ice_request_fw()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_request_fw().html" class="internal-link" target="_self" rel="noopener nofollow">ice_request_fw()</a><br>
<a data-href="ice_init_pf()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_init_pf().html" class="internal-link" target="_self" rel="noopener nofollow">ice_init_pf()</a><br>
<a data-href="ice_init_interrupt_scheme()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_init_interrupt_scheme().html" class="internal-link" target="_self" rel="noopener nofollow">ice_init_interrupt_scheme()</a><br>
<a data-href="ice_req_irq_msix_misc()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_req_irq_msix_misc().html" class="internal-link" target="_self" rel="noopener nofollow">ice_req_irq_msix_misc()</a><br>
pf로 dev를 가져옴
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_init_dev().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_init_dev().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_init_devlink()]]></title><description><![CDATA[ 
 <br>static int ice_init_devlink(struct ice_pf *pf)
{
	int err;

	err = ice_devlink_register_params(pf);
	if (err)
		return err;

	ice_devlink_init_regions(pf);
	ice_devlink_register(pf);

	return 0;
}
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_init_devlink().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_init_devlink().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_init_feature_support()]]></title><description><![CDATA[ 
 <br>/**
 * ice_init_feature_support
 * @pf: pointer to the struct ice_pf instance
 *
 * called during init to setup supported feature
 */
void ice_init_feature_support(struct ice_pf *pf)
{
	switch (pf-&gt;hw.device_id) {
	case ICE_DEV_ID_E810C_BACKPLANE:
	case ICE_DEV_ID_E810C_QSFP:
	case ICE_DEV_ID_E810C_SFP:
	case ICE_DEV_ID_E810_XXV_BACKPLANE:
	case ICE_DEV_ID_E810_XXV_QSFP:
	case ICE_DEV_ID_E810_XXV_SFP:
		ice_set_feature_support(pf, ICE_F_DSCP);
		if (ice_is_phy_rclk_in_netlist(&amp;pf-&gt;hw))
			ice_set_feature_support(pf, ICE_F_PHY_RCLK);
		/* If we don't own the timer - don't enable other caps */
		if (!ice_pf_src_tmr_owned(pf))
			break;
		if (ice_is_cgu_in_netlist(&amp;pf-&gt;hw))
			ice_set_feature_support(pf, ICE_F_CGU);
		if (ice_is_clock_mux_in_netlist(&amp;pf-&gt;hw))
			ice_set_feature_support(pf, ICE_F_SMA_CTRL);
		if (ice_gnss_is_gps_present(&amp;pf-&gt;hw))
			ice_set_feature_support(pf, ICE_F_GNSS);
		break;
	default:
		break;
	}
}
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_init_feature_support().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_init_feature_support().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_init_interrupt_scheme()]]></title><description><![CDATA[ 
 <br>/**
 * ice_init_interrupt_scheme - Determine proper interrupt scheme
 * @pf: board private structure to initialize
 */
int ice_init_interrupt_scheme(struct ice_pf *pf)
{
	int total_vectors = pf-&gt;hw.func_caps.common_cap.num_msix_vectors;
	int vectors, max_vectors;

	vectors = ice_ena_msix_range(pf);

	if (vectors &lt; 0)
		return -ENOMEM;

	if (pci_msix_can_alloc_dyn(pf-&gt;pdev))
		max_vectors = total_vectors;
	else
		max_vectors = vectors;

	ice_init_irq_tracker(pf, max_vectors, vectors);

	return 0;
}
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_init_interrupt_scheme().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_init_interrupt_scheme().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_init_link()]]></title><description><![CDATA[ 
 <br>static int ice_init_link(struct ice_pf *pf)
{
	struct device *dev = ice_pf_to_dev(pf);
	int err;

	err = ice_init_link_events(pf-&gt;hw.port_info);
	if (err) {
		dev_err(dev, "ice_init_link_events failed: %d\n", err);
		return err;
	}

	/* not a fatal error if this fails */
	err = ice_init_nvm_phy_type(pf-&gt;hw.port_info);
	if (err)
		dev_err(dev, "ice_init_nvm_phy_type failed: %d\n", err);

	/* not a fatal error if this fails */
	err = ice_update_link_info(pf-&gt;hw.port_info);
	if (err)
		dev_err(dev, "ice_update_link_info failed: %d\n", err);

	ice_init_link_dflt_override(pf-&gt;hw.port_info);

	ice_check_link_cfg_err(pf,
			       pf-&gt;hw.port_info-&gt;phy.link_info.link_cfg_err);

	/* if media available, initialize PHY settings */
	if (pf-&gt;hw.port_info-&gt;phy.link_info.link_info &amp;
	    ICE_AQ_MEDIA_AVAILABLE) {
		/* not a fatal error if this fails */
		err = ice_init_phy_user_cfg(pf-&gt;hw.port_info);
		if (err)
			dev_err(dev, "ice_init_phy_user_cfg failed: %d\n", err);

		if (!test_bit(ICE_FLAG_LINK_DOWN_ON_CLOSE_ENA, pf-&gt;flags)) {
			struct ice_vsi *vsi = ice_get_main_vsi(pf);

			if (vsi)
				ice_configure_phy(vsi);
		}
	} else {
		set_bit(ICE_FLAG_NO_MEDIA, pf-&gt;flags);
	}

	return err;
}
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_init_link().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_init_link().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_init_pf_sw()]]></title><description><![CDATA[ 
 <br>static int ice_init_pf_sw(struct ice_pf *pf)
{
	bool dvm = ice_is_dvm_ena(&amp;pf-&gt;hw);
	struct ice_vsi *vsi;
	int err;

	/* create switch struct for the switch element created by FW on boot */
	pf-&gt;first_sw = kzalloc(sizeof(*pf-&gt;first_sw), GFP_KERNEL);
	if (!pf-&gt;first_sw)
		return -ENOMEM;

	if (pf-&gt;hw.evb_veb)
		pf-&gt;first_sw-&gt;bridge_mode = BRIDGE_MODE_VEB;
	else
		pf-&gt;first_sw-&gt;bridge_mode = BRIDGE_MODE_VEPA;

	pf-&gt;first_sw-&gt;pf = pf;

	/* record the sw_id available for later use */
	pf-&gt;first_sw-&gt;sw_id = pf-&gt;hw.port_info-&gt;sw_id;

	err = ice_aq_set_port_params(pf-&gt;hw.port_info, dvm, NULL);
	if (err)
		goto err_aq_set_port_params;

	vsi = ice_pf_vsi_setup(pf, pf-&gt;hw.port_info); // [[ice_pf_vsi_setup()]]
	if (!vsi) {
		err = -ENOMEM;
		goto err_pf_vsi_setup;
	}

	return 0;

err_pf_vsi_setup:
err_aq_set_port_params:
	kfree(pf-&gt;first_sw);
	return err;
}
<br><a data-href="ice_pf_vsi_setup()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_pf_vsi_setup().html" class="internal-link" target="_self" rel="noopener nofollow">ice_pf_vsi_setup()</a>]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_init_pf_sw().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_init_pf_sw().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_init_pf()]]></title><description><![CDATA[ 
 <br>/**
 * ice_init_pf - Initialize general software structures (struct ice_pf)
 * @pf: board private structure to initialize
 */
static int ice_init_pf(struct ice_pf *pf)
{
	ice_set_pf_caps(pf);

	mutex_init(&amp;pf-&gt;sw_mutex);
	mutex_init(&amp;pf-&gt;tc_mutex);
	mutex_init(&amp;pf-&gt;adev_mutex);
	mutex_init(&amp;pf-&gt;lag_mutex);

	INIT_HLIST_HEAD(&amp;pf-&gt;aq_wait_list);
	spin_lock_init(&amp;pf-&gt;aq_wait_lock);
	init_waitqueue_head(&amp;pf-&gt;aq_wait_queue);

	init_waitqueue_head(&amp;pf-&gt;reset_wait_queue);

	/* setup service timer and periodic service task */
	timer_setup(&amp;pf-&gt;serv_tmr, ice_service_timer, 0);
	pf-&gt;serv_tmr_period = HZ;
	INIT_WORK(&amp;pf-&gt;serv_task, ice_service_task);
	clear_bit(ICE_SERVICE_SCHED, pf-&gt;state);

	mutex_init(&amp;pf-&gt;avail_q_mutex);
	pf-&gt;avail_txqs = bitmap_zalloc(pf-&gt;max_pf_txqs, GFP_KERNEL);
	if (!pf-&gt;avail_txqs)
		return -ENOMEM;

	pf-&gt;avail_rxqs = bitmap_zalloc(pf-&gt;max_pf_rxqs, GFP_KERNEL);
	if (!pf-&gt;avail_rxqs) {
		bitmap_free(pf-&gt;avail_txqs);
		pf-&gt;avail_txqs = NULL;
		return -ENOMEM;
	}

	mutex_init(&amp;pf-&gt;vfs.table_lock);
	hash_init(pf-&gt;vfs.table);
	ice_mbx_init_snapshot(&amp;pf-&gt;hw);

	return 0;
}
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_init_pf().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_init_pf().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_init_wakeup()]]></title><description><![CDATA[ 
 <br>static void ice_init_wakeup(struct ice_pf *pf)
{
	/* Save wakeup reason register for later use */
	pf-&gt;wakeup_reason = rd32(&amp;pf-&gt;hw, PFPM_WUS);

	/* check for a power management event */
	ice_print_wake_reason(pf);

	/* clear wake status, all bits */
	wr32(&amp;pf-&gt;hw, PFPM_WUS, U32_MAX);

	/* Disable WoL at init, wait for user to enable */
	device_set_wakeup_enable(ice_pf_to_dev(pf), false);
}
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_init_wakeup().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_init_wakeup().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_init()]]></title><description><![CDATA[ 
 <br>static int ice_init(struct ice_pf *pf)
{
	int err;

	err = ice_init_dev(pf); // [[ice_init_dev()]]
	if (err)
		return err;

	err = ice_alloc_vsis(pf); // [[ice_alloc_vsis()]]
	if (err)
		goto err_alloc_vsis;

	err = ice_init_pf_sw(pf); // [[ice_init_pf_sw()]]
	if (err)
		goto err_init_pf_sw;

	ice_init_wakeup(pf); // [[ice_init_wakeup()]]

	err = ice_init_link(pf); // [[ice_init_link()]]
	if (err)
		goto err_init_link;

	err = ice_send_version(pf); // [[ice_send_version()]]
	if (err)
		goto err_init_link;

	ice_verify_cacheline_size(pf); // [[ice_verify_cacheline_size()]]

	if (ice_is_safe_mode(pf))
		ice_set_safe_mode_vlan_cfg(pf);
	else
		/* print PCI link speed and width */
		pcie_print_link_status(pf-&gt;pdev);

	/* ready to go, so clear down state bit */
	clear_bit(ICE_DOWN, pf-&gt;state);
	clear_bit(ICE_SERVICE_DIS, pf-&gt;state);

	/* since everything is good, start the service timer */
	mod_timer(&amp;pf-&gt;serv_tmr, round_jiffies(jiffies + pf-&gt;serv_tmr_period));

	return 0;

err_init_link:
	ice_deinit_pf_sw(pf);
err_init_pf_sw:
	ice_dealloc_vsis(pf);
err_alloc_vsis:
	ice_deinit_dev(pf);
	return err;
}
<br><a data-href="ice_init_dev()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_init_dev().html" class="internal-link" target="_self" rel="noopener nofollow">ice_init_dev()</a><br>
<a data-href="ice_alloc_vsis()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_alloc_vsis().html" class="internal-link" target="_self" rel="noopener nofollow">ice_alloc_vsis()</a><br>
<a data-href="ice_init_pf_sw()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_init_pf_sw().html" class="internal-link" target="_self" rel="noopener nofollow">ice_init_pf_sw()</a><br>
<a data-href="ice_init_wakeup()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_init_wakeup().html" class="internal-link" target="_self" rel="noopener nofollow">ice_init_wakeup()</a><br>
<a data-href="ice_init_link()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_init_link().html" class="internal-link" target="_self" rel="noopener nofollow">ice_init_link()</a><br>
<a data-href="ice_send_version()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_send_version().html" class="internal-link" target="_self" rel="noopener nofollow">ice_send_version()</a><br>
<a data-href="ice_verify_cacheline_size()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_verify_cacheline_size().html" class="internal-link" target="_self" rel="noopener nofollow">ice_verify_cacheline_size()</a>]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_init().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_init().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_irq_dynamic_ena()]]></title><description><![CDATA[ 
 <br>/**
 * ice_irq_dynamic_ena - Enable default interrupt generation settings
 * @hw: pointer to HW struct
 * @vsi: pointer to VSI struct, can be NULL
 * @q_vector: pointer to q_vector, can be NULL
 */
static inline void
ice_irq_dynamic_ena(struct ice_hw *hw, struct ice_vsi *vsi,
		    struct ice_q_vector *q_vector)
{
	u32 vector = (vsi &amp;&amp; q_vector) ? q_vector-&gt;reg_idx :
				((struct ice_pf *)hw-&gt;back)-&gt;oicr_irq.index;
	int itr = ICE_ITR_NONE;
	u32 val;

	/* clear the PBA here, as this function is meant to clean out all
	 * previous interrupts and enable the interrupt
	 */
	val = GLINT_DYN_CTL_INTENA_M | GLINT_DYN_CTL_CLEARPBA_M |
	      (itr &lt;&lt; GLINT_DYN_CTL_ITR_INDX_S);
	if (vsi)
		if (test_bit(ICE_VSI_DOWN, vsi-&gt;state))
			return;
	wr32(hw, GLINT_DYN_CTL(vector), val);
}
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_irq_dynamic_ena().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_irq_dynamic_ena().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_load()]]></title><description><![CDATA[ 
 <br>/**
 * ice_load - load pf by init hw and starting VSI
 * @pf: pointer to the pf instance
 *
 * This function has to be called under devl_lock.
 */
int ice_load(struct ice_pf *pf)
{
	struct ice_vsi *vsi;
	int err;

	devl_assert_locked(priv_to_devlink(pf));

	vsi = ice_get_main_vsi(pf);

	/* init channel list */
	INIT_LIST_HEAD(&amp;vsi-&gt;ch_list);

	err = ice_cfg_netdev(vsi);
	if (err)
		return err;

	/* Setup DCB netlink interface */
	ice_dcbnl_setup(vsi);

	err = ice_init_mac_fltr(pf);
	if (err)
		goto err_init_mac_fltr;

	err = ice_devlink_create_pf_port(pf);
	if (err)
		goto err_devlink_create_pf_port;

	SET_NETDEV_DEVLINK_PORT(vsi-&gt;netdev, &amp;pf-&gt;devlink_port);

	err = ice_register_netdev(vsi);
	if (err)
		goto err_register_netdev;

	err = ice_tc_indir_block_register(vsi);
	if (err)
		goto err_tc_indir_block_register;

	ice_napi_add(vsi); // [[ice_napi_add()]]

	err = ice_init_rdma(pf);
	if (err)
		goto err_init_rdma;

	ice_init_features(pf);
	ice_service_task_restart(pf);

	clear_bit(ICE_DOWN, pf-&gt;state);

	return 0;

err_init_rdma:
	ice_tc_indir_block_unregister(vsi);
err_tc_indir_block_register:
	ice_unregister_netdev(vsi);
err_register_netdev:
	ice_devlink_destroy_pf_port(pf);
err_devlink_create_pf_port:
err_init_mac_fltr:
	ice_decfg_netdev(vsi);
	return err;
}
<br><a data-href="ice_napi_add()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_napi_add().html" class="internal-link" target="_self" rel="noopener nofollow">ice_napi_add()</a>]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_load().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_load().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_msix_clean_rings()]]></title><description><![CDATA[ 
 <br>/**
 * ice_msix_clean_rings - MSIX mode Interrupt Handler
 * @irq: interrupt number
 * @data: pointer to a q_vector
 */
static irqreturn_t ice_msix_clean_rings(int __always_unused irq, void *data)
{
	struct ice_q_vector *q_vector = (struct ice_q_vector *)data;

	if (!q_vector-&gt;tx.tx_ring &amp;&amp; !q_vector-&gt;rx.rx_ring)
		return IRQ_HANDLED;

	q_vector-&gt;total_events++;

	napi_schedule(&amp;q_vector-&gt;napi); // [[napi_schedule()]]

	return IRQ_HANDLED;
}
<br><a data-href="napi_schedule()" href="encyclopedia-of-networksystem/function/include-linux/napi_schedule().html" class="internal-link" target="_self" rel="noopener nofollow">napi_schedule()</a><br>void *data를 통해 ice_q_vector가 들어오고 여기서 napi struct를 볼 수 있다. —&gt; hardware interrupt임. napi 스케쥴링 이후 종료.]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_msix_clean_rings().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_msix_clean_rings().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_napi_add()]]></title><description><![CDATA[ 
 <br>static void ice_napi_add(struct ice_vsi *vsi)
{
	int v_idx;

	if (!vsi-&gt;netdev)
		return;

	ice_for_each_q_vector(vsi, v_idx) {
		netif_napi_add(vsi-&gt;netdev, &amp;vsi-&gt;q_vectors[v_idx]-&gt;napi,
			       ice_napi_poll); // [[netif_napi_add()]]
		__ice_q_vector_set_napi_queues(vsi-&gt;q_vectors[v_idx], false); // _[[__ice_q_vector_set_napi_queues()]]
	}
}
<br><a data-href="netif_napi_add()" href="encyclopedia-of-networksystem/function/include-linux/netif_napi_add().html" class="internal-link" target="_self" rel="noopener nofollow">netif_napi_add()</a><br>
<a data-href="__ice_q_vector_set_napi_queues()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/__ice_q_vector_set_napi_queues().html" class="internal-link" target="_self" rel="noopener nofollow">__ice_q_vector_set_napi_queues()</a>]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_napi_add().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_napi_add().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_napi_enable_all()]]></title><description><![CDATA[ 
 <br>/**
 * ice_napi_enable_all - Enable NAPI for all q_vectors in the VSI
 * @vsi: the VSI being configured
 */
static void ice_napi_enable_all(struct ice_vsi *vsi)
{
	int q_idx;

	if (!vsi-&gt;netdev)
		return;

	ice_for_each_q_vector(vsi, q_idx) {
		struct ice_q_vector *q_vector = vsi-&gt;q_vectors[q_idx];

		ice_init_moderation(q_vector);

		if (q_vector-&gt;rx.rx_ring || q_vector-&gt;tx.tx_ring)
			napi_enable(&amp;q_vector-&gt;napi);
	}
}
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_napi_enable_all().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_napi_enable_all().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_napi_poll()]]></title><description><![CDATA[ 
 <br>/**
 * ice_napi_poll - NAPI polling Rx/Tx cleanup routine
 * @napi: napi struct with our devices info in it
 * @budget: amount of work driver is allowed to do this pass, in packets
 *
 * This function will clean all queues associated with a q_vector.
 *
 * Returns the amount of work done
 */
int ice_napi_poll(struct napi_struct *napi, int budget)
{
	struct ice_q_vector *q_vector = // [[ice_q_vector]]
				container_of(napi, struct ice_q_vector, napi);
	struct ice_tx_ring *tx_ring; // [[ice_tx_ring]]
	struct ice_rx_ring *rx_ring; // [[ice_rx_ring]]
	bool clean_complete = true;
	int budget_per_ring;
	int work_done = 0;

	/* Since the actual Tx work is minimal, we can give the Tx a larger
	 * budget and be more aggressive about cleaning up the Tx descriptors.
	 */
	ice_for_each_tx_ring(tx_ring, q_vector-&gt;tx) {
		bool wd;

		if (tx_ring-&gt;xsk_pool)
			wd = ice_xmit_zc(tx_ring); // [[ice_xmit_zc()]]
		else if (ice_ring_is_xdp(tx_ring))
			wd = true;
		else
			wd = ice_clean_tx_irq(tx_ring, budget); // [[ice_clean_tx_irq()]]

		if (!wd)
			clean_complete = false;
	}

	/* Handle case where we are called by netpoll with a budget of 0 */
	if (unlikely(budget &lt;= 0))
		return budget;

	/* normally we have 1 Rx ring per q_vector */
	if (unlikely(q_vector-&gt;num_ring_rx &gt; 1))
		/* We attempt to distribute budget to each Rx queue fairly, but
		 * don't allow the budget to go below 1 because that would exit
		 * polling early.
		 */
		budget_per_ring = max_t(int, budget / q_vector-&gt;num_ring_rx, 1);
	else
		/* Max of 1 Rx ring in this q_vector so give it the budget */
		budget_per_ring = budget;

	ice_for_each_rx_ring(rx_ring, q_vector-&gt;rx) {
		int cleaned;

		/* A dedicated path for zero-copy allows making a single
		 * comparison in the irq context instead of many inside the
		 * ice_clean_rx_irq function and makes the codebase cleaner.
		 */
		cleaned = rx_ring-&gt;xsk_pool ?
			  ice_clean_rx_irq_zc(rx_ring, budget_per_ring) :
			  ice_clean_rx_irq(rx_ring, budget_per_ring); // [[Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_clean_rx_irq()|ice_clean_rx_irq()]]
		work_done += cleaned;
		/* if we clean as many as budgeted, we must not be done */
		if (cleaned &gt;= budget_per_ring)
			clean_complete = false;
	}

	/* If work not completed, return budget and polling will return */
	if (!clean_complete) {
		/* Set the writeback on ITR so partial completions of
		 * cache-lines will still continue even if we're polling.
		 */
		ice_set_wb_on_itr(q_vector);
		return budget;
	}

	/* Exit the polling mode, but don't re-enable interrupts if stack might
	 * poll us due to busy-polling
	 */
	if (napi_complete_done(napi, work_done)) {
		ice_net_dim(q_vector); // [[ice_net_dim()]]
		ice_enable_interrupt(q_vector); // [[ice_enable_interrupt()]]
	} else {
		ice_set_wb_on_itr(q_vector);
	}

	return min_t(int, work_done, budget - 1);
}

<br><a data-href="ice_q_vector" href="encyclopedia-of-networksystem/struct/drivers-net-ethernet-intel-ice/ice_q_vector.html" class="internal-link" target="_self" rel="noopener nofollow">ice_q_vector</a><br>
<a data-href="ice_tx_ring" href="encyclopedia-of-networksystem/struct/drivers-net-ethernet-intel-ice/ice_tx_ring.html" class="internal-link" target="_self" rel="noopener nofollow">ice_tx_ring</a><br>
<a data-href="ice_rx_ring" href="encyclopedia-of-networksystem/struct/drivers-net-ethernet-intel-ice/ice_rx_ring.html" class="internal-link" target="_self" rel="noopener nofollow">ice_rx_ring</a><br>
<a data-href="ice_xmit_zc()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_xmit_zc().html" class="internal-link" target="_self" rel="noopener nofollow">ice_xmit_zc()</a><br>
<a data-href="ice_clean_tx_irq()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_clean_tx_irq().html" class="internal-link" target="_self" rel="noopener nofollow">ice_clean_tx_irq()</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_clean_rx_irq()" data-href="Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_clean_rx_irq()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_clean_rx_irq().html" class="internal-link" target="_self" rel="noopener nofollow">ice_clean_rx_irq()</a><br>
<a data-href="ice_net_dim()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_net_dim().html" class="internal-link" target="_self" rel="noopener nofollow">ice_net_dim()</a><br>
<a data-href="ice_enable_interrupt()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_enable_interrupt().html" class="internal-link" target="_self" rel="noopener nofollow">ice_enable_interrupt()</a><br>
napi_poll은 Rx와 Tx를 동시에 다루게 된다. 모든 tx링에 대하여 ice_xmit_zc()혹은 ice_clean_tx_irq()등의 함수로 다루게 되는데 일단 bottom up path이므로 추후에 살펴보고자 한다. 각각의 rx_ring에 대하여 xsk_pool이 있다면 zc옵션이 있는 함수인 ice_clean_rx_irq_zc, 아니라면 ice_clean_rx_irq를 실행하게 된다.
<br>특히 num_ring_rx를 확인하는 부분에서, 보통은 rx_ring이 1개임을 알 수 있다. unlikely로 if문에 들어가 있기 때문이다. 또한 이러한 경우, 각각의 rx_ring에 budget을 골고루 분배해야하기 때문에 budget_per_ring이 budget을 rx_ring의 갯수로 나눠준 값이 된다.<br>
<br>TX ring 초기화하기 (clean)

<br>wd = <a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_xmit_zc().md" data-href="Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_xmit_zc().md" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_xmit_zc().html" class="internal-link" target="_self" rel="noopener nofollow">ice_xmit_zc</a>(tx_ring);

<br>xsk_pool 이 있을 경우, zero copy pkt tx 할 경우, kernel bypass에 사용된다?
<br>wd에 tx가 성공적이었는지 저장


<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_ring_is_xdp().md" data-href="Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_ring_is_xdp().md" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_ring_is_xdp().html" class="internal-link" target="_self" rel="noopener nofollow">ice_ring_is_xdp</a>(tx_ring) 일 때는 항상 true로 설정한다

<br>xdp (express data path) 사용할 때


<br>wd = <a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_clean_tx_irq().md" data-href="Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_clean_tx_irq().md" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_clean_tx_irq().html" class="internal-link" target="_self" rel="noopener nofollow">ice_clean_tx_irq</a>(tx_ring, budget);

<br>default tx 초기 과정이다
<br>descriptor를 free해준다




<br>budget_per_ring = max_t(int, budget / q_vector-&gt;num_ring_rx, 1);

<br>budget은 rx_ring 개수로 나눠서 공평하게 배분한다
<br>최소 1 은 받게 설정


<br>clean rx ring

<br>clean 된 irq의 개수를 budget과 비교한다
<br>만약 전자가 더 많으면 uncomplete된 상태이므로 계속 polling을 한다
<br>clean 된 개수와 budget을 대소 비교로 complete 여부를 알수있나?


<br>if not complete, return the budget
<br>exit polling<br>
<a data-href="ice_net_dim()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_net_dim().html" class="internal-link" target="_self" rel="noopener nofollow">ice_net_dim()</a>
<br>/**
 * ice_net_dim - Update net DIM algorithm
 * @q_vector: the vector associated with the interrupt
 *
 * Create a DIM sample and notify net_dim() so that it can possibly decide
 * a new ITR value based on incoming packets, bytes, and interrupts.
 *
 * This function is a no-op if the ring is not configured to dynamic ITR.
 */
static void ice_net_dim(struct ice_q_vector *q_vector)
{
	struct ice_ring_container *tx = &amp;q_vector-&gt;tx;
	struct ice_ring_container *rx = &amp;q_vector-&gt;rx;

	if (ITR_IS_DYNAMIC(tx)) {
		struct dim_sample dim_sample;

		__ice_update_sample(q_vector, tx, &amp;dim_sample, true);
		net_dim(&amp;tx-&gt;dim, dim_sample);
	}

	if (ITR_IS_DYNAMIC(rx)) {
		struct dim_sample dim_sample;

		__ice_update_sample(q_vector, rx, &amp;dim_sample, false);
		net_dim(&amp;rx-&gt;dim, dim_sample);
	}
}
<br>[[ice_enable_interrupt()]]
<br>/**
 * ice_enable_interrupt - re-enable MSI-X interrupt
 * @q_vector: the vector associated with the interrupt to enable
 *
 * If the VSI is down, the interrupt will not be re-enabled. Also,
 * when enabling the interrupt always reset the wb_on_itr to false
 * and trigger a software interrupt to clean out internal state.
 */
static void ice_enable_interrupt(struct ice_q_vector *q_vector)
{
	struct ice_vsi *vsi = q_vector-&gt;vsi;
	bool wb_en = q_vector-&gt;wb_on_itr;
	u32 itr_val;

	if (test_bit(ICE_DOWN, vsi-&gt;state))
		return;

	/* trigger an ITR delayed software interrupt when exiting busy poll, to
	 * make sure to catch any pending cleanups that might have been missed
	 * due to interrupt state transition. If busy poll or poll isn't
	 * enabled, then don't update ITR, and just enable the interrupt.
	 */
	if (!wb_en) {
		itr_val = ice_buildreg_itr(ICE_ITR_NONE, 0);
	} else {
		q_vector-&gt;wb_on_itr = false;

		/* do two things here with a single write. Set up the third ITR
		 * index to be used for software interrupt moderation, and then
		 * trigger a software interrupt with a rate limit of 20K on
		 * software interrupts, this will help avoid high interrupt
		 * loads due to frequently polling and exiting polling.
		 */
		itr_val = ice_buildreg_itr(ICE_IDX_ITR2, ICE_ITR_20K);
		itr_val |= GLINT_DYN_CTL_SWINT_TRIG_M |
			   ICE_IDX_ITR2 &lt;&lt; GLINT_DYN_CTL_SW_ITR_INDX_S |
			   GLINT_DYN_CTL_SW_ITR_INDX_ENA_M;
	}
	wr32(&amp;vsi-&gt;back-&gt;hw, GLINT_DYN_CTL(q_vector-&gt;reg_idx), itr_val);
}
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_napi_poll().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_napi_poll().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_net_dim()]]></title><description><![CDATA[ 
 <br>/**
 * ice_net_dim - Update net DIM algorithm
 * @q_vector: the vector associated with the interrupt
 *
 * Create a DIM sample and notify net_dim() so that it can possibly decide
 * a new ITR value based on incoming packets, bytes, and interrupts.
 *
 * This function is a no-op if the ring is not configured to dynamic ITR.
 */
static void ice_net_dim(struct ice_q_vector *q_vector)
{
	struct ice_ring_container *tx = &amp;q_vector-&gt;tx;
	struct ice_ring_container *rx = &amp;q_vector-&gt;rx;

	if (ITR_IS_DYNAMIC(tx)) {
		struct dim_sample dim_sample;

		__ice_update_sample(q_vector, tx, &amp;dim_sample, true);
		net_dim(&amp;tx-&gt;dim, dim_sample);
	}

	if (ITR_IS_DYNAMIC(rx)) {
		struct dim_sample dim_sample;

		__ice_update_sample(q_vector, rx, &amp;dim_sample, false);
		net_dim(&amp;rx-&gt;dim, dim_sample);
	}
}
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_net_dim().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_net_dim().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_open_internal()]]></title><description><![CDATA[ 
 <br>/**
 * ice_open_internal - Called when a network interface becomes active
 * @netdev: network interface device structure
 *
 * Internal ice_open implementation. Should not be used directly except for ice_open and reset
 * handling routine
 *
 * Returns 0 on success, negative value on failure
 */
int ice_open_internal(struct net_device *netdev)
{
	struct ice_netdev_priv *np = netdev_priv(netdev);
	struct ice_vsi *vsi = np-&gt;vsi;
	struct ice_pf *pf = vsi-&gt;back;
	struct ice_port_info *pi;
	int err;

	if (test_bit(ICE_NEEDS_RESTART, pf-&gt;state)) {
		netdev_err(netdev, "driver needs to be unloaded and reloaded\n");
		return -EIO;
	}

	netif_carrier_off(netdev);

	pi = vsi-&gt;port_info;
	err = ice_update_link_info(pi);
	if (err) {
		netdev_err(netdev, "Failed to get link info, error %d\n", err);
		return err;
	}

	ice_check_link_cfg_err(pf, pi-&gt;phy.link_info.link_cfg_err);

	/* Set PHY if there is media, otherwise, turn off PHY */
	if (pi-&gt;phy.link_info.link_info &amp; ICE_AQ_MEDIA_AVAILABLE) {
		clear_bit(ICE_FLAG_NO_MEDIA, pf-&gt;flags);
		if (!test_bit(ICE_PHY_INIT_COMPLETE, pf-&gt;state)) {
			err = ice_init_phy_user_cfg(pi);
			if (err) {
				netdev_err(netdev, "Failed to initialize PHY settings, error %d\n",
					   err);
				return err;
			}
		}

		err = ice_configure_phy(vsi);
		if (err) {
			netdev_err(netdev, "Failed to set physical link up, error %d\n",
				   err);
			return err;
		}
	} else {
		set_bit(ICE_FLAG_NO_MEDIA, pf-&gt;flags);
		ice_set_link(vsi, false);
	}

	err = ice_vsi_open(vsi); // [[Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_vsi_open()|ice_vsi_open()]]
	if (err)
		netdev_err(netdev, "Failed to open VSI 0x%04X on switch 0x%04X\n",
			   vsi-&gt;vsi_num, vsi-&gt;vsw-&gt;sw_id);

	/* Update existing tunnels information */
	udp_tunnel_get_rx_info(netdev);

	return err;
}
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_vsi_open()" data-href="Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_vsi_open()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_open().html" class="internal-link" target="_self" rel="noopener nofollow">ice_vsi_open()</a>]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_open_internal().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_open_internal().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_open()]]></title><description><![CDATA[ 
 <br>/**
 * ice_open - Called when a network interface becomes active
 * @netdev: network interface device structure
 *
 * The open entry point is called when a network interface is made
 * active by the system (IFF_UP). At this point all resources needed
 * for transmit and receive operations are allocated, the interrupt
 * handler is registered with the OS, the netdev watchdog is enabled,
 * and the stack is notified that the interface is ready.
 *
 * Returns 0 on success, negative value on failure
 */
int ice_open(struct net_device *netdev)
{
	struct ice_netdev_priv *np = netdev_priv(netdev);
	struct ice_pf *pf = np-&gt;vsi-&gt;back;

	if (ice_is_reset_in_progress(pf-&gt;state)) {
		netdev_err(netdev, "can't open net device while reset is in progress");
		return -EBUSY;
	}

	return ice_open_internal(netdev); // [[Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_open_internal()|ice_open_internal()]]
}
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_open_internal()" data-href="Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_open_internal()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_open_internal().html" class="internal-link" target="_self" rel="noopener nofollow">ice_open_internal()</a><br>NIC가 활성화 되었을 때 네트워크 작동을 위해 초기화 하는 단계]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_open().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_open().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_pf_vsi_setup()]]></title><description><![CDATA[ 
 <br>/**
 * ice_pf_vsi_setup - Set up a PF VSI
 * @pf: board private structure
 * @pi: pointer to the port_info instance
 *
 * Returns pointer to the successfully allocated VSI software struct
 * on success, otherwise returns NULL on failure.
 */
static struct ice_vsi *
ice_pf_vsi_setup(struct ice_pf *pf, struct ice_port_info *pi)
{
	struct ice_vsi_cfg_params params = {};

	params.type = ICE_VSI_PF;
	params.pi = pi;
	params.flags = ICE_VSI_FLAG_INIT;

	return ice_vsi_setup(pf, &amp;params); // [[ice_vsi_setup()]]
}
<br><a data-href="ice_vsi_setup()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_setup().html" class="internal-link" target="_self" rel="noopener nofollow">ice_vsi_setup()</a>]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_pf_vsi_setup().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_pf_vsi_setup().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_probe()]]></title><description><![CDATA[<a class="tag" href="?query=tag:ifndef" style="background-color: rgb(4, 108, 116); color: white; font-weight: 700; border: none; border-radius: 1em; padding: 0.2em 0.5em;">#ifndef</a> 
 <br>/**
 * ice_probe - Device initialization routine
 * @pdev: PCI device information struct
 * @ent: entry in ice_pci_tbl
 *
 * Returns 0 on success, negative on failure
 */
static int
ice_probe(struct pci_dev *pdev, const struct pci_device_id __always_unused *ent)
{
	struct device *dev = &amp;pdev-&gt;dev;
	struct ice_pf *pf;
	struct ice_hw *hw;
	int err;

	if (pdev-&gt;is_virtfn) {
		dev_err(dev, "can't probe a virtual function\n");
		return -EINVAL;
	}

	/* when under a kdump kernel initiate a reset before enabling the
	 * device in order to clear out any pending DMA transactions. These
	 * transactions can cause some systems to machine check when doing
	 * the pcim_enable_device() below.
	 */
	if (is_kdump_kernel()) {
		pci_save_state(pdev); // [[pci_save_state()]]
		pci_clear_master(pdev); // [[pci_clear_master()]]
		err = pcie_flr(pdev);
		if (err)
			return err;
		pci_restore_state(pdev);
	}

	/* this driver uses devres, see
	 * Documentation/driver-api/driver-model/devres.rst
	 */
	err = pcim_enable_device(pdev);
	if (err)
		return err;

	err = pcim_iomap_regions(pdev, BIT(ICE_BAR0), dev_driver_string(dev));
	if (err) {
		dev_err(dev, "BAR0 I/O map error %d\n", err);
		return err;
	}

	pf = ice_allocate_pf(dev); // [[ice_allocate_pf()]]
	if (!pf)
		return -ENOMEM;

	/* initialize Auxiliary index to invalid value */
	pf-&gt;aux_idx = -1;

	/* set up for high or low DMA */
	err = dma_set_mask_and_coherent(dev, DMA_BIT_MASK(64)); // [[dma_set_mask_and_coherent()]]
	if (err) {
		dev_err(dev, "DMA configuration failed: 0x%x\n", err);
		return err;
	}

	pci_set_master(pdev); // [[pci_set_master()]]

	pf-&gt;pdev = pdev;
	pci_set_drvdata(pdev, pf);
	set_bit(ICE_DOWN, pf-&gt;state);
	/* Disable service task until DOWN bit is cleared */
	set_bit(ICE_SERVICE_DIS, pf-&gt;state);

	hw = &amp;pf-&gt;hw;
	hw-&gt;hw_addr = pcim_iomap_table(pdev)[ICE_BAR0];
	pci_save_state(pdev); // [[pci_save_state()]]

	hw-&gt;back = pf;
	hw-&gt;port_info = NULL;
	hw-&gt;vendor_id = pdev-&gt;vendor;
	hw-&gt;device_id = pdev-&gt;device;
	pci_read_config_byte(pdev, PCI_REVISION_ID, &amp;hw-&gt;revision_id);
	hw-&gt;subsystem_vendor_id = pdev-&gt;subsystem_vendor;
	hw-&gt;subsystem_device_id = pdev-&gt;subsystem_device;
	hw-&gt;bus.device = PCI_SLOT(pdev-&gt;devfn);
	hw-&gt;bus.func = PCI_FUNC(pdev-&gt;devfn);
	ice_set_ctrlq_len(hw);

	pf-&gt;msg_enable = netif_msg_init(debug, ICE_DFLT_NETIF_M);

#ifndef CONFIG_DYNAMIC_DEBUG
	if (debug &lt; -1)
		hw-&gt;debug_mask = debug;
#endif

	err = ice_init(pf); // [[ice_init()]]
	if (err)
		goto err_init;

	devl_lock(priv_to_devlink(pf));
	err = ice_load(pf); // [[ice_load()]]
	devl_unlock(priv_to_devlink(pf));
	if (err)
		goto err_load;

	err = ice_init_devlink(pf); // [[ice_init_devlink()]]
	if (err)
		goto err_init_devlink;

	return 0;

err_init_devlink:
	devl_lock(priv_to_devlink(pf));
	ice_unload(pf);
	devl_unlock(priv_to_devlink(pf));
err_load:
	ice_deinit(pf);
err_init:
	pci_disable_device(pdev);
	return err;
}
<br><a data-href="pci_save_state()" href="encyclopedia-of-networksystem/function/drivers-pci/pci_save_state().html" class="internal-link" target="_self" rel="noopener nofollow">pci_save_state()</a><br>
<a data-href="pci_clear_master()" href="encyclopedia-of-networksystem/function/drivers-pci/pci_clear_master().html" class="internal-link" target="_self" rel="noopener nofollow">pci_clear_master()</a><br>
<a data-href="ice_allocate_pf()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_allocate_pf().html" class="internal-link" target="_self" rel="noopener nofollow">ice_allocate_pf()</a><br>
<a data-href="dma_set_mask_and_coherent()" href="encyclopedia-of-networksystem/function/include-linux/dma_set_mask_and_coherent().html" class="internal-link" target="_self" rel="noopener nofollow">dma_set_mask_and_coherent()</a><br>
<a data-href="pci_set_master()" href="encyclopedia-of-networksystem/function/drivers-pci/pci_set_master().html" class="internal-link" target="_self" rel="noopener nofollow">pci_set_master()</a><br>
<a data-href="ice_init()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_init().html" class="internal-link" target="_self" rel="noopener nofollow">ice_init()</a><br>
<a data-href="ice_load()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_load().html" class="internal-link" target="_self" rel="noopener nofollow">ice_load()</a><br>
<a data-href="ice_init_devlink()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_init_devlink().html" class="internal-link" target="_self" rel="noopener nofollow">ice_init_devlink()</a><br>ice_probe 함수는 Intel의 NIC(Network Interface Card) 드라이버에서 PCI 장치(PNIC)를 초기화하고 설정하는 과정에서 호출되는 함수이다. 이 함수는 NIC를 시스템에 등록하고, 필요한 리소스를 할당하며, 초기화를 수행한다. 함수의 각 단계와 해당 단계에서 호출되는 함수의 기능을 자세히 설명하겠다.<br><br><br>/**
 * ice_probe - Device initialization routine
 * @pdev: PCI device information struct
 * @ent: entry in ice_pci_tbl
 *
 * Returns 0 on success, negative on failure
 */
static int ice_probe(struct pci_dev *pdev, const struct pci_device_id __always_unused *ent)
{
	struct device *dev = &amp;pdev-&gt;dev;
	struct ice_pf *pf;
	struct ice_hw *hw;
	int err;
<br><br>if (pdev-&gt;is_virtfn) {
    dev_err(dev, "can't probe a virtual function\\n");
    return -EINVAL;
}
<br>목적: 장치가 가상 함수(Virtual Function)인지 확인한다.<br>설명: pdev-&gt;is_virtfn이 참이면, 가상 함수를 프로브하지 않고 오류 메시지를 출력한 후, -EINVAL을 반환하며 함수를 종료한다.<br><br>/* when under a kdump kernel initiate a reset before enabling the
* device in order to clear out any pending DMA transactions. These
* transactions can cause some systems to machine check when doing
* the pcim_enable_device() below.
*/
if (is_kdump_kernel()) {
		pci_save_state(pdev);
		pci_clear_master(pdev);
		err = pcie_flr(pdev);
		if (err)
			return err;
		pci_restore_state(pdev);
	}
<br>목적: kdump 커널에서 장치 초기화 시 남아 있는 DMA 트랜잭션을 정리한다.<br>설명:<br>
<br>pci_save_state(pdev): 현재 PCI 상태를 저장한다.
<br>pci_clear_master(pdev): DMA 마스터 비트를 비활성화하여 DMA 트랜잭션을 중지한다.
<br>pcie_flr(pdev): PCIe 기능 레벨 리셋(FLR)을 수행한다.
<br>오류가 발생하면 해당 오류를 반환하고 함수를 종료한다.
<br>pci_restore_state(pdev): 저장된 PCI 상태를 복원한다.
<br><br>/* this driver uses devres, see
* Documentation/driver-api/driver-model/devres.rst
*/
err = pcim_enable_device(pdev);
if (err)
    return err;

err = pcim_iomap_regions(pdev, BIT(ICE_BAR0), dev_driver_string(dev));
if (err) {
    dev_err(dev, "BAR0 I/O map error %d\\n", err);
    return err;
}
<br>목적: PCI 장치를 활성화하고 I/O 메모리에 매핑한다.<br>설명:<br>
<br>pcim_enable_device(pdev): PCI 장치를 활성화한다. 오류 발생 시 해당 오류를 반환하고 함수를 종료한다.
<br>pcim_iomap_regions(pdev, BIT(ICE_BAR0), dev_driver_string(dev)): BAR0를 I/O 메모리에 매핑한다. 오류 발생 시 오류 메시지를 출력하고 해당 오류를 반환하며 함수를 종료한다.
<br><br>pf = ice_allocate_pf(dev);
if (!pf)
    return -ENOMEM;
    
/* initialize Auxiliary index to invalid value */
pf-&gt;aux_idx = -1;
<br>목적: PF 구조체를 할당하고 초기화한다.<br>설명:<br>
<br>ice_allocate_pf(dev): PF 구조체를 할당한다. 할당 실패 시 메모리 부족 오류 -ENOMEM을 반환하며 함수를 종료한다.
<br>pf-&gt;aux_idx = -1: 보조 인덱스를 무효 값으로 초기화한다.
<br><br>/* set up for high or low DMA */
err = dma_set_mask_and_coherent(dev, DMA_BIT_MASK(64));
if (err) {
    dev_err(dev, "DMA configuration failed: 0x%x\\n", err);
    return err;
}
pci_set_master(pdev);
<br>목적: DMA 설정을 수행하고 장치를 DMA 마스터로 설정한다.<br>설명:<br>
<br>dma_set_mask_and_coherent(dev, DMA_BIT_MASK(64)): 64비트 DMA 주소 마스크를 설정한다. 오류 발생 시 오류 메시지를 출력하고 해당 오류를 반환하며 함수를 종료한다.
<br>pci_set_master(pdev): PCI 장치를 DMA 마스터로 설정한다.
<br><br>pf-&gt;pdev = pdev;
pci_set_drvdata(pdev, pf);
set_bit(ICE_DOWN, pf-&gt;state);
/* Disable service task until DOWN bit is cleared */
set_bit(ICE_SERVICE_DIS, pf-&gt;state);
<br>목적: PF 구조체에 PCI 장치 데이터를 설정하고 초기 상태를 설정한다.<br>설명:<br>
<br>pf-&gt;pdev = pdev: PF 구조체에 PCI 장치 데이터를 설정한다.
<br>pci_set_drvdata(pdev, pf): PCI 장치 데이터에 PF 구조체를 저장한다.
<br>set_bit(ICE_DOWN, pf-&gt;state): 초기 상태를 ICE_DOWN으로 설정한다.
<br>set_bit(ICE_SERVICE_DIS, pf-&gt;state): 서비스 작업을 비활성화한다.
<br><br>hw = &amp;pf-&gt;hw;
hw-&gt;hw_addr = pcim_iomap_table(pdev)[ICE_BAR0];
pci_save_state(pdev);

hw-&gt;back = pf;
hw-&gt;port_info = NULL;
hw-&gt;vendor_id = pdev-&gt;vendor;
hw-&gt;device_id = pdev-&gt;device;
pci_read_config_byte(pdev, PCI_REVISION_ID, &amp;hw-&gt;revision_id);
hw-&gt;subsystem_vendor_id = pdev-&gt;subsystem_vendor;
hw-&gt;subsystem_device_id = pdev-&gt;subsystem_device;
hw-&gt;bus.device = PCI_SLOT(pdev-&gt;devfn);
hw-&gt;bus.func = PCI_FUNC(pdev-&gt;devfn);
ice_set_ctrlq_len(hw);
<br>목적: 하드웨어 정보 구조체를 설정한다.<br>설명:<br>
<br>hw-&gt;hw_addr = pcim_iomap_table(pdev)[ICE_BAR0]: BAR0 주소를 매핑하여 하드웨어 주소를 설정한다.
<br>pci_save_state(pdev): PCI 상태를 저장한다.
<br>hw-&gt;back = pf: 하드웨어 구조체에 PF 구조체를 설정한다.
<br>hw-&gt;port_info = NULL: 포트 정보를 초기화한다.
<br>hw-&gt;vendor_id = pdev-&gt;vendor: 벤더 ID를 설정한다.
<br>hw-&gt;device_id = pdev-&gt;device: 디바이스 ID를 설정한다.
<br>pci_read_config_byte(pdev, PCI_REVISION_ID, &amp;hw-&gt;revision_id): PCI 리비전 ID를 읽어 설정한다.
<br>hw-&gt;subsystem_vendor_id = pdev-&gt;subsystem_vendor: 서브시스템 벤더 ID를 설정한다.
<br>hw-&gt;subsystem_device_id = pdev-&gt;subsystem_device: 서브시스템 디바이스 ID를 설정한다.
<br>hw-&gt;bus.device = PCI_SLOT(pdev-&gt;devfn): PCI 슬롯 정보를 설정한다.
<br>hw-&gt;bus.func = PCI_FUNC(pdev-&gt;devfn): PCI 함수 정보를 설정한다.
<br>ice_set_ctrlq_len(hw): 제어 큐 길이를 설정한다.
<br><br>pf-&gt;msg_enable = netif_msg_init(debug, ICE_DFLT_NETIF_M);

#ifndef CONFIG_DYNAMIC_DEBUG
if (debug &lt; -1)
    hw-&gt;debug_mask = debug;
#endif
<br>목적: 디버그 메시지 설정을 초기화한다.<br>설명:<br>
<br>pf-&gt;msg_enable = netif_msg_init(debug, ICE_DFLT_NETIF_M): 디버그 메시지 설정을 초기화한다.
<br><a href=".?query=tag:ifndef" class="tag" target="_blank" rel="noopener nofollow">#ifndef</a> CONFIG_DYNAMIC_DEBUG: 동적 디버그가 설정되지 않은 경우,

<br>if (debug &lt; -1) hw-&gt;debug_mask = debug;: 디버그 레벨을 설정한다.


<br><br> 	err = ice_init(pf);
	if (err)
		goto err_init;

	devl_lock(priv_to_devlink(pf));
	err = ice_load(pf);
	devl_unlock(priv_to_devlink(pf));
	if (err)
		goto err_load;

	err = ice_init_devlink(pf);
	if (err)
		goto err_init_devlink;
<br>목적: 다양한 초기화 함수를 호출하여 하드웨어 및 소프트웨어를 초기화한다.<br><br>return 0;
<br>목적: 함수가 성공적으로 완료되었음을 알린다.<br>설명: 모든 초기화가 성공적으로 완료되면 0을 반환하여 성공을 알린다.<br><br>err_init_devlink:
	devl_lock(priv_to_devlink(pf));
	ice_unload(pf);
	devl_unlock(priv_to_devlink(pf));
err_load:
	ice_deinit(pf);
err_init:
	pci_disable_device(pdev);
	return err;
<br>목적: 초기화 과정에서 오류가 발생했을 때 적절히 정리하고 오류를 반환한다.<br><br>ice_probe 함수는 PCI 장치 초기화의 핵심 부분으로, 가상 함수 확인, kdump 커널 지원, PCI 장치 활성화, PF 구조체 할당 및 초기화, DMA 설정, 디버그 설정, 다양한 초기화 함수 호출 등을 수행하며, 각 단계에서 발생하는 오류를 적절히 처리하고 정리한다. 이 함수는 NIC 드라이버가 시스템에 올바르게 등록되고 작동할 수 있도록 하는 중요한 역할을 한다.]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_probe().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_probe().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_process_skb_fields()]]></title><description><![CDATA[ 
 <br>/**
 * ice_process_skb_fields - Populate skb header fields from Rx descriptor
 * @rx_ring: Rx descriptor ring packet is being transacted on
 * @rx_desc: pointer to the EOP Rx descriptor
 * @skb: pointer to current skb being populated
 *
 * This function checks the ring, descriptor, and packet information in
 * order to populate the hash, checksum, VLAN, protocol, and
 * other fields within the skb.
 */
void
ice_process_skb_fields(struct ice_rx_ring *rx_ring,
		       union ice_32b_rx_flex_desc *rx_desc,
		       struct sk_buff *skb)
{
	u16 ptype = ice_get_ptype(rx_desc);

	ice_rx_hash_to_skb(rx_ring, rx_desc, skb, ptype);

	/* modifies the skb - consumes the enet header */
	skb-&gt;protocol = eth_type_trans(skb, rx_ring-&gt;netdev);

	ice_rx_csum(rx_ring, skb, rx_desc, ptype);

	if (rx_ring-&gt;ptp_rx)
		ice_ptp_rx_hwts_to_skb(rx_ring, rx_desc, skb);
}
<br>
skb 필드를 처리하기 위해 사용되는 함수. ring, descriptor, packet informaion등의 정보를 체크하고, hash, checksum, VLAN, protocol 등의 필드를 채우게 된다.
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_process_skb_fields().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_process_skb_fields().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_put_rx_buf()]]></title><description><![CDATA[ 
 <br>/**
 * ice_put_rx_buf - Clean up used buffer and either recycle or free
 * @rx_ring: Rx descriptor ring to transact packets on
 * @rx_buf: Rx buffer to pull data from
 *
 * This function will clean up the contents of the rx_buf. It will either
 * recycle the buffer or unmap it and free the associated resources.
 */
static void
ice_put_rx_buf(struct ice_rx_ring *rx_ring, struct ice_rx_buf *rx_buf)
{
	if (!rx_buf)
		return;

	if (ice_can_reuse_rx_page(rx_buf)) {
		/* hand second half of page back to the ring */
		ice_reuse_rx_page(rx_ring, rx_buf);
	} else {
		/* we are not reusing the buffer so unmap it */
		dma_unmap_page_attrs(rx_ring-&gt;dev, rx_buf-&gt;dma,
				     ice_rx_pg_size(rx_ring), DMA_FROM_DEVICE,
				     ICE_RX_DMA_ATTR);
		__page_frag_cache_drain(rx_buf-&gt;page, rx_buf-&gt;pagecnt_bias);
	}

	/* clear contents of buffer_info */
	rx_buf-&gt;page = NULL;
}
<br>
napi를 통해 skb로 올라간 뒤, 다 쓰여진 버퍼를 재활용하거나 아니면 할당 해제 해주는 함수. 만약 재사용이 가능하다면 ice_reuse_rx_page(rx_ring, rx_buf)를 통하여 사용하게 된다. 아니라면, dma_unmap_page_attrs를 통해 unmap을 하게 된다.
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_put_rx_buf().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_put_rx_buf().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_receive_skb()]]></title><description><![CDATA[ 
 <br>/**
 * ice_receive_skb - Send a completed packet up the stack
 * @rx_ring: Rx ring in play
 * @skb: packet to send up
 * @vlan_tci: VLAN TCI for packet
 *
 * This function sends the completed packet (via. skb) up the stack using
 * gro receive functions (with/without VLAN tag)
 */
void
ice_receive_skb(struct ice_rx_ring *rx_ring, struct sk_buff *skb, u16 vlan_tci) // [[ice_rx_ring]] [[Encyclopedia of NetworkSystem/Struct/include-linux/sk_buff|sk_buff]]
{
	if ((vlan_tci &amp; VLAN_VID_MASK) &amp;&amp; rx_ring-&gt;vlan_proto)
		__vlan_hwaccel_put_tag(skb, rx_ring-&gt;vlan_proto,
				       vlan_tci);

	napi_gro_receive(&amp;rx_ring-&gt;q_vector-&gt;napi, skb); // [[napi_gro_receive()]]
}
<br><a data-href="ice_rx_ring" href="encyclopedia-of-networksystem/struct/drivers-net-ethernet-intel-ice/ice_rx_ring.html" class="internal-link" target="_self" rel="noopener nofollow">ice_rx_ring</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Struct/include-linux/sk_buff" data-href="Encyclopedia of NetworkSystem/Struct/include-linux/sk_buff" href="encyclopedia-of-networksystem/struct/include-linux/sk_buff.html" class="internal-link" target="_self" rel="noopener nofollow">sk_buff</a><br>
<a data-href="napi_gro_receive()" href="encyclopedia-of-networksystem/function/net-core/napi_gro_receive().html" class="internal-link" target="_self" rel="noopener nofollow">napi_gro_receive()</a><br>
처리가 완료된 패킷을 스택으로 올려보내주는 역할을 하게 된다. napi_gro_receive() 호출하는 함수이다. gro가 가능하다면, gro 처리를 하여 skb를 만들게 된다.
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_receive_skb().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_receive_skb().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_req_irq_msix_misc()]]></title><description><![CDATA[ 
 <br>/**
 * ice_req_irq_msix_misc - Setup the misc vector to handle non queue events
 * @pf: board private structure
 *
 * This sets up the handler for MSIX 0, which is used to manage the
 * non-queue interrupts, e.g. AdminQ and errors. This is not used
 * when in MSI or Legacy interrupt mode.
 */
static int ice_req_irq_msix_misc(struct ice_pf *pf)
{
	struct device *dev = ice_pf_to_dev(pf);
	struct ice_hw *hw = &amp;pf-&gt;hw;
	u32 pf_intr_start_offset;
	struct msi_map irq;
	int err = 0;

	if (!pf-&gt;int_name[0])
		snprintf(pf-&gt;int_name, sizeof(pf-&gt;int_name) - 1, "%s-%s:misc",
			 dev_driver_string(dev), dev_name(dev));

	if (!pf-&gt;int_name_ll_ts[0])
		snprintf(pf-&gt;int_name_ll_ts, sizeof(pf-&gt;int_name_ll_ts) - 1,
			 "%s-%s:ll_ts", dev_driver_string(dev), dev_name(dev));
	/* Do not request IRQ but do enable OICR interrupt since settings are
	 * lost during reset. Note that this function is called only during
	 * rebuild path and not while reset is in progress.
	 */
	if (ice_is_reset_in_progress(pf-&gt;state))
		goto skip_req_irq;

	/* reserve one vector in irq_tracker for misc interrupts */
	irq = ice_alloc_irq(pf, false);
	if (irq.index &lt; 0)
		return irq.index;

	pf-&gt;oicr_irq = irq;
	err = devm_request_threaded_irq(dev, pf-&gt;oicr_irq.virq, ice_misc_intr,
					ice_misc_intr_thread_fn, 0,
					pf-&gt;int_name, pf);
	if (err) {
		dev_err(dev, "devm_request_threaded_irq for %s failed: %d\n",
			pf-&gt;int_name, err);
		ice_free_irq(pf, pf-&gt;oicr_irq);
		return err;
	}

	/* reserve one vector in irq_tracker for ll_ts interrupt */
	if (!pf-&gt;hw.dev_caps.ts_dev_info.ts_ll_int_read)
		goto skip_req_irq;

	irq = ice_alloc_irq(pf, false);
	if (irq.index &lt; 0)
		return irq.index;

	pf-&gt;ll_ts_irq = irq;
	err = devm_request_irq(dev, pf-&gt;ll_ts_irq.virq, ice_ll_ts_intr, 0,
			       pf-&gt;int_name_ll_ts, pf);
	if (err) {
		dev_err(dev, "devm_request_irq for %s failed: %d\n",
			pf-&gt;int_name_ll_ts, err);
		ice_free_irq(pf, pf-&gt;ll_ts_irq);
		return err;
	}

skip_req_irq:
	ice_ena_misc_vector(pf);

	ice_ena_ctrlq_interrupts(hw, pf-&gt;oicr_irq.index);
	/* This enables LL TS interrupt */
	pf_intr_start_offset = rd32(hw, PFINT_ALLOC) &amp; PFINT_ALLOC_FIRST;
	if (pf-&gt;hw.dev_caps.ts_dev_info.ts_ll_int_read)
		wr32(hw, PFINT_SB_CTL,
		     ((pf-&gt;ll_ts_irq.index + pf_intr_start_offset) &amp;
		      PFINT_SB_CTL_MSIX_INDX_M) | PFINT_SB_CTL_CAUSE_ENA_M);
	wr32(hw, GLINT_ITR(ICE_RX_ITR, pf-&gt;oicr_irq.index),
	     ITR_REG_ALIGN(ICE_ITR_8K) &gt;&gt; ICE_ITR_GRAN_S);

	ice_flush(hw);
	ice_irq_dynamic_ena(hw, NULL, NULL);

	return 0;
}
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_req_irq_msix_misc().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_req_irq_msix_misc().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_request_fw()]]></title><description><![CDATA[ 
 <br>/**
 * ice_request_fw - Device initialization routine
 * @pf: pointer to the PF instance
 */
static void ice_request_fw(struct ice_pf *pf)
{
	char *opt_fw_filename = ice_get_opt_fw_name(pf);
	const struct firmware *firmware = NULL;
	struct device *dev = ice_pf_to_dev(pf);
	int err = 0;

	/* optional device-specific DDP (if present) overrides the default DDP
	 * package file. kernel logs a debug message if the file doesn't exist,
	 * and warning messages for other errors.
	 */
	if (opt_fw_filename) {
		err = firmware_request_nowarn(&amp;firmware, opt_fw_filename, dev);
		if (err) {
			kfree(opt_fw_filename);
			goto dflt_pkg_load;
		}

		/* request for firmware was successful. Download to device */
		ice_load_pkg(firmware, pf);
		kfree(opt_fw_filename);
		release_firmware(firmware);
		return;
	}

dflt_pkg_load:
	err = request_firmware(&amp;firmware, ICE_DDP_PKG_FILE, dev);
	if (err) {
		dev_err(dev, "The DDP package file was not found or could not be read. Entering Safe Mode\n");
		return;
	}

	/* request for firmware was successful. Download to device */
	ice_load_pkg(firmware, pf);
	release_firmware(firmware);
}
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_request_fw().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_request_fw().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_ring_is_xdp()]]></title><description><![CDATA[ 
 <br>static inline bool ice_ring_is_xdp(struct ice_tx_ring *ring)
{
	return !!(ring-&gt;flags &amp; ICE_TX_FLAGS_RING_XDP);
}
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_ring_is_xdp().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_ring_is_xdp().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_ring_uses_build_skb()]]></title><description><![CDATA[ 
 <br>static inline bool ice_ring_uses_build_skb(struct ice_rx_ring *ring)
{
	return !!(ring-&gt;flags &amp; ICE_RX_FLAGS_RING_BUILD_SKB);
}
<br>
간단한 인라인 함수이다. rx_ring의 flag에서 ICE_RX_FLAGS_RINGS_BUILD_SKB가 켜져 있는지만 확인하는 함수이다.
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_ring_uses_build_skb().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_ring_uses_build_skb().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_run_xdp()]]></title><description><![CDATA[ 
 <br>/**
 * ice_run_xdp - Executes an XDP program on initialized xdp_buff
 * @rx_ring: Rx ring
 * @xdp: xdp_buff used as input to the XDP program
 * @xdp_prog: XDP program to run
 * @xdp_ring: ring to be used for XDP_TX action
 * @rx_buf: Rx buffer to store the XDP action
 * @eop_desc: Last descriptor in packet to read metadata from
 *
 * Returns any of ICE_XDP_{PASS, CONSUMED, TX, REDIR}
 */
static void
ice_run_xdp(struct ice_rx_ring *rx_ring, struct xdp_buff *xdp,
	    struct bpf_prog *xdp_prog, struct ice_tx_ring *xdp_ring,
	    struct ice_rx_buf *rx_buf, union ice_32b_rx_flex_desc *eop_desc)
{
	unsigned int ret = ICE_XDP_PASS;
	u32 act;

	if (!xdp_prog)
		goto exit;

	ice_xdp_meta_set_desc(xdp, eop_desc);

	act = bpf_prog_run_xdp(xdp_prog, xdp);
	switch (act) {
	case XDP_PASS:
		break;
	case XDP_TX:
		if (static_branch_unlikely(&amp;ice_xdp_locking_key))
			spin_lock(&amp;xdp_ring-&gt;tx_lock);
		ret = __ice_xmit_xdp_ring(xdp, xdp_ring, false); // [[__ice_xmit_xdp_ring()]]
		if (static_branch_unlikely(&amp;ice_xdp_locking_key))
			spin_unlock(&amp;xdp_ring-&gt;tx_lock);
		if (ret == ICE_XDP_CONSUMED)
			goto out_failure;
		break;
	case XDP_REDIRECT:
		if (xdp_do_redirect(rx_ring-&gt;netdev, xdp, xdp_prog))
			goto out_failure;
		ret = ICE_XDP_REDIR;
		break;
	default:
		bpf_warn_invalid_xdp_action(rx_ring-&gt;netdev, xdp_prog, act);
		fallthrough;
	case XDP_ABORTED:
out_failure:
		trace_xdp_exception(rx_ring-&gt;netdev, xdp_prog, act);
		fallthrough;
	case XDP_DROP:
		ret = ICE_XDP_CONSUMED;
	}
exit:
	ice_set_rx_bufs_act(xdp, rx_ring, ret);
}
<br><a data-href="__ice_xmit_xdp_ring()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/__ice_xmit_xdp_ring().html" class="internal-link" target="_self" rel="noopener nofollow">__ice_xmit_xdp_ring()</a><br>
bpf_prog_run_xdp(xdp_prog, xdp) 함수를 를 통해서 act를 얻게 되고, 이걸 바탕으로 switch문으로 들어가게 된다. XDP_PASS, XDP_TX, XDP_REDIRECT, XDP_ABORTED, XDP_DROP 등의 enum이 있으며, 가운데 bpf_prog_run_xdp()를 통해 bpf가 실행되는데, 여기서 XDP_PASS act가 되어 ret이 ICE_XDP_PASS가 되면 네트워크 스택을 지나지 않는 것이고, 아니라면 XDP_DROP에서 ret이 ICE_XDP_CONSUMED로 전통적인 네트워크 스택을 지나게 될 것이다.
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_run_xdp().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_run_xdp().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_send_version()]]></title><description><![CDATA[ 
 <br>/**
 * ice_send_version - update firmware with driver version
 * @pf: PF struct
 *
 * Returns 0 on success, else error code
 */
static int ice_send_version(struct ice_pf *pf)
{
	struct ice_driver_ver dv;

	dv.major_ver = 0xff;
	dv.minor_ver = 0xff;
	dv.build_ver = 0xff;
	dv.subbuild_ver = 0;
	strscpy((char *)dv.driver_string, UTS_RELEASE,
		sizeof(dv.driver_string));
	return ice_aq_send_driver_ver(&amp;pf-&gt;hw, &amp;dv, NULL);
}
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_send_version().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_send_version().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_set_cpu_rx_rmap()]]></title><description><![CDATA[ 
 <br>/**
 * ice_set_cpu_rx_rmap - setup CPU reverse map for each queue
 * @vsi: the VSI to be forwarded to
 */
int ice_set_cpu_rx_rmap(struct ice_vsi *vsi)
{
	struct net_device *netdev;
	struct ice_pf *pf;
	int i;

	if (!vsi || vsi-&gt;type != ICE_VSI_PF)
		return 0;

	pf = vsi-&gt;back;
	netdev = vsi-&gt;netdev;
	if (!pf || !netdev || !vsi-&gt;num_q_vectors)
		return -EINVAL;

	netdev_dbg(netdev, "Setup CPU RMAP: vsi type 0x%x, ifname %s, q_vectors %d\n",
		   vsi-&gt;type, netdev-&gt;name, vsi-&gt;num_q_vectors);

	netdev-&gt;rx_cpu_rmap = alloc_irq_cpu_rmap(vsi-&gt;num_q_vectors);
	if (unlikely(!netdev-&gt;rx_cpu_rmap))
		return -EINVAL;

	ice_for_each_q_vector(vsi, i)
		if (irq_cpu_rmap_add(netdev-&gt;rx_cpu_rmap,
				     vsi-&gt;q_vectors[i]-&gt;irq.virq)) {
			ice_free_cpu_rx_rmap(vsi);
			return -EINVAL;
		}

	return 0;
}
<br>각각의 queue에 대하여 aRFS를 위해 매핑하는 함수.]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_set_cpu_rx_rmap().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_set_cpu_rx_rmap().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_set_rx_bufs_act()]]></title><description><![CDATA[ 
 <br>/**
* ice_set_rx_bufs_act - propagate Rx buffer action to frags
* @xdp: XDP buffer representing frame (linear and frags part)
* @rx_ring: Rx ring struct
* act: action to store onto Rx buffers related to XDP buffer parts
*
* Set action that should be taken before putting Rx buffer from first frag
* to the last.
*/
static inline void
ice_set_rx_bufs_act(struct xdp_buff *xdp, const struct ice_rx_ring *rx_ring,
			const unsigned int act)
{
	u32 sinfo_frags = xdp_get_shared_info_from_buff(xdp)-&gt;nr_frags;
	u32 nr_frags = rx_ring-&gt;nr_frags + 1;
	u32 idx = rx_ring-&gt;first_desc;
	u32 cnt = rx_ring-&gt;count;
	struct ice_rx_buf *buf;
	  
	for (int i = 0; i &lt; nr_frags; i++) {
		buf = &amp;rx_ring-&gt;rx_buf[idx];
		buf-&gt;act = act;
		  
		if (++idx == cnt)
			idx = 0;
}

	/* adjust pagecnt_bias on frags freed by XDP prog */
	if (sinfo_frags &lt; rx_ring-&gt;nr_frags &amp;&amp; act == ICE_XDP_CONSUMED) {
		u32 delta = rx_ring-&gt;nr_frags - sinfo_frags;
		  
		while (delta) {
			if (idx == 0)
				idx = cnt - 1;
			else
				idx--;
			buf = &amp;rx_ring-&gt;rx_buf[idx];
			buf-&gt;pagecnt_bias--;
			delta--;
		}
	}
}
<br>
해당하는 패킷에 대하여 rx_buf 에 해당하는 xdp의 반환 값을 저장해주는 함수이다. xdp_get_shared_info_from_buff() 함수의 경우 xdp가 가르키고 있는 data 영역의 맨 마지막을 리턴하게 되는데, 이 함수는 include/net/xdp.h에 있다. data 영역의 시작 부분과 frame size를 통해 마지막에 달라 붙어 있는 skb_shared_info를 가르키는 포인터를 반환 할 수 밖에 없을 것이다.<br>
frag의 갯수만큼 for문을 돌려서 rx_buf array 중 idx로 해당하는 rx_buf에 접근하여 그 buf의 act를 인수로 받은 act로 설정하게 된다.
추가적으로 만약 XDP program에 의해 freed 된 frags가 있다면 이를 반영하기 위해 pagecnt_bias를 조정해주는 코드도 있다. 이는 해당 링 버퍼에서 pagecnt_bias를 그 차이만큼 줄여주는 역할을 한다.
<br>
first_desc는 링 구조에서 첫 번째 디스크립터의 인덱스 번호를 가르키고 있고, rx_ring의 nr_frags는 가지고 있는 유효한 디스크립터의 갯수를 의미한다. 또한, count는 전체(최대) 가질 수 있는 링 디스크립터의 수로, 여기서는 rx_buf array의 선언된 entry 최대 갯수라고 볼 수 있다.
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_set_rx_bufs_act().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_set_rx_bufs_act().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_setup_rx_ring()]]></title><description><![CDATA[ 
 <br>/**
 * ice_setup_rx_ring - Allocate the Rx descriptors
 * @rx_ring: the Rx ring to set up
 *
 * Return 0 on success, negative on error
 */
int ice_setup_rx_ring(struct ice_rx_ring *rx_ring)
{
	struct device *dev = rx_ring-&gt;dev;
	u32 size;

	if (!dev)
		return -ENOMEM;

	/* warn if we are about to overwrite the pointer */
	WARN_ON(rx_ring-&gt;rx_buf);
	rx_ring-&gt;rx_buf =
		kcalloc(rx_ring-&gt;count, sizeof(*rx_ring-&gt;rx_buf), GFP_KERNEL);
	if (!rx_ring-&gt;rx_buf)
		return -ENOMEM;

	/* round up to nearest page */
	size = ALIGN(rx_ring-&gt;count * sizeof(union ice_32byte_rx_desc),
		     PAGE_SIZE);
	rx_ring-&gt;desc = dmam_alloc_coherent(dev, size, &amp;rx_ring-&gt;dma,
					    GFP_KERNEL); // [[dmam_alloc_coherent()]]
	if (!rx_ring-&gt;desc) {
		dev_err(dev, "Unable to allocate memory for the Rx descriptor ring, size=%d\n",
			size);
		goto err;
	}

	rx_ring-&gt;next_to_use = 0;
	rx_ring-&gt;next_to_clean = 0;
	rx_ring-&gt;first_desc = 0;

	if (ice_is_xdp_ena_vsi(rx_ring-&gt;vsi))
		WRITE_ONCE(rx_ring-&gt;xdp_prog, rx_ring-&gt;vsi-&gt;xdp_prog);

	return 0;

err:
	kfree(rx_ring-&gt;rx_buf);
	rx_ring-&gt;rx_buf = NULL;
	return -ENOMEM;
}
<br><a data-href="dmam_alloc_coherent()" href="encyclopedia-of-networksystem/function/include-linux/dmam_alloc_coherent().html" class="internal-link" target="_self" rel="noopener nofollow">dmam_alloc_coherent()</a><br>rx_ring→rx_buf를 kcalloc을 통해 새로 선언하게 됨. 이때, rx_ring→count만큼의 배열로 선언되게 됨.<br>이후 ALIGN을 통해 PAGE_SIZE 크기의 단위로 몇 바이트가 필요한지 계산하여 size에 저장하게 됨. 이때, 만약 4500바이트의 rx_buf array이고, PAGE_SIZE가 4096이라면, size는 8182가 됨. 이는 ice_32byte_rx_desc라는 구조체를 할당하기 위함임. rx_ring→dma에 해당 주소를 할당하며, 이 때 담고자 하는 정보는 각각의 버퍼에 대한 디스크립터임.<br>만들어진 모든 rx_ring은 ice_main.c의 ice_vsi_setup_rx_rings 에서 vsi→rx_rings[i]를 가르키는 포인터에서 수행하므로 결과적으로 vsi 의 rx_rings 배열의 rx_ring들을 하나하나 설정하게 됨.]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_setup_rx_ring().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_setup_rx_ring().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_setup_tx_ring()]]></title><description><![CDATA[ 
 <br>/**
 * ice_setup_tx_ring - Allocate the Tx descriptors
 * @tx_ring: the Tx ring to set up
 *
 * Return 0 on success, negative on error
 */
int ice_setup_tx_ring(struct ice_tx_ring *tx_ring) // [[ice_tx_ring]]
{
	struct device *dev = tx_ring-&gt;dev;
	u32 size;

	if (!dev)
		return -ENOMEM;

	/* warn if we are about to overwrite the pointer */
	WARN_ON(tx_ring-&gt;tx_buf);
	tx_ring-&gt;tx_buf =
		devm_kcalloc(dev, sizeof(*tx_ring-&gt;tx_buf), tx_ring-&gt;count,
			     GFP_KERNEL); // [[devm_kcalloc()]]
	if (!tx_ring-&gt;tx_buf)
		return -ENOMEM;

	/* round up to nearest page */
	size = ALIGN(tx_ring-&gt;count * sizeof(struct ice_tx_desc),
		     PAGE_SIZE);
	tx_ring-&gt;desc = dmam_alloc_coherent(dev, size, &amp;tx_ring-&gt;dma,
					    GFP_KERNEL); // [[dmam_alloc_coherent()]]
	if (!tx_ring-&gt;desc) {
		dev_err(dev, "Unable to allocate memory for the Tx descriptor ring, size=%d\n",
			size);
		goto err;
	}

	tx_ring-&gt;next_to_use = 0;
	tx_ring-&gt;next_to_clean = 0;
	tx_ring-&gt;ring_stats-&gt;tx_stats.prev_pkt = -1;
	return 0;

err:
	devm_kfree(dev, tx_ring-&gt;tx_buf);
	tx_ring-&gt;tx_buf = NULL;
	return -ENOMEM;
}
<br><a data-href="ice_tx_ring" href="encyclopedia-of-networksystem/struct/drivers-net-ethernet-intel-ice/ice_tx_ring.html" class="internal-link" target="_self" rel="noopener nofollow">ice_tx_ring</a><br>
<a data-href="devm_kcalloc()" href="encyclopedia-of-networksystem/function/include-linux/devm_kcalloc().html" class="internal-link" target="_self" rel="noopener nofollow">devm_kcalloc()</a><br>
<a data-href="dmam_alloc_coherent()" href="encyclopedia-of-networksystem/function/include-linux/dmam_alloc_coherent().html" class="internal-link" target="_self" rel="noopener nofollow">dmam_alloc_coherent()</a>]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_setup_tx_ring().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_setup_tx_ring().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_up_complete()]]></title><description><![CDATA[ 
 <br>/**
 * ice_up_complete - Finish the last steps of bringing up a connection
 * @vsi: The VSI being configured
 *
 * Return 0 on success and negative value on error
 */
static int ice_up_complete(struct ice_vsi *vsi)
{
	struct ice_pf *pf = vsi-&gt;back;
	int err;

	ice_vsi_cfg_msix(vsi);

	/* Enable only Rx rings, Tx rings were enabled by the FW when the
	 * Tx queue group list was configured and the context bits were
	 * programmed using ice_vsi_cfg_txqs
	 */
	err = ice_vsi_start_all_rx_rings(vsi); // [[ice_vsi_start_all_rx_rings()]]
	if (err)
		return err;

	clear_bit(ICE_VSI_DOWN, vsi-&gt;state);
	ice_napi_enable_all(vsi); // [[ice_napi_enable_all()]]
	ice_vsi_ena_irq(vsi); // [[ice_vsi_ena_irq()]]

	if (vsi-&gt;port_info &amp;&amp;
	    (vsi-&gt;port_info-&gt;phy.link_info.link_info &amp; ICE_AQ_LINK_UP) &amp;&amp;
	    vsi-&gt;netdev &amp;&amp; vsi-&gt;type == ICE_VSI_PF) {
		ice_print_link_msg(vsi, true);
		netif_tx_start_all_queues(vsi-&gt;netdev); // [[netif_tx_start_all_queues()]]
		netif_carrier_on(vsi-&gt;netdev); // [[netif_carrier_on()]]
		ice_ptp_link_change(pf, pf-&gt;hw.pf_id, true);
	}

	/* Perform an initial read of the statistics registers now to
	 * set the baseline so counters are ready when interface is up
	 */
	ice_update_eth_stats(vsi);

	if (vsi-&gt;type == ICE_VSI_PF)
		ice_service_task_schedule(pf);

	return 0;
}
<br><a data-href="ice_vsi_start_all_rx_rings()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_start_all_rx_rings().html" class="internal-link" target="_self" rel="noopener nofollow">ice_vsi_start_all_rx_rings()</a><br>
<a data-href="ice_napi_enable_all()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_napi_enable_all().html" class="internal-link" target="_self" rel="noopener nofollow">ice_napi_enable_all()</a><br>
<a data-href="ice_vsi_ena_irq()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_ena_irq().html" class="internal-link" target="_self" rel="noopener nofollow">ice_vsi_ena_irq()</a><br>
<a data-href="netif_tx_start_all_queues()" href="encyclopedia-of-networksystem/function/include-linux/netif_tx_start_all_queues().html" class="internal-link" target="_self" rel="noopener nofollow">netif_tx_start_all_queues()</a><br>
<a data-href="netif_carrier_on()" href="encyclopedia-of-networksystem/function/net-sched/netif_carrier_on().html" class="internal-link" target="_self" rel="noopener nofollow">netif_carrier_on()</a><br>최종적으로 다 열렸는지 확인함.]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_up_complete().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_up_complete().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_update_rx_ring_stats()]]></title><description><![CDATA[ 
 <br>/**
 * ice_update_rx_ring_stats - Update Rx ring specific counters
 * @rx_ring: ring to update
 * @pkts: number of processed packets
 * @bytes: number of processed bytes
 */
void ice_update_rx_ring_stats(struct ice_rx_ring *rx_ring, u64 pkts, u64 bytes)
{
	u64_stats_update_begin(&amp;rx_ring-&gt;ring_stats-&gt;syncp);
	ice_update_ring_stats(&amp;rx_ring-&gt;ring_stats-&gt;stats, pkts, bytes);
	u64_stats_update_end(&amp;rx_ring-&gt;ring_stats-&gt;syncp);
}
<br>
rx ring의 상태를 업데이트하는 코드
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_update_rx_ring_stats().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_update_rx_ring_stats().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_verify_cacheline_size()]]></title><description><![CDATA[ 
 <br>/**
 * ice_verify_cacheline_size - verify driver's assumption of 64 Byte cache lines
 * @pf: pointer to the PF structure
 *
 * There is no error returned here because the driver should be able to handle
 * 128 Byte cache lines, so we only print a warning in case issues are seen,
 * specifically with Tx.
 */
static void ice_verify_cacheline_size(struct ice_pf *pf)
{
	if (rd32(&amp;pf-&gt;hw, GLPCI_CNF2) &amp; GLPCI_CNF2_CACHELINE_SIZE_M)
		dev_warn(ice_pf_to_dev(pf), "%d Byte cache line assumption is invalid, driver may have Tx timeouts!\n",
			 ICE_CACHE_LINE_BYTES);
}
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_verify_cacheline_size().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_verify_cacheline_size().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_vsi_alloc_def()]]></title><description><![CDATA[ 
 <br>static int
ice_vsi_alloc_def(struct ice_vsi *vsi, struct ice_channel *ch)
{
	if (vsi-&gt;type != ICE_VSI_CHNL) {
		ice_vsi_set_num_qs(vsi);
		if (ice_vsi_alloc_arrays(vsi))
			return -ENOMEM;
	}

	switch (vsi-&gt;type) {
	case ICE_VSI_SWITCHDEV_CTRL:
		/* Setup eswitch MSIX irq handler for VSI */
		vsi-&gt;irq_handler = ice_eswitch_msix_clean_rings;
		break;
	case ICE_VSI_PF:
		/* Setup default MSIX irq handler for VSI */
		vsi-&gt;irq_handler = ice_msix_clean_rings; // [[ice_msix_clean_rings()]]
		break;
	case ICE_VSI_CTRL:
		/* Setup ctrl VSI MSIX irq handler */
		vsi-&gt;irq_handler = ice_msix_clean_ctrl_vsi;
		break;
	case ICE_VSI_CHNL:
		if (!ch)
			return -EINVAL;

		vsi-&gt;num_rxq = ch-&gt;num_rxq;
		vsi-&gt;num_txq = ch-&gt;num_txq;
		vsi-&gt;next_base_q = ch-&gt;base_q;
		break;
	case ICE_VSI_VF:
	case ICE_VSI_LB:
		break;
	default:
		ice_vsi_free_arrays(vsi);
		return -EINVAL;
	}

	return 0;
}
<br><a data-href="ice_msix_clean_rings()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_msix_clean_rings().html" class="internal-link" target="_self" rel="noopener nofollow">ice_msix_clean_rings()</a><br>vsi의 타입에 따라 irq_handler가 정의되고 있는 함수임.<br>vsi의 종류에 따라 irq_handler가 다르게 allocation되고 있는것이 보임. 가장 기본적으로는 ice_msix_clean_rings() 라는 함수가 할당 되게 됨. irq_handler는 함수 포인터임. 이 함수는 같은 파일인 ice/ice_lib.c에 존재함.]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_alloc_def().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_vsi_alloc_def().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_vsi_alloc_q_vector()]]></title><description><![CDATA[ 
 <br>/**
 * ice_vsi_alloc_q_vector - Allocate memory for a single interrupt vector
 * @vsi: the VSI being configured
 * @v_idx: index of the vector in the VSI struct
 *
 * We allocate one q_vector and set default value for ITR setting associated
 * with this q_vector. If allocation fails we return -ENOMEM.
 */
static int ice_vsi_alloc_q_vector(struct ice_vsi *vsi, u16 v_idx)
{
	struct ice_pf *pf = vsi-&gt;back;
	struct ice_q_vector *q_vector; // [[ice_q_vector]]
	int err;

	/* allocate q_vector */
	q_vector = kzalloc(sizeof(*q_vector), GFP_KERNEL); // [[kzalloc()]]
	if (!q_vector)
		return -ENOMEM;

	q_vector-&gt;vsi = vsi;
	q_vector-&gt;v_idx = v_idx;
	q_vector-&gt;tx.itr_setting = ICE_DFLT_TX_ITR;
	q_vector-&gt;rx.itr_setting = ICE_DFLT_RX_ITR;
	q_vector-&gt;tx.itr_mode = ITR_DYNAMIC;
	q_vector-&gt;rx.itr_mode = ITR_DYNAMIC;
	q_vector-&gt;tx.type = ICE_TX_CONTAINER;
	q_vector-&gt;rx.type = ICE_RX_CONTAINER;
	q_vector-&gt;irq.index = -ENOENT;

	if (vsi-&gt;type == ICE_VSI_VF) {
		q_vector-&gt;reg_idx = ice_calc_vf_reg_idx(vsi-&gt;vf, q_vector);
		goto out;
	} else if (vsi-&gt;type == ICE_VSI_CTRL &amp;&amp; vsi-&gt;vf) {
		struct ice_vsi *ctrl_vsi = ice_get_vf_ctrl_vsi(pf, vsi);

		if (ctrl_vsi) {
			if (unlikely(!ctrl_vsi-&gt;q_vectors)) {
				err = -ENOENT;
				goto err_free_q_vector;
			}

			q_vector-&gt;irq = ctrl_vsi-&gt;q_vectors[0]-&gt;irq;
			goto skip_alloc;
		}
	}

	q_vector-&gt;irq = ice_alloc_irq(pf, vsi-&gt;irq_dyn_alloc); // [[ice_alloc_irq()]]
	if (q_vector-&gt;irq.index &lt; 0) {
		err = -ENOMEM;
		goto err_free_q_vector;
	}

skip_alloc:
	q_vector-&gt;reg_idx = q_vector-&gt;irq.index;

	/* only set affinity_mask if the CPU is online */
	if (cpu_online(v_idx))
		cpumask_set_cpu(v_idx, &amp;q_vector-&gt;affinity_mask);

	/* This will not be called in the driver load path because the netdev
	 * will not be created yet. All other cases with register the NAPI
	 * handler here (i.e. resume, reset/rebuild, etc.)
	 */
	if (vsi-&gt;netdev)
		netif_napi_add(vsi-&gt;netdev, &amp;q_vector-&gt;napi, ice_napi_poll); // [[netif_napi_add()]] [[ice_napi_poll()]]
		//ice driver에서 사용하는 ice_napi_poll이라는 함수의 포인터를 전달하여 napi를 추가하는데 해당 함수포인터를 매핑하는 역할을 함. 
		//그런데 로드 될 때는 netdev가 없으므로 실행되지 않음. 따라서 앞에 if(vsi→netdev)가 붙게 됨.

out:
	/* tie q_vector and VSI together */
	vsi-&gt;q_vectors[v_idx] = q_vector;

	return 0;

err_free_q_vector:
	kfree(q_vector);

	return err;
}
<br><a data-href="ice_q_vector" href="encyclopedia-of-networksystem/struct/drivers-net-ethernet-intel-ice/ice_q_vector.html" class="internal-link" target="_self" rel="noopener nofollow">ice_q_vector</a><br>
<a data-href="kzalloc()" href="encyclopedia-of-networksystem/function/include-linux/kzalloc().html" class="internal-link" target="_self" rel="noopener nofollow">kzalloc()</a><br>
<a data-href="ice_alloc_irq()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_alloc_irq().html" class="internal-link" target="_self" rel="noopener nofollow">ice_alloc_irq()</a><br>
<a data-href="netif_napi_add()" href="encyclopedia-of-networksystem/function/include-linux/netif_napi_add().html" class="internal-link" target="_self" rel="noopener nofollow">netif_napi_add()</a>&nbsp;<br>
<a data-href="ice_napi_poll()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_napi_poll().html" class="internal-link" target="_self" rel="noopener nofollow">ice_napi_poll()</a><br>
<br>ice_probe() → ice_init() → ice_vsi_init() → ice vsi alloc q vector()
<br>q_vector을 위한 공간을 할당하는 함수이다<br>
a. q_vector 내부에 vsi, napi_struct, rx_queue, tx_queue 있다<br>
<a data-href="ice_q_vector" href="encyclopedia-of-networksystem/struct/drivers-net-ethernet-intel-ice/ice_q_vector.html" class="internal-link" target="_self" rel="noopener nofollow">ice_q_vector</a>
    struct ice_q_vector {
    	struct ice_vsi *vsi;
    
    	u16 v_idx;			/* index in the vsi-%3Eq_vector array. */
    	u16 reg_idx;
    	u8 num_ring_rx;			/* total number of Rx rings in vector */
    	u8 num_ring_tx;			/* total number of Tx rings in vector */
    	u8 wb_on_itr:1;			/* if true, WB on ITR is enabled */
    	/* in usecs, need to use ice_intrl_to_usecs_reg() before writing this
    	 * value to the device
    	 */
    	u8 intrl;
    
    	struct napi_struct napi;
    
    	struct ice_ring_container rx;
    	struct ice_ring_container tx;
    
    	cpumask_t affinity_mask;
    	struct irq_affinity_notify affinity_notify;
    
    	struct ice_channel *ch;
    
    	char name[ICE_INT_NAME_STR_LEN];
    
    	u16 total_events;	/* net_dim(): number of interrupts processed */
    	struct msi_map irq;
    } ____cacheline_internodealigned_in_smp;


<br>       
3. 이미 vsi에 q_vector가 존재하면 넘어간다
4. q_vector = [[Encyclopedia of NetworkSystem/Function/include-linux/kzalloc().md|kzalloc]](sizeof(`*q_vector`), GFP_KERNEL)
   a. q_vector를 kzalloc()을 통해 공간 할당한다
   b. kzalloc() = kmalloc() + memset(), 연속된 공간 할당 + 0으로 초기화
   c. GFP_KERNEL : get free page, kernel data struct, inode cache, DMAable mem 에 사용
       i. GFP_NOWAIT (atomic context에서 allocation 할때 사용)
       ii. GFP_NOWARN (Allocations which have a reasonable fallback should be using)
       iii. GFP_ATOMIC….
5. vf일 경우 따로 처리를 해준다
   a. ice_alloc_irq 안함
6. q_vector-&gt;irq = [[Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_alloc_irq().md|ice_alloc_irq]](pf, vsi-&gt;irq_dyn_alloc)
   a. q_vector 마다 irq를 할당한다
   [[ice_alloc_irq()]]
   ```c title=ice_alloc_irq()
       /**
        * ice_alloc_irq - Allocate new interrupt vector
        * @pf: board private structure
        * @dyn_only: force dynamic allocation of the interrupt
        *
        * Allocate new interrupt vector for a given owner id.
        * return struct msi_map with interrupt details and track
        * allocated interrupt appropriately.
        *
        * This function reserves new irq entry from the irq_tracker.
        * if according to the tracker information all interrupts that
        * were allocated with ice_pci_alloc_irq_vectors are already used
        * and dynamically allocated interrupts are supported then new
        * interrupt will be allocated with pci_msix_alloc_irq_at.
        *
        * Some callers may only support dynamically allocated interrupts.
        * This is indicated with dyn_only flag.
        *
        * On failure, return map with negative .index. The caller
        * is expected to check returned map index.
        *
        */
       struct msi_map ice_alloc_irq(struct ice_pf *pf, bool dyn_only)
       {
       	int sriov_base_vector = pf-&gt;sriov_base_vector;
       	struct msi_map map = { .index = -ENOENT };
       	struct device *dev = ice_pf_to_dev(pf);
       	struct ice_irq_entry *entry;
       
       	**entry = ice_get_irq_res(pf, dyn_only);**
       	if (!entry)
       		return map;
       
       	/* fail if we're about to violate SRIOV vectors space */
       	if (sriov_base_vector &amp;&amp; entry-&gt;index &gt;= sriov_base_vector)
       		goto exit_free_res;
       
       	if (pci_msix_can_alloc_dyn(pf-&gt;pdev) &amp;&amp; entry-&gt;dynamic) {
       		map = pci_msix_alloc_irq_at(pf-&gt;pdev, entry-&gt;index, NULL);
       		if (map.index %3C 0)
       			goto exit_free_res;
       		dev_dbg(dev, "allocated new irq at index %d\n", map.index);
       	} else {
       		map.index = entry-&gt;index;
       		map.virq = pci_irq_vector(pf-&gt;pdev, map.index);
       	}
       
       	return map;
       
       exit_free_res:
       	dev_err(dev, "Could not allocate irq at idx %d\n", entry-&gt;index);
       	ice_free_irq_res(pf, entry-&gt;index);
       	return map;
       }
<br>- msix (messeage signaled interrupt - x)
        - 권장되는 interrrupt 방식, 특히 multi rx queue NIC에서
        - 더 flexible 하다고 한다??
        - rx queue가 그것만의 hw interrupt를 할당할 수 있다. 이는 특정 cpu에 의해 관리된다?
        - This is because each RX queue can **have its own hardware interrupt assigned**, which can then be handled by a specific CPU (with&nbsp;`irqbalance`&nbsp;or by modifying&nbsp;`/proc/irq/IRQ_NUMBER/smp_affinity`) ???
- interrupt vector 를 할당해준다
    - interrupt vector는 interrupt handler 으로의 주소가 적혀있다
    - Interrupt는 0에서 255까지 숫자와 연결되어 있고 interrupt vector와도 연결되어 있다.
    - interrupt vector을 모아 놓은게 interrupt vector table
    - interrupt이 발생할 경우, 이에 맞는 interrupt handler를 실행할 수 있다.
<br>
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/include-linux/netif_napi_add().md" data-href="Encyclopedia of NetworkSystem/Function/include-linux/netif_napi_add().md" href="encyclopedia-of-networksystem/function/include-linux/netif_napi_add().html" class="internal-link" target="_self" rel="noopener nofollow">netif_napi_add</a>(vsi-&gt;netdev, &amp;q_vector-&gt;napi, ice_napi_poll);<br>
a. netdevice가 존재할 경우 실행<br>
b. <a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_napi_poll().md" data-href="Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_napi_poll().md" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_napi_poll().html" class="internal-link" target="_self" rel="noopener nofollow">ice_napi_poll()</a> 을 polling function으로 사용<br>
i. tx queue 초기화, budget 분배, rx queue 초기화
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_alloc_q_vector().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_vsi_alloc_q_vector().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_vsi_alloc_q_vectors()]]></title><description><![CDATA[ 
 <br>/**
 * ice_vsi_alloc_q_vectors - Allocate memory for interrupt vectors
 * @vsi: the VSI being configured
 *
 * We allocate one q_vector per queue interrupt. If allocation fails we
 * return -ENOMEM.
 */
int ice_vsi_alloc_q_vectors(struct ice_vsi *vsi)
{
	struct device *dev = ice_pf_to_dev(vsi-&gt;back);
	u16 v_idx;
	int err;

	if (vsi-&gt;q_vectors[0]) {
		dev_dbg(dev, "VSI %d has existing q_vectors\n", vsi-&gt;vsi_num);
		return -EEXIST;
	}

	for (v_idx = 0; v_idx &lt; vsi-&gt;num_q_vectors; v_idx++) {
		err = ice_vsi_alloc_q_vector(vsi, v_idx); // [[ice_vsi_alloc_q_vector()]]
		if (err)
			goto err_out;
	}

	return 0;

err_out:
	while (v_idx--)
		ice_free_q_vector(vsi, v_idx);

	dev_err(dev, "Failed to allocate %d q_vector for VSI %d, ret=%d\n",
		vsi-&gt;num_q_vectors, vsi-&gt;vsi_num, err);
	vsi-&gt;num_q_vectors = 0;
	return err;
}
<br><a data-href="ice_vsi_alloc_q_vector()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_alloc_q_vector().html" class="internal-link" target="_self" rel="noopener nofollow">ice_vsi_alloc_q_vector()</a>]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_alloc_q_vectors().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_vsi_alloc_q_vectors().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_vsi_alloc_ring_stats()]]></title><description><![CDATA[ 
 <br>/**
 * ice_vsi_alloc_ring_stats - Allocates Tx and Rx ring stats for the VSI
 * @vsi: VSI which is having stats allocated
 */
static int ice_vsi_alloc_ring_stats(struct ice_vsi *vsi)
{
	struct ice_ring_stats **tx_ring_stats;
	struct ice_ring_stats **rx_ring_stats;
	struct ice_vsi_stats *vsi_stats;
	struct ice_pf *pf = vsi-&gt;back;
	u16 i;

	vsi_stats = pf-&gt;vsi_stats[vsi-&gt;idx];
	tx_ring_stats = vsi_stats-&gt;tx_ring_stats;
	rx_ring_stats = vsi_stats-&gt;rx_ring_stats;

	/* Allocate Tx ring stats */
	ice_for_each_alloc_txq(vsi, i) {
		struct ice_ring_stats *ring_stats;
		struct ice_tx_ring *ring;

		ring = vsi-&gt;tx_rings[i];
		ring_stats = tx_ring_stats[i];

		if (!ring_stats) {
			ring_stats = kzalloc(sizeof(*ring_stats), GFP_KERNEL);
			if (!ring_stats)
				goto err_out;

			WRITE_ONCE(tx_ring_stats[i], ring_stats);
		}

		ring-&gt;ring_stats = ring_stats;
	}

	/* Allocate Rx ring stats */
	ice_for_each_alloc_rxq(vsi, i) {
		struct ice_ring_stats *ring_stats;
		struct ice_rx_ring *ring;

		ring = vsi-&gt;rx_rings[i];
		ring_stats = rx_ring_stats[i];

		if (!ring_stats) {
			ring_stats = kzalloc(sizeof(*ring_stats), GFP_KERNEL);
			if (!ring_stats)
				goto err_out;

			WRITE_ONCE(rx_ring_stats[i], ring_stats);
		}

		ring-&gt;ring_stats = ring_stats;
	}

	return 0;

err_out:
	ice_vsi_free_stats(vsi);
	return -ENOMEM;
}
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_alloc_ring_stats().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_vsi_alloc_ring_stats().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_vsi_alloc_rings()]]></title><description><![CDATA[ 
 <br>/**
 * ice_vsi_alloc_rings - Allocates Tx and Rx rings for the VSI
 * @vsi: VSI which is having rings allocated
 */
static int ice_vsi_alloc_rings(struct ice_vsi *vsi)
{
	bool dvm_ena = ice_is_dvm_ena(&amp;vsi-&gt;back-&gt;hw);
	struct ice_pf *pf = vsi-&gt;back;
	struct device *dev;
	u16 i;

	dev = ice_pf_to_dev(pf);
	/* Allocate Tx rings */
	ice_for_each_alloc_txq(vsi, i) {
		struct ice_tx_ring *ring;

		/* allocate with kzalloc(), free with kfree_rcu() */
		ring = kzalloc(sizeof(*ring), GFP_KERNEL);

		if (!ring)
			goto err_out;

		ring-&gt;q_index = i;
		ring-&gt;reg_idx = vsi-&gt;txq_map[i];
		ring-&gt;vsi = vsi;
		ring-&gt;tx_tstamps = &amp;pf-&gt;ptp.port.tx;
		ring-&gt;dev = dev;
		ring-&gt;count = vsi-&gt;num_tx_desc;
		ring-&gt;txq_teid = ICE_INVAL_TEID;
		if (dvm_ena)
			ring-&gt;flags |= ICE_TX_FLAGS_RING_VLAN_L2TAG2;
		else
			ring-&gt;flags |= ICE_TX_FLAGS_RING_VLAN_L2TAG1;
		WRITE_ONCE(vsi-&gt;tx_rings[i], ring);
	}

	/* Allocate Rx rings */
	ice_for_each_alloc_rxq(vsi, i) {
		struct ice_rx_ring *ring;

		/* allocate with kzalloc(), free with kfree_rcu() */
		ring = kzalloc(sizeof(*ring), GFP_KERNEL);
		if (!ring)
			goto err_out;

		ring-&gt;q_index = i;
		ring-&gt;reg_idx = vsi-&gt;rxq_map[i];
		ring-&gt;vsi = vsi;
		ring-&gt;netdev = vsi-&gt;netdev;
		ring-&gt;dev = dev;
		ring-&gt;count = vsi-&gt;num_rx_desc;
		ring-&gt;cached_phctime = pf-&gt;ptp.cached_phc_time;
		WRITE_ONCE(vsi-&gt;rx_rings[i], ring);
	}

	return 0;

err_out:
	ice_vsi_clear_rings(vsi);
	return -ENOMEM;
}
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_alloc_rings().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_vsi_alloc_rings().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_vsi_alloc_stat_arrays()]]></title><description><![CDATA[ 
 <br>/**
 * ice_vsi_alloc_stat_arrays - Allocate statistics arrays
 * @vsi: VSI pointer
 */
static int ice_vsi_alloc_stat_arrays(struct ice_vsi *vsi)
{
	struct ice_vsi_stats *vsi_stat;
	struct ice_pf *pf = vsi-&gt;back;

	if (vsi-&gt;type == ICE_VSI_CHNL)
		return 0;
	if (!pf-&gt;vsi_stats)
		return -ENOENT;

	if (pf-&gt;vsi_stats[vsi-&gt;idx])
	/* realloc will happen in rebuild path */
		return 0;

	vsi_stat = kzalloc(sizeof(*vsi_stat), GFP_KERNEL);
	if (!vsi_stat)
		return -ENOMEM;

	vsi_stat-&gt;tx_ring_stats =
		kcalloc(vsi-&gt;alloc_txq, sizeof(*vsi_stat-&gt;tx_ring_stats),
			GFP_KERNEL);
	if (!vsi_stat-&gt;tx_ring_stats)
		goto err_alloc_tx;

	vsi_stat-&gt;rx_ring_stats =
		kcalloc(vsi-&gt;alloc_rxq, sizeof(*vsi_stat-&gt;rx_ring_stats),
			GFP_KERNEL);
	if (!vsi_stat-&gt;rx_ring_stats)
		goto err_alloc_rx;

	pf-&gt;vsi_stats[vsi-&gt;idx] = vsi_stat;

	return 0;

err_alloc_rx:
	kfree(vsi_stat-&gt;rx_ring_stats);
err_alloc_tx:
	kfree(vsi_stat-&gt;tx_ring_stats);
	kfree(vsi_stat);
	pf-&gt;vsi_stats[vsi-&gt;idx] = NULL;
	return -ENOMEM;
}
<br>vsi의 링의 상태를 나타내는 포인터의 실제 공간을 정의하는 함수임.]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_alloc_stat_arrays().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_vsi_alloc_stat_arrays().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_vsi_cfg_def()]]></title><description><![CDATA[ 
 <br>/**
 * ice_vsi_cfg_def - configure default VSI based on the type
 * @vsi: pointer to VSI
 * @params: the parameters to configure this VSI with
 */
static int
ice_vsi_cfg_def(struct ice_vsi *vsi, struct ice_vsi_cfg_params *params)
{
	struct device *dev = ice_pf_to_dev(vsi-&gt;back);
	struct ice_pf *pf = vsi-&gt;back;
	int ret;

	vsi-&gt;vsw = pf-&gt;first_sw;

	ret = ice_vsi_alloc_def(vsi, params-&gt;ch); // [[ice_vsi_alloc_def()]]
	if (ret)
		return ret;

	/* allocate memory for Tx/Rx ring stat pointers */
	ret = ice_vsi_alloc_stat_arrays(vsi); // [[Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_vsi_alloc_stat_arrays().md|ice_vsi_alloc_stat_arrays()]]
	if (ret)
		goto unroll_vsi_alloc;

	ice_alloc_fd_res(vsi);

	ret = ice_vsi_get_qs(vsi); // [[ice_vsi_get_qs()]]
	if (ret) {
		dev_err(dev, "Failed to allocate queues. vsi-&gt;idx = %d\n",
			vsi-&gt;idx);
		goto unroll_vsi_alloc_stat;
	}

	/* set RSS capabilities */
	ice_vsi_set_rss_params(vsi);

	/* set TC configuration */
	ice_vsi_set_tc_cfg(vsi);

	/* create the VSI */
	ret = ice_vsi_init(vsi, params-&gt;flags); // [[ice_vsi_init()]]
	if (ret)
		goto unroll_get_qs;

	ice_vsi_init_vlan_ops(vsi);

	switch (vsi-&gt;type) {
	case ICE_VSI_CTRL:
	case ICE_VSI_SWITCHDEV_CTRL:
	case ICE_VSI_PF:
		ret = ice_vsi_alloc_q_vectors(vsi); // [[ice_vsi_alloc_q_vectors()]]
		if (ret)
			goto unroll_vsi_init;

		ret = ice_vsi_alloc_rings(vsi); // [[ice_vsi_alloc_rings()]]
		if (ret)
			goto unroll_vector_base;

		ret = ice_vsi_alloc_ring_stats(vsi); // [[ice_vsi_alloc_ring_stats()]]
		if (ret)
			goto unroll_vector_base;

		ice_vsi_map_rings_to_vectors(vsi); //PF일때만 해당, [[ice_vsi_map_rings_to_vectors()]]

		/* Associate q_vector rings to napi */
		ice_vsi_set_napi_queues(vsi);

		vsi-&gt;stat_offsets_loaded = false;

		if (ice_is_xdp_ena_vsi(vsi)) {
			ret = ice_vsi_determine_xdp_res(vsi);
			if (ret)
				goto unroll_vector_base;
			ret = ice_prepare_xdp_rings(vsi, vsi-&gt;xdp_prog);
			if (ret)
				goto unroll_vector_base;
		}

		/* ICE_VSI_CTRL does not need RSS so skip RSS processing */
		if (vsi-&gt;type != ICE_VSI_CTRL)
			/* Do not exit if configuring RSS had an issue, at
			 * least receive traffic on first queue. Hence no
			 * need to capture return value
			 */
			if (test_bit(ICE_FLAG_RSS_ENA, pf-&gt;flags)) {
				ice_vsi_cfg_rss_lut_key(vsi);
				ice_vsi_set_rss_flow_fld(vsi);
			}
		ice_init_arfs(vsi);
		break;
	case ICE_VSI_CHNL:
		if (test_bit(ICE_FLAG_RSS_ENA, pf-&gt;flags)) {
			ice_vsi_cfg_rss_lut_key(vsi);
			ice_vsi_set_rss_flow_fld(vsi);
		}
		break;
	case ICE_VSI_VF:
		/* VF driver will take care of creating netdev for this type and
		 * map queues to vectors through Virtchnl, PF driver only
		 * creates a VSI and corresponding structures for bookkeeping
		 * purpose
		 */
		ret = ice_vsi_alloc_q_vectors(vsi);
		if (ret)
			goto unroll_vsi_init;

		ret = ice_vsi_alloc_rings(vsi);
		if (ret)
			goto unroll_alloc_q_vector;

		ret = ice_vsi_alloc_ring_stats(vsi);
		if (ret)
			goto unroll_vector_base;

		vsi-&gt;stat_offsets_loaded = false;

		/* Do not exit if configuring RSS had an issue, at least
		 * receive traffic on first queue. Hence no need to capture
		 * return value
		 */
		if (test_bit(ICE_FLAG_RSS_ENA, pf-&gt;flags)) {
			ice_vsi_cfg_rss_lut_key(vsi);
			ice_vsi_set_vf_rss_flow_fld(vsi);
		}
		break;
	case ICE_VSI_LB:
		ret = ice_vsi_alloc_rings(vsi);
		if (ret)
			goto unroll_vsi_init;

		ret = ice_vsi_alloc_ring_stats(vsi);
		if (ret)
			goto unroll_vector_base;

		break;
	default:
		/* clean up the resources and exit */
		ret = -EINVAL;
		goto unroll_vsi_init;
	}

	return 0;

unroll_vector_base:
	/* reclaim SW interrupts back to the common pool */
unroll_alloc_q_vector:
	ice_vsi_free_q_vectors(vsi);
unroll_vsi_init:
	ice_vsi_delete_from_hw(vsi);
unroll_get_qs:
	ice_vsi_put_qs(vsi);
unroll_vsi_alloc_stat:
	ice_vsi_free_stats(vsi);
unroll_vsi_alloc:
	ice_vsi_free_arrays(vsi);
	return ret;
}
<br><a data-href="ice_vsi_alloc_def()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_alloc_def().html" class="internal-link" target="_self" rel="noopener nofollow">ice_vsi_alloc_def()</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_vsi_alloc_stat_arrays().md" data-href="Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_vsi_alloc_stat_arrays().md" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_alloc_stat_arrays().html" class="internal-link" target="_self" rel="noopener nofollow">ice_vsi_alloc_stat_arrays()</a><br>
<a data-href="ice_vsi_get_qs()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_get_qs().html" class="internal-link" target="_self" rel="noopener nofollow">ice_vsi_get_qs()</a><br>
<a data-href="ice_vsi_init()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_init().html" class="internal-link" target="_self" rel="noopener nofollow">ice_vsi_init()</a><br>
<a data-href="ice_vsi_alloc_q_vectors()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_alloc_q_vectors().html" class="internal-link" target="_self" rel="noopener nofollow">ice_vsi_alloc_q_vectors()</a><br>
<a data-href="ice_vsi_alloc_rings()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_alloc_rings().html" class="internal-link" target="_self" rel="noopener nofollow">ice_vsi_alloc_rings()</a><br>
<a data-href="ice_vsi_alloc_ring_stats()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_alloc_ring_stats().html" class="internal-link" target="_self" rel="noopener nofollow">ice_vsi_alloc_ring_stats()</a><br>
<a data-href="ice_vsi_map_rings_to_vectors()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_map_rings_to_vectors().html" class="internal-link" target="_self" rel="noopener nofollow">ice_vsi_map_rings_to_vectors()</a>]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_cfg_def().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_vsi_cfg_def().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_vsi_cfg_lan()]]></title><description><![CDATA[ 
 <br>/**
 * ice_vsi_cfg_lan - Setup the VSI lan related config
 * @vsi: the VSI being configured
 *
 * Return 0 on success and negative value on error
 */
int ice_vsi_cfg_lan(struct ice_vsi *vsi)
{
	int err;

	if (vsi-&gt;netdev &amp;&amp; vsi-&gt;type == ICE_VSI_PF) {
		ice_set_rx_mode(vsi-&gt;netdev);

		err = ice_vsi_vlan_setup(vsi);
		if (err)
			return err;
	}
	ice_vsi_cfg_dcb_rings(vsi);

	err = ice_vsi_cfg_lan_txqs(vsi);
	if (!err &amp;&amp; ice_is_xdp_ena_vsi(vsi))
		err = ice_vsi_cfg_xdp_txqs(vsi);
	if (!err)
		err = ice_vsi_cfg_rxqs(vsi); // [[ice_vsi_cfg_rxqs()]]

	return err;
}
<br><a data-href="ice_vsi_cfg_rxqs()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_cfg_rxqs().html" class="internal-link" target="_self" rel="noopener nofollow">ice_vsi_cfg_rxqs()</a>]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_cfg_lan().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_vsi_cfg_lan().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_vsi_cfg_rxq()]]></title><description><![CDATA[ 
 <br>/**
 * ice_vsi_cfg_rxq - Configure an Rx queue
 * @ring: the ring being configured
 *
 * Return 0 on success and a negative value on error.
 */
static int ice_vsi_cfg_rxq(struct ice_rx_ring *ring)
{
	struct device *dev = ice_pf_to_dev(ring-&gt;vsi-&gt;back);
	u32 num_bufs = ICE_RX_DESC_UNUSED(ring);
	int err;

	ring-&gt;rx_buf_len = ring-&gt;vsi-&gt;rx_buf_len;

	if (ring-&gt;vsi-&gt;type == ICE_VSI_PF) {
		if (!xdp_rxq_info_is_reg(&amp;ring-&gt;xdp_rxq)) {
			err = __xdp_rxq_info_reg(&amp;ring-&gt;xdp_rxq, ring-&gt;netdev,
						 ring-&gt;q_index,
						 ring-&gt;q_vector-&gt;napi.napi_id,
						 ring-&gt;rx_buf_len);
			if (err)
				return err;
		}

		ring-&gt;xsk_pool = ice_xsk_pool(ring);
		if (ring-&gt;xsk_pool) {
			xdp_rxq_info_unreg(&amp;ring-&gt;xdp_rxq);

			ring-&gt;rx_buf_len =
				xsk_pool_get_rx_frame_size(ring-&gt;xsk_pool);
			err = __xdp_rxq_info_reg(&amp;ring-&gt;xdp_rxq, ring-&gt;netdev,
						 ring-&gt;q_index,
						 ring-&gt;q_vector-&gt;napi.napi_id,
						 ring-&gt;rx_buf_len);
			if (err)
				return err;
			err = xdp_rxq_info_reg_mem_model(&amp;ring-&gt;xdp_rxq,
							 MEM_TYPE_XSK_BUFF_POOL,
							 NULL);
			if (err)
				return err;
			xsk_pool_set_rxq_info(ring-&gt;xsk_pool, &amp;ring-&gt;xdp_rxq);
			ice_xsk_pool_fill_cb(ring);

			dev_info(dev, "Registered XDP mem model MEM_TYPE_XSK_BUFF_POOL on Rx ring %d\n",
				 ring-&gt;q_index);
		} else {
			if (!xdp_rxq_info_is_reg(&amp;ring-&gt;xdp_rxq)) {
				err = __xdp_rxq_info_reg(&amp;ring-&gt;xdp_rxq, ring-&gt;netdev,
							 ring-&gt;q_index,
							 ring-&gt;q_vector-&gt;napi.napi_id,
							 ring-&gt;rx_buf_len);
				if (err)
					return err;
			}

			err = xdp_rxq_info_reg_mem_model(&amp;ring-&gt;xdp_rxq,
							 MEM_TYPE_PAGE_SHARED,
							 NULL);
			if (err)
				return err;
		}
	}

	xdp_init_buff(&amp;ring-&gt;xdp, ice_rx_pg_size(ring) / 2, &amp;ring-&gt;xdp_rxq);
	ring-&gt;xdp.data = NULL;
	ring-&gt;xdp_ext.pkt_ctx = &amp;ring-&gt;pkt_ctx;
	err = ice_setup_rx_ctx(ring);
	if (err) {
		dev_err(dev, "ice_setup_rx_ctx failed for RxQ %d, err %d\n",
			ring-&gt;q_index, err);
		return err;
	}

	if (ring-&gt;xsk_pool) {
		bool ok;

		if (!xsk_buff_can_alloc(ring-&gt;xsk_pool, num_bufs)) {
			dev_warn(dev, "XSK buffer pool does not provide enough addresses to fill %d buffers on Rx ring %d\n",
				 num_bufs, ring-&gt;q_index);
			dev_warn(dev, "Change Rx ring/fill queue size to avoid performance issues\n");

			return 0;
		}

		ok = ice_alloc_rx_bufs_zc(ring, num_bufs);
		if (!ok) {
			u16 pf_q = ring-&gt;vsi-&gt;rxq_map[ring-&gt;q_index];

			dev_info(dev, "Failed to allocate some buffers on XSK buffer pool enabled Rx ring %d (pf_q %d)\n",
				 ring-&gt;q_index, pf_q);
		}

		return 0;
	}

	ice_alloc_rx_bufs(ring, num_bufs);

	return 0;
}
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_cfg_rxq().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_vsi_cfg_rxq().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_vsi_cfg_rxqs()]]></title><description><![CDATA[ 
 <br>/**
 * ice_vsi_cfg_rxqs - Configure the VSI for Rx
 * @vsi: the VSI being configured
 *
 * Return 0 on success and a negative value on error
 * Configure the Rx VSI for operation.
 */
int ice_vsi_cfg_rxqs(struct ice_vsi *vsi)
{
	u16 i;

	if (vsi-&gt;type == ICE_VSI_VF)
		goto setup_rings;

	ice_vsi_cfg_frame_size(vsi);
setup_rings:
	/* set up individual rings */
	ice_for_each_rxq(vsi, i) {
		int err = ice_vsi_cfg_rxq(vsi-&gt;rx_rings[i]); // [[ice_vsi_cfg_rxq()]]

		if (err)
			return err;
	}

	return 0;
}
<br><a data-href="ice_vsi_cfg_rxq()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_cfg_rxq().html" class="internal-link" target="_self" rel="noopener nofollow">ice_vsi_cfg_rxq()</a>]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_cfg_rxqs().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_vsi_cfg_rxqs().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_vsi_cfg()]]></title><description><![CDATA[ 
 <br>/**
* ice_vsi_cfg - configure a previously allocated VSI
* @vsi: pointer to VSI
* @params: parameters used to configure this VSI
*/
int ice_vsi_cfg(struct ice_vsi *vsi, struct ice_vsi_cfg_params *params)
{
	struct ice_pf *pf = vsi-&gt;back;
	int ret;

	if (WARN_ON(params-&gt;type == ICE_VSI_VF &amp;&amp; !params-&gt;vf))
		return -EINVAL;

	vsi-&gt;type = params-&gt;type;
	vsi-&gt;port_info = params-&gt;pi;

	/* For VSIs which don't have a connected VF, this will be NULL */
	vsi-&gt;vf = params-&gt;vf;

	ret = ice_vsi_cfg_def(vsi, params); // [[ice_vsi_cfg_def()]]
	if (ret)
		return ret;

	ret = ice_vsi_cfg_tc_lan(vsi-&gt;back, vsi);
	if (ret)
		ice_vsi_decfg(vsi);

	if (vsi-&gt;type == ICE_VSI_CTRL) {
		if (vsi-&gt;vf) {
			WARN_ON(vsi-&gt;vf-&gt;ctrl_vsi_idx != ICE_NO_VSI);
			vsi-&gt;vf-&gt;ctrl_vsi_idx = vsi-&gt;idx;
		} else {
			WARN_ON(pf-&gt;ctrl_vsi_idx != ICE_NO_VSI);
			pf-&gt;ctrl_vsi_idx = vsi-&gt;idx;
		}
	}

	return ret;
}

<br><a data-href="ice_vsi_cfg_def()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_cfg_def().html" class="internal-link" target="_self" rel="noopener nofollow">ice_vsi_cfg_def()</a>]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_cfg().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_vsi_cfg().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_vsi_ctrl_all_rx_rings()]]></title><description><![CDATA[ 
 <br>/**
 * ice_vsi_ctrl_all_rx_rings - Start or stop a VSI's Rx rings
 * @vsi: the VSI being configured
 * @ena: start or stop the Rx rings
 *
 * First enable/disable all of the Rx rings, flush any remaining writes, and
 * then verify that they have all been enabled/disabled successfully. This will
 * let all of the register writes complete when enabling/disabling the Rx rings
 * before waiting for the change in hardware to complete.
 */
static int ice_vsi_ctrl_all_rx_rings(struct ice_vsi *vsi, bool ena)
{
	int ret = 0;
	u16 i;

	ice_for_each_rxq(vsi, i)
		ice_vsi_ctrl_one_rx_ring(vsi, ena, i, false);

	ice_flush(&amp;vsi-&gt;back-&gt;hw);

	ice_for_each_rxq(vsi, i) {
		ret = ice_vsi_wait_one_rx_ring(vsi, ena, i);
		if (ret)
			break;
	}

	return ret;
}
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_ctrl_all_rx_rings().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_vsi_ctrl_all_rx_rings().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_vsi_ena_irq()]]></title><description><![CDATA[ 
 <br>/**
 * ice_vsi_ena_irq - Enable IRQ for the given VSI
 * @vsi: the VSI being configured
 */
static int ice_vsi_ena_irq(struct ice_vsi *vsi)
{
	struct ice_hw *hw = &amp;vsi-&gt;back-&gt;hw;
	int i;

	ice_for_each_q_vector(vsi, i)
		ice_irq_dynamic_ena(hw, vsi, vsi-&gt;q_vectors[i]); // [[ice_irq_dynamic_ena()]]
		//각각의 q_vector에 대하여 인터럽트를 활성화 함.

	ice_flush(hw);
	return 0;
}
<br><a data-href="ice_irq_dynamic_ena()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_irq_dynamic_ena().html" class="internal-link" target="_self" rel="noopener nofollow">ice_irq_dynamic_ena()</a>]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_ena_irq().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_vsi_ena_irq().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_vsi_get_qs()]]></title><description><![CDATA[ 
 <br>/**
 * ice_vsi_get_qs - Assign queues from PF to VSI
 * @vsi: the VSI to assign queues to
 *
 * Returns 0 on success and a negative value on error
 */
static int ice_vsi_get_qs(struct ice_vsi *vsi)
{
	struct ice_pf *pf = vsi-&gt;back;
	struct ice_qs_cfg tx_qs_cfg = {
		.qs_mutex = &amp;pf-&gt;avail_q_mutex,
		.pf_map = pf-&gt;avail_txqs,
		.pf_map_size = pf-&gt;max_pf_txqs,
		.q_count = vsi-&gt;alloc_txq,
		.scatter_count = ICE_MAX_SCATTER_TXQS,
		.vsi_map = vsi-&gt;txq_map,
		.vsi_map_offset = 0,
		.mapping_mode = ICE_VSI_MAP_CONTIG
	};
	struct ice_qs_cfg rx_qs_cfg = {
		.qs_mutex = &amp;pf-&gt;avail_q_mutex,
		.pf_map = pf-&gt;avail_rxqs,
		.pf_map_size = pf-&gt;max_pf_rxqs,
		.q_count = vsi-&gt;alloc_rxq,
		.scatter_count = ICE_MAX_SCATTER_RXQS,
		.vsi_map = vsi-&gt;rxq_map,
		.vsi_map_offset = 0,
		.mapping_mode = ICE_VSI_MAP_CONTIG
	};
	int ret;

	if (vsi-&gt;type == ICE_VSI_CHNL)
		return 0;

	ret = __ice_vsi_get_qs(&amp;tx_qs_cfg);
	if (ret)
		return ret;
	vsi-&gt;tx_mapping_mode = tx_qs_cfg.mapping_mode;

	ret = __ice_vsi_get_qs(&amp;rx_qs_cfg);
	if (ret)
		return ret;
	vsi-&gt;rx_mapping_mode = rx_qs_cfg.mapping_mode;

	return 0;
}
<br>큐를 pf에서 vsi로 할당해주는 함수]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_get_qs().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_vsi_get_qs().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_vsi_init()]]></title><description><![CDATA[ 
 <br>/**
 * ice_vsi_init - Create and initialize a VSI
 * @vsi: the VSI being configured
 * @vsi_flags: VSI configuration flags
 *
 * Set ICE_FLAG_VSI_INIT to initialize a new VSI context, clear it to
 * reconfigure an existing context.
 *
 * This initializes a VSI context depending on the VSI type to be added and
 * passes it down to the add_vsi aq command to create a new VSI.
 */
static int ice_vsi_init(struct ice_vsi *vsi, u32 vsi_flags)
{
	struct ice_pf *pf = vsi-&gt;back;
	struct ice_hw *hw = &amp;pf-&gt;hw;
	struct ice_vsi_ctx *ctxt;
	struct device *dev;
	int ret = 0;

	dev = ice_pf_to_dev(pf);
	ctxt = kzalloc(sizeof(*ctxt), GFP_KERNEL);
	if (!ctxt)
		return -ENOMEM;

	switch (vsi-&gt;type) {
	case ICE_VSI_CTRL:
	case ICE_VSI_LB:
	case ICE_VSI_PF:
		ctxt-&gt;flags = ICE_AQ_VSI_TYPE_PF;
		break;
	case ICE_VSI_SWITCHDEV_CTRL:
	case ICE_VSI_CHNL:
		ctxt-&gt;flags = ICE_AQ_VSI_TYPE_VMDQ2;
		break;
	case ICE_VSI_VF:
		ctxt-&gt;flags = ICE_AQ_VSI_TYPE_VF;
		/* VF number here is the absolute VF number (0-255) */
		ctxt-&gt;vf_num = vsi-&gt;vf-&gt;vf_id + hw-&gt;func_caps.vf_base_id;
		break;
	default:
		ret = -ENODEV;
		goto out;
	}

	/* Handle VLAN pruning for channel VSI if main VSI has VLAN
	 * prune enabled
	 */
	if (vsi-&gt;type == ICE_VSI_CHNL) {
		struct ice_vsi *main_vsi;

		main_vsi = ice_get_main_vsi(pf);
		if (main_vsi &amp;&amp; ice_vsi_is_vlan_pruning_ena(main_vsi))
			ctxt-&gt;info.sw_flags2 |=
				ICE_AQ_VSI_SW_FLAG_RX_VLAN_PRUNE_ENA;
		else
			ctxt-&gt;info.sw_flags2 &amp;=
				~ICE_AQ_VSI_SW_FLAG_RX_VLAN_PRUNE_ENA;
	}

	ice_set_dflt_vsi_ctx(hw, ctxt);
	if (test_bit(ICE_FLAG_FD_ENA, pf-&gt;flags))
		ice_set_fd_vsi_ctx(ctxt, vsi);
	/* if the switch is in VEB mode, allow VSI loopback */
	if (vsi-&gt;vsw-&gt;bridge_mode == BRIDGE_MODE_VEB)
		ctxt-&gt;info.sw_flags |= ICE_AQ_VSI_SW_FLAG_ALLOW_LB;

	/* Set LUT type and HASH type if RSS is enabled */
	if (test_bit(ICE_FLAG_RSS_ENA, pf-&gt;flags) &amp;&amp;
	    vsi-&gt;type != ICE_VSI_CTRL) {
		ice_set_rss_vsi_ctx(ctxt, vsi);
		/* if updating VSI context, make sure to set valid_section:
		 * to indicate which section of VSI context being updated
		 */
		if (!(vsi_flags &amp; ICE_VSI_FLAG_INIT))
			ctxt-&gt;info.valid_sections |=
				cpu_to_le16(ICE_AQ_VSI_PROP_Q_OPT_VALID);
	}

	ctxt-&gt;info.sw_id = vsi-&gt;port_info-&gt;sw_id;
	if (vsi-&gt;type == ICE_VSI_CHNL) {
		ice_chnl_vsi_setup_q_map(vsi, ctxt);
	} else {
		ret = ice_vsi_setup_q_map(vsi, ctxt);
		if (ret)
			goto out;

		if (!(vsi_flags &amp; ICE_VSI_FLAG_INIT))
			/* means VSI being updated */
			/* must to indicate which section of VSI context are
			 * being modified
			 */
			ctxt-&gt;info.valid_sections |=
				cpu_to_le16(ICE_AQ_VSI_PROP_RXQ_MAP_VALID);
	}

	/* Allow control frames out of main VSI */
	if (vsi-&gt;type == ICE_VSI_PF) {
		ctxt-&gt;info.sec_flags |= ICE_AQ_VSI_SEC_FLAG_ALLOW_DEST_OVRD;
		ctxt-&gt;info.valid_sections |=
			cpu_to_le16(ICE_AQ_VSI_PROP_SECURITY_VALID);
	}

	if (vsi_flags &amp; ICE_VSI_FLAG_INIT) {
		ret = ice_add_vsi(hw, vsi-&gt;idx, ctxt, NULL);
		if (ret) {
			dev_err(dev, "Add VSI failed, err %d\n", ret);
			ret = -EIO;
			goto out;
		}
	} else {
		ret = ice_update_vsi(hw, vsi-&gt;idx, ctxt, NULL);
		if (ret) {
			dev_err(dev, "Update VSI failed, err %d\n", ret);
			ret = -EIO;
			goto out;
		}
	}

	/* keep context for update VSI operations */
	vsi-&gt;info = ctxt-&gt;info;

	/* record VSI number returned */
	vsi-&gt;vsi_num = ctxt-&gt;vsi_num;

out:
	kfree(ctxt);
	return ret;
}
<br>vsi→type에 따라 달라짐.]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_init().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_vsi_init().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_vsi_map_rings_to_vectors()]]></title><description><![CDATA[ 
 <br>/**
 * ice_vsi_map_rings_to_vectors - Map VSI rings to interrupt vectors
 * @vsi: the VSI being configured
 *
 * This function maps descriptor rings to the queue-specific vectors allotted
 * through the MSI-X enabling code. On a constrained vector budget, we map Tx
 * and Rx rings to the vector as "efficiently" as possible.
 */
void ice_vsi_map_rings_to_vectors(struct ice_vsi *vsi)
{
	int q_vectors = vsi-&gt;num_q_vectors;
	u16 tx_rings_rem, rx_rings_rem;
	int v_id;

	/* initially assigning remaining rings count to VSIs num queue value */
	tx_rings_rem = vsi-&gt;num_txq;
	rx_rings_rem = vsi-&gt;num_rxq;

	for (v_id = 0; v_id &lt; q_vectors; v_id++) {
		struct ice_q_vector *q_vector = vsi-&gt;q_vectors[v_id];
		u8 tx_rings_per_v, rx_rings_per_v;
		u16 q_id, q_base;

		/* Tx rings mapping to vector */
		tx_rings_per_v = (u8)DIV_ROUND_UP(tx_rings_rem,
						  q_vectors - v_id);
		q_vector-&gt;num_ring_tx = tx_rings_per_v;
		q_vector-&gt;tx.tx_ring = NULL;
		q_vector-&gt;tx.itr_idx = ICE_TX_ITR;
		q_base = vsi-&gt;num_txq - tx_rings_rem;

		for (q_id = q_base; q_id &lt; (q_base + tx_rings_per_v); q_id++) {
			struct ice_tx_ring *tx_ring = vsi-&gt;tx_rings[q_id];

			tx_ring-&gt;q_vector = q_vector;
			tx_ring-&gt;next = q_vector-&gt;tx.tx_ring;
			q_vector-&gt;tx.tx_ring = tx_ring;
		}
		tx_rings_rem -= tx_rings_per_v;

		/* Rx rings mapping to vector */
		rx_rings_per_v = (u8)DIV_ROUND_UP(rx_rings_rem,
						  q_vectors - v_id);
		q_vector-&gt;num_ring_rx = rx_rings_per_v;
		q_vector-&gt;rx.rx_ring = NULL;
		q_vector-&gt;rx.itr_idx = ICE_RX_ITR;
		q_base = vsi-&gt;num_rxq - rx_rings_rem;

		for (q_id = q_base; q_id &lt; (q_base + rx_rings_per_v); q_id++) {
			struct ice_rx_ring *rx_ring = vsi-&gt;rx_rings[q_id];

			rx_ring-&gt;q_vector = q_vector;
			rx_ring-&gt;next = q_vector-&gt;rx.rx_ring;
			q_vector-&gt;rx.rx_ring = rx_ring;
		}
		rx_rings_rem -= rx_rings_per_v;
	}
}
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_map_rings_to_vectors().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_vsi_map_rings_to_vectors().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_vsi_open()]]></title><description><![CDATA[ 
 <br>/**
 * ice_vsi_open - Called when a network interface is made active
 * @vsi: the VSI to open
 *
 * Initialization of the VSI
 *
 * Returns 0 on success, negative value on error
 */
int ice_vsi_open(struct ice_vsi *vsi)
{
	char int_name[ICE_INT_NAME_STR_LEN];
	struct ice_pf *pf = vsi-&gt;back;
	int err;

	/* allocate descriptors */
	err = ice_vsi_setup_tx_rings(vsi); // [[ice_vsi_setup_tx_rings()]]
	if (err)
		goto err_setup_tx;

	err = ice_vsi_setup_rx_rings(vsi); // [[ice_vsi_setup_rx_rings()]]
	if (err)
		goto err_setup_rx;

	err = ice_vsi_cfg_lan(vsi); // [[ice_vsi_cfg_lan()]]
	if (err)
		goto err_setup_rx;

	snprintf(int_name, sizeof(int_name) - 1, "%s-%s",
		 dev_driver_string(ice_pf_to_dev(pf)), vsi-&gt;netdev-&gt;name);
	err = ice_vsi_req_irq_msix(vsi, int_name); // [[ice_vsi_req_irq_msix()]]
	if (err)
		goto err_setup_rx;

	ice_vsi_cfg_netdev_tc(vsi, vsi-&gt;tc_cfg.ena_tc);

	if (vsi-&gt;type == ICE_VSI_PF) {
		/* Notify the stack of the actual queue counts. */
		err = netif_set_real_num_tx_queues(vsi-&gt;netdev, vsi-&gt;num_txq);
		if (err)
			goto err_set_qs;

		err = netif_set_real_num_rx_queues(vsi-&gt;netdev, vsi-&gt;num_rxq);
		if (err)
			goto err_set_qs;
	}

	err = ice_up_complete(vsi); // [[ice_up_complete()]]
	if (err)
		goto err_up_complete;

	return 0;

err_up_complete:
	ice_down(vsi);
err_set_qs:
	ice_vsi_free_irq(vsi);
err_setup_rx:
	ice_vsi_free_rx_rings(vsi);
err_setup_tx:
	ice_vsi_free_tx_rings(vsi);

	return err;
}
<br><a data-href="ice_vsi_setup_tx_rings()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_setup_tx_rings().html" class="internal-link" target="_self" rel="noopener nofollow">ice_vsi_setup_tx_rings()</a><br>
<a data-href="ice_vsi_setup_rx_rings()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_setup_rx_rings().html" class="internal-link" target="_self" rel="noopener nofollow">ice_vsi_setup_rx_rings()</a><br>
<a data-href="ice_vsi_cfg_lan()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_cfg_lan().html" class="internal-link" target="_self" rel="noopener nofollow">ice_vsi_cfg_lan()</a><br>
<a data-href="ice_vsi_req_irq_msix()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_req_irq_msix().html" class="internal-link" target="_self" rel="noopener nofollow">ice_vsi_req_irq_msix()</a><br>
<a data-href="ice_up_complete()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_up_complete().html" class="internal-link" target="_self" rel="noopener nofollow">ice_up_complete()</a><br>vsi : virtual station interface, 가상화를 통해 여러 개의 논리 NIC로 사용할 수 있게 한다.<br>device라는 struct는 linux가 device를 관리하는데 사용함. “devm_” 은 device 메모리 관리 인터페이스를 통하므로, driver가 unload 되면 모든 자원을 해제하여 explicit하게 free를 할 필요가 없다는 장점이 있다.]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_open().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_vsi_open().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_vsi_req_irq_msix()]]></title><description><![CDATA[ 
 <br>/**
 * ice_vsi_req_irq_msix - get MSI-X vectors from the OS for the VSI
 * @vsi: the VSI being configured
 * @basename: name for the vector
 */
static int ice_vsi_req_irq_msix(struct ice_vsi *vsi, char *basename)
{
	int q_vectors = vsi-&gt;num_q_vectors;
	struct ice_pf *pf = vsi-&gt;back;
	struct device *dev;
	int rx_int_idx = 0;
	int tx_int_idx = 0;
	int vector, err;
	int irq_num;

	dev = ice_pf_to_dev(pf);
	for (vector = 0; vector &lt; q_vectors; vector++) {
		struct ice_q_vector *q_vector = vsi-&gt;q_vectors[vector];

		irq_num = q_vector-&gt;irq.virq;

		if (q_vector-&gt;tx.tx_ring &amp;&amp; q_vector-&gt;rx.rx_ring) {
			snprintf(q_vector-&gt;name, sizeof(q_vector-&gt;name) - 1,
				 "%s-%s-%d", basename, "TxRx", rx_int_idx++);
			tx_int_idx++;
		} else if (q_vector-&gt;rx.rx_ring) {
			snprintf(q_vector-&gt;name, sizeof(q_vector-&gt;name) - 1,
				 "%s-%s-%d", basename, "rx", rx_int_idx++);
		} else if (q_vector-&gt;tx.tx_ring) {
			snprintf(q_vector-&gt;name, sizeof(q_vector-&gt;name) - 1,
				 "%s-%s-%d", basename, "tx", tx_int_idx++);
		} else {
			/* skip this unused q_vector */
			continue;
		}
		if (vsi-&gt;type == ICE_VSI_CTRL &amp;&amp; vsi-&gt;vf)
			err = devm_request_irq(dev, irq_num, vsi-&gt;irq_handler,
					       IRQF_SHARED, q_vector-&gt;name,
					       q_vector); // [[devm_request_irq()]]
		else
			err = devm_request_irq(dev, irq_num, vsi-&gt;irq_handler,
					       0, q_vector-&gt;name, q_vector); // [[devm_request_irq()]]
		if (err) {
			netdev_err(vsi-&gt;netdev, "MSIX request_irq failed, error: %d\n",
				   err);
			goto free_q_irqs;
		}

		/* register for affinity change notifications */
		if (!IS_ENABLED(CONFIG_RFS_ACCEL)) {
			struct irq_affinity_notify *affinity_notify;

			affinity_notify = &amp;q_vector-&gt;affinity_notify;
			affinity_notify-&gt;notify = ice_irq_affinity_notify;
			affinity_notify-&gt;release = ice_irq_affinity_release;
			irq_set_affinity_notifier(irq_num, affinity_notify);
		}

		/* assign the mask for this irq */
		irq_set_affinity_hint(irq_num, &amp;q_vector-&gt;affinity_mask);
	}

	err = ice_set_cpu_rx_rmap(vsi); // [[ice_set_cpu_rx_rmap()]]
	if (err) {
		netdev_err(vsi-&gt;netdev, "Failed to setup CPU RMAP on VSI %u: %pe\n",
			   vsi-&gt;vsi_num, ERR_PTR(err));
		goto free_q_irqs;
	}

	vsi-&gt;irqs_ready = true;
	return 0;

free_q_irqs:
	while (vector--) {
		irq_num = vsi-&gt;q_vectors[vector]-&gt;irq.virq;
		if (!IS_ENABLED(CONFIG_RFS_ACCEL))
			irq_set_affinity_notifier(irq_num, NULL);
		irq_set_affinity_hint(irq_num, NULL);
		devm_free_irq(dev, irq_num, &amp;vsi-&gt;q_vectors[vector]);
	}
	return err;
}
<br><a data-href="devm_request_irq()" href="encyclopedia-of-networksystem/function/include-linux/devm_request_irq().html" class="internal-link" target="_self" rel="noopener nofollow">devm_request_irq()</a><br>
<a data-href="ice_set_cpu_rx_rmap()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_set_cpu_rx_rmap().html" class="internal-link" target="_self" rel="noopener nofollow">ice_set_cpu_rx_rmap()</a><br>msi-x?<br>각각의 Rx queue는 구분되고 연관된 IRQ를 가지고 있다. NIC가 이걸 trigger하여 CPU가 새로운 패킷이 들어왔음을 알린다. PCIe를 통해 인터럽트를 일으키는 것으로 MSI-x를 사용하는데, 흔히 ifconfig를 했을 때 나오는 eth(x) 이 것을 가리킨다. 하드웨어 핀으로 인터럽트를 발생시키는 것은 장치 갯수에 한계가 있어 이를 메시지 형태로 발생시키게 하여 더 많은 장치를 다룰 수 있도록 하는 것이다. 이를 이용하여 NIC는 특정 CPU를 지정하여 처리하도록 보낼 수가 있다. 각각의 큐와 IRQ에 대한 매핑은 /proc/interrupt 에서 설정이 가능하다.]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_req_irq_msix().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_vsi_req_irq_msix().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_vsi_setup_rx_rings()]]></title><description><![CDATA[ 
 <br>/**
 * ice_vsi_setup_rx_rings - Allocate VSI Rx queue resources
 * @vsi: VSI having resources allocated
 *
 * Return 0 on success, negative on failure
 */
int ice_vsi_setup_rx_rings(struct ice_vsi *vsi)
{
	int i, err = 0;

	if (!vsi-&gt;num_rxq) {
		dev_err(ice_pf_to_dev(vsi-&gt;back), "VSI %d has 0 Rx queues\n",
			vsi-&gt;vsi_num);
		return -EINVAL;
	}

	ice_for_each_rxq(vsi, i) {
		struct ice_rx_ring *ring = vsi-&gt;rx_rings[i]; // [[ice_rx_ring]]

		if (!ring)
			return -EINVAL;

		if (vsi-&gt;netdev)
			ring-&gt;netdev = vsi-&gt;netdev;
		err = ice_setup_rx_ring(ring); // [[ice_setup_rx_ring()]]
		if (err)
			break;
	}

	return err;
}
<br><a data-href="ice_rx_ring" href="encyclopedia-of-networksystem/struct/drivers-net-ethernet-intel-ice/ice_rx_ring.html" class="internal-link" target="_self" rel="noopener nofollow">ice_rx_ring</a><br>
<a data-href="ice_setup_rx_ring()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_setup_rx_ring().html" class="internal-link" target="_self" rel="noopener nofollow">ice_setup_rx_ring()</a><br>우선 vsi의 num_rxq를 통해 할당 된 rx queue의 갯수가 0이면 에러를 출력하고, 각각의 큐에 대하여, ice_rx_ring을 선언하여 ice_setup_rx_ring 함수를 호출하여 기본 설정을 시작한다.<br>ice_rx_ring struct에서 u16 q_index는 해당 링의 Queue number로, CPU가 수신 링들을 구분할 때 사용하는 번호이다. napi에서 skb를 만들 때 해당 번호 + 1 을 skb→queue_mapping에다가 넣어준다. 다음으로 next_to_use와 next_to_clean이 있는데, 이는 일반적인 큐와 비슷하다. push를 하면 next_to_use를 사용하게 되고, napi로 인해 skb로 만들어져 네트워크 스택으로 전달 될 경우 next_to_clean을 사용하여 인덱스에 접근하게 된다. 마찬가지로 index가 overflow라면 0으로 돌아가게 된다.]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_setup_rx_rings().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_vsi_setup_rx_rings().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_vsi_setup_tx_rings()]]></title><description><![CDATA[ 
 <br>/**
 * ice_vsi_setup_tx_rings - Allocate VSI Tx queue resources
 * @vsi: VSI having resources allocated
 *
 * Return 0 on success, negative on failure
 */
int ice_vsi_setup_tx_rings(struct ice_vsi *vsi)
{
	int i, err = 0;

	if (!vsi-&gt;num_txq) {
		dev_err(ice_pf_to_dev(vsi-&gt;back), "VSI %d has 0 Tx queues\n",
			vsi-&gt;vsi_num);
		return -EINVAL;
	}

	ice_for_each_txq(vsi, i) {
		struct ice_tx_ring *ring = vsi-&gt;tx_rings[i]; // [[ice_tx_ring]]

		if (!ring)
			return -EINVAL;

		if (vsi-&gt;netdev)
			ring-&gt;netdev = vsi-&gt;netdev;
		err = ice_setup_tx_ring(ring); // [[ice_setup_tx_ring()]]
 		if (err)
			break;
	}

	return err;
}
<br><a data-href="ice_tx_ring" href="encyclopedia-of-networksystem/struct/drivers-net-ethernet-intel-ice/ice_tx_ring.html" class="internal-link" target="_self" rel="noopener nofollow">ice_tx_ring</a><br>
<a data-href="ice_setup_tx_ring()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_setup_tx_ring().html" class="internal-link" target="_self" rel="noopener nofollow">ice_setup_tx_ring()</a><br>우선 num_txq를 통해 할당 된 tx queue의 갯수가 0이면 에러를 출력하고, 각각의 큐에 대하여, ice_tx_ring을 선언하여 ice_setup_tx_ring 함수를 호출하여 기본 설정을 시작한다.<br>struct ice_tx_ring ⇒ drivers/net/ethernet/intel/ice/ice_txrx.h 에서 볼 수 있음.<br>만들어진 모든 tx_ring은 ice_main.c의 ice_vsi_setup_tx_rings 에서 vsi→tx_rings[i]를 가르키는 포인터에서 수행하므로 결과적으로 vsi의 tx_rings 배열의 tx_ring들을 하나하나 설정하게 됨.]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_setup_tx_rings().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_vsi_setup_tx_rings().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_vsi_setup()]]></title><description><![CDATA[ 
 <br>/**
 * ice_vsi_setup - Set up a VSI by a given type
 * @pf: board private structure
 * @params: parameters to use when creating the VSI
 *
 * This allocates the sw VSI structure and its queue resources.
 *
 * Returns pointer to the successfully allocated and configured VSI sw struct on
 * success, NULL on failure.
 */
struct ice_vsi *
ice_vsi_setup(struct ice_pf *pf, struct ice_vsi_cfg_params *params)
{
	struct device *dev = ice_pf_to_dev(pf);
	struct ice_vsi *vsi;
	int ret;

	/* ice_vsi_setup can only initialize a new VSI, and we must have
	 * a port_info structure for it.
	 */
	if (WARN_ON(!(params-&gt;flags &amp; ICE_VSI_FLAG_INIT)) ||
	    WARN_ON(!params-&gt;pi))
		return NULL;

	vsi = ice_vsi_alloc(pf);
	if (!vsi) {
		dev_err(dev, "could not allocate VSI\n");
		return NULL;
	}

	ret = ice_vsi_cfg(vsi, params); // [[ice_vsi_cfg()]]
	if (ret)
		goto err_vsi_cfg;

	/* Add switch rule to drop all Tx Flow Control Frames, of look up
	 * type ETHERTYPE from VSIs, and restrict malicious VF from sending
	 * out PAUSE or PFC frames. If enabled, FW can still send FC frames.
	 * The rule is added once for PF VSI in order to create appropriate
	 * recipe, since VSI/VSI list is ignored with drop action...
	 * Also add rules to handle LLDP Tx packets.  Tx LLDP packets need to
	 * be dropped so that VFs cannot send LLDP packets to reconfig DCB
	 * settings in the HW.
	 */
	if (!ice_is_safe_mode(pf) &amp;&amp; vsi-&gt;type == ICE_VSI_PF) {
		ice_fltr_add_eth(vsi, ETH_P_PAUSE, ICE_FLTR_TX,
				 ICE_DROP_P ACKET);
		ice_cfg_sw_lldp(vsi, true, true);
	}

	if (!vsi-&gt;agg_node)
		ice_set_agg_vsi(vsi);

	return vsi;

err_vsi_cfg:
	ice_vsi_free(vsi);

	return NULL;
}
<br><a data-href="ice_vsi_cfg()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_cfg().html" class="internal-link" target="_self" rel="noopener nofollow">ice_vsi_cfg()</a>]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_setup().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_vsi_setup().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_vsi_start_all_rx_rings()]]></title><description><![CDATA[ 
 <br>/**
 * ice_vsi_start_all_rx_rings - start/enable all of a VSI's Rx rings
 * @vsi: the VSI whose rings are to be enabled
 *
 * Returns 0 on success and a negative value on error
 */
int ice_vsi_start_all_rx_rings(struct ice_vsi *vsi)
{
	return ice_vsi_ctrl_all_rx_rings(vsi, true); // [[ice_vsi_ctrl_all_rx_rings()]]
}
<br><a data-href="ice_vsi_ctrl_all_rx_rings()" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_ctrl_all_rx_rings().html" class="internal-link" target="_self" rel="noopener nofollow">ice_vsi_ctrl_all_rx_rings()</a><br>각각의 하나씩의 rx ring에 대하여 시작하고, 정상적으로 시작되는 것을 확인하고 함수 반환]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_vsi_start_all_rx_rings().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_vsi_start_all_rx_rings().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_xmit_zc()]]></title><description><![CDATA[ 
 <br>/**
 * ice_xmit_zc - take entries from XSK Tx ring and place them onto HW Tx ring
 * @xdp_ring: XDP ring to produce the HW Tx descriptors on
 *
 * Returns true if there is no more work that needs to be done, false otherwise
 */
bool ice_xmit_zc(struct ice_tx_ring *xdp_ring)
{
	struct xdp_desc *descs = xdp_ring-&gt;xsk_pool-&gt;tx_descs;
	u32 nb_pkts, nb_processed = 0;
	unsigned int total_bytes = 0;
	int budget;

	ice_clean_xdp_irq_zc(xdp_ring);

	budget = ICE_DESC_UNUSED(xdp_ring);
	budget = min_t(u16, budget, ICE_RING_QUARTER(xdp_ring));

	nb_pkts = xsk_tx_peek_release_desc_batch(xdp_ring-&gt;xsk_pool, budget);
	if (!nb_pkts)
		return true;

	if (xdp_ring-&gt;next_to_use + nb_pkts &gt;= xdp_ring-&gt;count) {
		nb_processed = xdp_ring-&gt;count - xdp_ring-&gt;next_to_use;
		ice_fill_tx_hw_ring(xdp_ring, descs, nb_processed, &amp;total_bytes);
		xdp_ring-&gt;next_to_use = 0;
	}

	ice_fill_tx_hw_ring(xdp_ring, &amp;descs[nb_processed], nb_pkts - nb_processed,
			    &amp;total_bytes);

	ice_set_rs_bit(xdp_ring);
	ice_xdp_ring_update_tail(xdp_ring);
	ice_update_tx_ring_stats(xdp_ring, nb_pkts, total_bytes);

	if (xsk_uses_need_wakeup(xdp_ring-&gt;xsk_pool))
		xsk_set_tx_need_wakeup(xdp_ring-&gt;xsk_pool);

	return nb_pkts &lt; budget;
}
]]></description><link>encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_xmit_zc().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_xmit_zc().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[pci_clear_master()]]></title><description><![CDATA[ 
 <br>void pci_clear_master(struct pci_dev *dev)
{
	__pci_set_master(dev, false);
}
EXPORT_SYMBOL(pci_clear_master);
]]></description><link>encyclopedia-of-networksystem/function/drivers-pci/pci_clear_master().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-pci/pci_clear_master().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[함수 설명]]></title><description><![CDATA[ 
 <br>/**
 * pci_save_state - save the PCI configuration space of a device before
 *		    suspending
 * @dev: PCI device that we're dealing with
 */
int pci_save_state(struct pci_dev *dev)
{
	int i;
	/* XXX: 100% dword access ok here? */
	for (i = 0; i &lt; 16; i++) {
		pci_read_config_dword(dev, i * 4, &amp;dev-&gt;saved_config_space[i]);
		pci_dbg(dev, "save config %#04x: %#010x\n",
			i * 4, dev-&gt;saved_config_space[i]);
	}
	dev-&gt;state_saved = true;

	i = pci_save_pcie_state(dev);
	if (i != 0)
		return i;

	i = pci_save_pcix_state(dev);
	if (i != 0)
		return i;

	pci_save_ltr_state(dev);
	pci_save_dpc_state(dev);
	pci_save_aer_state(dev);
	pci_save_ptm_state(dev);
	return pci_save_vc_state(dev);
}
EXPORT_SYMBOL(pci_save_state);
<br>pci_save_state 함수는 PCI 장치의 현재 상태를 저장하는 역할을 한다. 이 함수는 PCI 구성 공간(Config Space)을 읽어와 저장하고, 다양한 PCIe 확장 상태를 저장한다. 이는 주로 시스템이 절전 모드로 전환되거나, 장치를 재설정할 때 현재 상태를 나중에 복원하기 위해 사용된다.<br><br><br>int pci_save_state(struct pci_dev *dev)
{
	int i;
<br><br>for (i = 0; i &lt; 16; i++) {
    pci_read_config_dword(dev, i * 4, &amp;dev-&gt;saved_config_space[i]);
    pci_dbg(dev, "save config %#04x: %#010x\\n",
        i * 4, dev-&gt;saved_config_space[i]);
}
dev-&gt;state_saved = true;
<br>목적: PCI 구성 공간의 첫 16개 DWORD(64바이트)를 저장한다.<br>설명:<br>
<br>for (i = 0; i &lt; 16; i++): 구성 공간의 첫 16개 DWORD(64바이트)를 순회한다.
<br>pci_read_config_dword(dev, i * 4, &amp;dev-&gt;saved_config_space[i]): pci_read_config_dword 함수를 사용해 구성 공간의 각 DWORD를 읽어 dev-&gt;saved_config_space 배열에 저장한다.
<br>pci_dbg(dev, "save config %#04x: %#010x\n", i * 4, dev-&gt;saved_config_space[i]): 디버그 로그를 출력하여 저장된 값을 확인한다.
<br>dev-&gt;state_saved = true: 상태가 저장되었음을 표시한다.
<br><br>i = pci_save_pcie_state(dev);
if (i != 0)
    return i;

i = pci_save_pcix_state(dev);
if (i != 0)
    return i;

pci_save_ltr_state(dev);
pci_save_dpc_state(dev);
pci_save_aer_state(dev);
pci_save_ptm_state(dev);
return pci_save_vc_state(dev);
<br>목적: 다양한 PCIe 확장 기능 상태를 저장한다.<br>설명:<br>
<br>pci_save_pcie_state(dev): PCI Express 상태를 저장한다. 오류 발생 시 해당 오류를 반환한다.
<br>pci_save_pcix_state(dev): PCI-X 상태를 저장한다. 오류 발생 시 해당 오류를 반환한다.
<br>pci_save_ltr_state(dev): LTR(Latency Tolerance Reporting) 상태를 저장한다.
<br>pci_save_dpc_state(dev): DPC(Device Power Control) 상태를 저장한다.
<br>pci_save_aer_state(dev): AER(Advanced Error Reporting) 상태를 저장한다.
<br>pci_save_ptm_state(dev): PTM(Precision Time Measurement) 상태를 저장한다.
<br>pci_save_vc_state(dev): VC(Virtual Channel) 상태를 저장하고, 반환한다.
<br><br>
<br>PCI 구성 공간 읽기
<br>
<br>pci_read_config_dword 함수는 PCI 장치의 구성 공간에서 지정된 오프셋에서 4바이트(DWORD)를 읽어온다.
<br>이 함수는 PCI 버스에서 지정된 장치의 레지스터 값을 읽어오는데 사용된다.
<br>
<br>디버그 메시지 출력
<br>
<br>pci_dbg 함수는 디버그 메시지를 출력하는데 사용된다.
<br>이 함수는 장치의 디버그 정보를 커널 로그에 기록하여 디버깅을 용이하게 한다.
<br>
<br>PCIe 상태 저장 함수들
<br>
<br>각 pci_save_*_state 함수는 특정 PCIe 확장 기능의 상태를 저장한다.
<br>이 함수들은 각각의 확장 기능에 대한 설정을 읽어와 저장하는 역할을 한다.
<br><br>
<br>함수가 성공적으로 완료되면 마지막으로 호출된 pci_save_vc_state 함수의 반환 값을 반환한다.
<br>중간에 오류가 발생하면 해당 오류 코드를 반환하고 함수를 종료한다.
<br><br>pci_save_state 함수는 PCI 장치의 구성 공간과 다양한 PCIe 확장 상태를 저장하는 중요한 함수이다. 이는 시스템 절전 모드 전환이나 장치 재설정 시 현재 상태를 나중에 복원하기 위해 필요하다. 이 함수는 각 단계에서 발생할 수 있는 오류를 처리하고, 오류가 발생하면 적절히 반환하여 시스템 안정성을 유지한다.]]></description><link>encyclopedia-of-networksystem/function/drivers-pci/pci_save_state().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-pci/pci_save_state().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[pci_set_master()]]></title><description><![CDATA[ 
 <br>/**
 * pci_set_master - enables bus-mastering for device dev
 * @dev: the PCI device to enable
 *
 * Enables bus-mastering on the device and calls pcibios_set_master()
 * to do the needed arch specific settings.
 */
void pci_set_master(struct pci_dev *dev)
{
	__pci_set_master(dev, true);
	pcibios_set_master(dev);
}
EXPORT_SYMBOL(pci_set_master);
<br>PCI 장치를 버스 마스터로 설정하여 DMA를 수행할 수 있도록 해줌.]]></description><link>encyclopedia-of-networksystem/function/drivers-pci/pci_set_master().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/drivers-pci/pci_set_master().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ep_events_available()]]></title><description><![CDATA[ 
 <br>/**
 * ep_events_available - Checks if ready events might be available.
 *
 * @ep: Pointer to the eventpoll context.
 *
 * Return: a value different than %zero if ready events are available,
 *          or %zero otherwise.
 */
static inline int ep_events_available(struct eventpoll *ep)
{
	return !list_empty_careful(&amp;ep-&gt;rdllist) ||
		READ_ONCE(ep-&gt;ovflist) != EP_UNACTIVE_PTR;
}
]]></description><link>encyclopedia-of-networksystem/function/fs/ep_events_available().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/fs/ep_events_available().md</guid><pubDate>Thu, 10 Oct 2024 11:46:39 GMT</pubDate></item><item><title><![CDATA[ep_item_poll()]]></title><description><![CDATA[ 
 <br>/*
 * Differs from ep_eventpoll_poll() in that internal callers already have
 * the ep-&gt;mtx so we need to start from depth=1, such that mutex_lock_nested()
 * is correctly annotated.
 */
static __poll_t ep_item_poll(const struct epitem *epi, poll_table *pt,
				 int depth)
{
	struct file *file = epi_fget(epi);
	__poll_t res;

	/*
	 * We could return EPOLLERR | EPOLLHUP or something, but let's
	 * treat this more as "file doesn't exist, poll didn't happen".
	 */
	if (!file)
		return 0;

	pt-&gt;_key = epi-&gt;event.events;
	if (!is_file_epoll(file))
		res = vfs_poll(file, pt);
	else
		res = __ep_eventpoll_poll(file, pt, depth);
	fput(file);
	return res &amp; epi-&gt;event.events;
}
]]></description><link>encyclopedia-of-networksystem/function/fs/ep_item_poll().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/fs/ep_item_poll().md</guid><pubDate>Thu, 10 Oct 2024 11:46:39 GMT</pubDate></item><item><title><![CDATA[ep_send_events()]]></title><description><![CDATA[ 
 <br>static int ep_send_events(struct eventpoll *ep,
			  struct epoll_event __user *events, int maxevents)
{
	struct epitem *epi, *tmp;
	LIST_HEAD(txlist);
	poll_table pt;
	int res = 0;

	/*
	 * Always short-circuit for fatal signals to allow threads to make a
	 * timely exit without the chance of finding more events available and
	 * fetching repeatedly.
	 */
	if (fatal_signal_pending(current))
		return -EINTR;

	init_poll_funcptr(&amp;pt, NULL);

	mutex_lock(&amp;ep-&gt;mtx);
	ep_start_scan(ep, &amp;txlist);

	/*
	 * We can loop without lock because we are passed a task private list.
	 * Items cannot vanish during the loop we are holding ep-&gt;mtx.
	 */
	list_for_each_entry_safe(epi, tmp, &amp;txlist, rdllink) {
		struct wakeup_source *ws;
		__poll_t revents;

		if (res &gt;= maxevents)
			break;

		/*
		 * Activate ep-&gt;ws before deactivating epi-&gt;ws to prevent
		 * triggering auto-suspend here (in case we reactive epi-&gt;ws
		 * below).
		 *
		 * This could be rearranged to delay the deactivation of epi-&gt;ws
		 * instead, but then epi-&gt;ws would temporarily be out of sync
		 * with ep_is_linked().
		 */
		ws = ep_wakeup_source(epi);
		if (ws) {
			if (ws-&gt;active)
				__pm_stay_awake(ep-&gt;ws);
			__pm_relax(ws);
		}

		list_del_init(&amp;epi-&gt;rdllink);

		/*
		 * If the event mask intersect the caller-requested one,
		 * deliver the event to userspace. Again, we are holding ep-&gt;mtx,
		 * so no operations coming from userspace can change the item.
		 */
		revents = ep_item_poll(epi, &amp;pt, 1);
		if (!revents)
			continue;

		events = epoll_put_uevent(revents, epi-&gt;event.data, events);
		if (!events) {
			list_add(&amp;epi-&gt;rdllink, &amp;txlist);
			ep_pm_stay_awake(epi);
			if (!res)
				res = -EFAULT;
			break;
		}
		res++;
		if (epi-&gt;event.events &amp; EPOLLONESHOT)
			epi-&gt;event.events &amp;= EP_PRIVATE_BITS;
		else if (!(epi-&gt;event.events &amp; EPOLLET)) {
			/*
			 * If this file has been added with Level
			 * Trigger mode, we need to insert back inside
			 * the ready list, so that the next call to
			 * epoll_wait() will check again the events
			 * availability. At this point, no one can insert
			 * into ep-&gt;rdllist besides us. The epoll_ctl()
			 * callers are locked out by
			 * ep_send_events() holding "mtx" and the
			 * poll callback will queue them in ep-&gt;ovflist.
			 */
			list_add_tail(&amp;epi-&gt;rdllink, &amp;ep-&gt;rdllist);
			ep_pm_stay_awake(epi);
		}
	}
	ep_done_scan(ep, &amp;txlist);
	mutex_unlock(&amp;ep-&gt;mtx);

	return res;
}
]]></description><link>encyclopedia-of-networksystem/function/fs/ep_send_events().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/fs/ep_send_events().md</guid><pubDate>Thu, 10 Oct 2024 11:46:39 GMT</pubDate></item><item><title><![CDATA[__skb_put()]]></title><description><![CDATA[ 
 <br>static inline void *__skb_put(struct sk_buff *skb, unsigned int len)
{
	void *tmp = skb_tail_pointer(skb);
	SKB_LINEAR_ASSERT(skb);
	skb-&gt;tail += len;
	skb-&gt;len  += len;
	return tmp;
}
<br>
통째로 집어 넣는 코드. 좀더 찾아봐야함.
]]></description><link>encyclopedia-of-networksystem/function/include-linux/__skb_put().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/include-linux/__skb_put().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[devm_kcalloc()]]></title><description><![CDATA[ 
 <br>static inline void *devm_kcalloc(struct device *dev,
				 size_t n, size_t size, gfp_t flags)
{
	return devm_kmalloc_array(dev, n, size, flags | __GFP_ZERO); // [[Encyclopedia of NetworkSystem/Function/include-linux/devm_kmalloc_array().md|devm_kmalloc_array()]]
}
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/include-linux/devm_kmalloc_array().md" data-href="Encyclopedia of NetworkSystem/Function/include-linux/devm_kmalloc_array().md" href="encyclopedia-of-networksystem/function/include-linux/devm_kmalloc_array().html" class="internal-link" target="_self" rel="noopener nofollow">devm_kmalloc_array()</a>]]></description><link>encyclopedia-of-networksystem/function/include-linux/devm_kcalloc().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/include-linux/devm_kcalloc().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[devm_kmalloc_array()]]></title><description><![CDATA[ 
 <br>static inline void *devm_kmalloc_array(struct device *dev,
				       size_t n, size_t size, gfp_t flags)
{
	size_t bytes;

	if (unlikely(check_mul_overflow(n, size, &amp;bytes)))
		return NULL;

	return devm_kmalloc(dev, bytes, flags); // [[Encyclopedia of NetworkSystem/Function/drivers-base/devm_kmalloc().md|devm_kmalloc()]]
}
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/drivers-base/devm_kmalloc().md" data-href="Encyclopedia of NetworkSystem/Function/drivers-base/devm_kmalloc().md" href="encyclopedia-of-networksystem/function/drivers-base/devm_kmalloc().html" class="internal-link" target="_self" rel="noopener nofollow">devm_kmalloc()</a>]]></description><link>encyclopedia-of-networksystem/function/include-linux/devm_kmalloc_array().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/include-linux/devm_kmalloc_array().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[devm_request_irq()]]></title><description><![CDATA[ 
 <br>static inline int __must_check
devm_request_irq(struct device *dev, unsigned int irq, irq_handler_t handler,
		 unsigned long irqflags, const char *devname, void *dev_id)
{
	return devm_request_threaded_irq(dev, irq, handler, NULL, irqflags,
					 devname, dev_id); // [[Encyclopedia of NetworkSystem/Function/kernel-irq/devm_request_threaded_irq().md|devm_request_threaded_irq()]]
}
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/kernel-irq/devm_request_threaded_irq().md" data-href="Encyclopedia of NetworkSystem/Function/kernel-irq/devm_request_threaded_irq().md" href="encyclopedia-of-networksystem/function/kernel-irq/devm_request_threaded_irq().html" class="internal-link" target="_self" rel="noopener nofollow">devm_request_threaded_irq()</a>]]></description><link>encyclopedia-of-networksystem/function/include-linux/devm_request_irq().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/include-linux/devm_request_irq().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[devres_alloc()]]></title><description><![CDATA[ 
 <br>void *__devres_alloc_node(dr_release_t release, size_t size, gfp_t gfp,
			  int nid, const char *name) __malloc;
#define devres_alloc(release, size, gfp) \
	__devres_alloc_node(release, size, gfp, NUMA_NO_NODE, #release)
	// [[Encyclopedia of NetworkSystem/Function/drivers-base/__devres_alloc_node().md|__devres_alloc_node()]]
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/drivers-base/__devres_alloc_node().md" data-href="Encyclopedia of NetworkSystem/Function/drivers-base/__devres_alloc_node().md" href="encyclopedia-of-networksystem/function/drivers-base/__devres_alloc_node().html" class="internal-link" target="_self" rel="noopener nofollow">__devres_alloc_node()</a>]]></description><link>encyclopedia-of-networksystem/function/include-linux/devres_alloc().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/include-linux/devres_alloc().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[dma_set_mask_and_coherent()]]></title><description><![CDATA[ 
 <br>static inline int dma_set_mask_and_coherent(struct device *dev, u64 mask)
{
	int rc = dma_set_mask(dev, mask);
	if (rc == 0)
		dma_set_coherent_mask(dev, mask);
	return rc;
}
<br>dma를 설정함]]></description><link>encyclopedia-of-networksystem/function/include-linux/dma_set_mask_and_coherent().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/include-linux/dma_set_mask_and_coherent().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[dmam_alloc_coherent()]]></title><description><![CDATA[ 
 <br>static inline void *dmam_alloc_coherent(struct device *dev, size_t size,
		dma_addr_t *dma_handle, gfp_t gfp)
{
	return dmam_alloc_attrs(dev, size, dma_handle, gfp,
			(gfp &amp; __GFP_NOWARN) ? DMA_ATTR_NO_WARN : 0); // [[Encyclopedia of NetworkSystem/Function/kernel-dma/dmam_alloc_attrs().md|dmam_alloc_attrs()]]
}
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/kernel-dma/dmam_alloc_attrs().md" data-href="Encyclopedia of NetworkSystem/Function/kernel-dma/dmam_alloc_attrs().md" href="encyclopedia-of-networksystem/function/kernel-dma/dmam_alloc_attrs().html" class="internal-link" target="_self" rel="noopener nofollow">dmam_alloc_attrs()</a>]]></description><link>encyclopedia-of-networksystem/function/include-linux/dmam_alloc_coherent().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/include-linux/dmam_alloc_coherent().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[irqd_set_activated()]]></title><description><![CDATA[ 
 <br>static inline void irqd_set_activated(struct irq_data *d)
{
	__irqd_to_state(d) |= IRQD_ACTIVATED;
}
]]></description><link>encyclopedia-of-networksystem/function/include-linux/irqd_set_activated().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/include-linux/irqd_set_activated().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[kmalloc_node_track_caller()]]></title><description><![CDATA[ 
 <br>void *__kmalloc_node_track_caller(size_t size, gfp_t flags, int node,
				  unsigned long caller) __alloc_size(1);
				  
#define kmalloc_node_track_caller(size, flags, node) \
	__kmalloc_node_track_caller(size, flags, node, \
				    _RET_IP_)

<br>매크로는 __kmalloc_node_track_caller 함수를 호출하며, _RET_IP_를 사용하여 호출자의 주소를 자동으로 전달한다.]]></description><link>encyclopedia-of-networksystem/function/include-linux/kmalloc_node_track_caller().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/include-linux/kmalloc_node_track_caller().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[kmalloc_node()]]></title><description><![CDATA[ 
 <br>static __always_inline __alloc_size(1) void *kmalloc_node(size_t size, gfp_t flags, int node)
{
	if (__builtin_constant_p(size) &amp;&amp; size) {
		unsigned int index;

		if (size &gt; KMALLOC_MAX_CACHE_SIZE)
			return kmalloc_large_node(size, flags, node);

		index = kmalloc_index(size);
		return kmalloc_node_trace(
				kmalloc_caches[kmalloc_type(flags, _RET_IP_)][index],
				flags, node, size);
	}
	return __kmalloc_node(size, flags, node);
}
]]></description><link>encyclopedia-of-networksystem/function/include-linux/kmalloc_node().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/include-linux/kmalloc_node().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[kzalloc()]]></title><description><![CDATA[ 
 <br>/**
 * kzalloc - allocate memory. The memory is set to zero.
 * @size: how many bytes of memory are required.
 * @flags: the type of memory to allocate (see kmalloc).
 */
static inline __alloc_size(1) void *kzalloc(size_t size, gfp_t flags)
{
	return kmalloc(size, flags | __GFP_ZERO);
}
]]></description><link>encyclopedia-of-networksystem/function/include-linux/kzalloc().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/include-linux/kzalloc().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[list_add_tail()]]></title><description><![CDATA[ 
 <br>/**
 * list_add_tail - add a new entry
 * @new: new entry to be added
 * @head: list head to add it before
 *
 * Insert a new entry before the specified head.
 * This is useful for implementing queues.
 */
static inline void list_add_tail(struct list_head *new, struct list_head *head)
{
	__list_add(new, head-&gt;prev, head);
}
<br>list_add_tail(&amp;napi→poll_list, &amp;sd→poll_list)를 통해서 softnet_data 구조체의 poll_list에다가 napi_struct 구조체의 poll_list의 내용을 추가하게 됨.]]></description><link>encyclopedia-of-networksystem/function/include-linux/list_add_tail().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/include-linux/list_add_tail().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[list_del_init() ]]></title><description><![CDATA[ 
 <br>/**
 * list_del_init - deletes entry from list and reinitialize it.
 * @entry: the element to delete from the list.
 */
static inline void list_del_init(struct list_head *entry)
{
	__list_del_entry(entry);
	INIT_LIST_HEAD(entry);
}
]]></description><link>encyclopedia-of-networksystem/function/include-linux/list_del_init()-.md/list_del_init()-.md.html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/include-linux/list_del_init() .md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[napi_complete() ]]></title><description><![CDATA[ 
 <br>/**
 * napi_complete_done - NAPI processing complete
 * @n: NAPI context
 * @work_done: number of packets processed
 *
 * Mark NAPI processing as complete. Should only be called if poll budget
 * has not been completely consumed.
 * Prefer over napi_complete().
 * Return false if device should avoid rearming interrupts.
 */
bool napi_complete_done(struct napi_struct *n, int work_done);

static inline bool napi_complete(struct napi_struct *n)
{
	return napi_complete_done(n, 0); // [[Encyclopedia of NetworkSystem/Function/net-core/napi_complete_done().md|napi_complete_done()]]
}
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-core/napi_complete_done().md" data-href="Encyclopedia of NetworkSystem/Function/net-core/napi_complete_done().md" href="encyclopedia-of-networksystem/function/net-core/napi_complete_done().html" class="internal-link" target="_self" rel="noopener nofollow">napi_complete_done()</a>]]></description><link>encyclopedia-of-networksystem/function/include-linux/napi_complete()-.md/napi_complete()-.md.html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/include-linux/napi_complete() .md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[napi_prefer_busy_poll() ]]></title><description><![CDATA[ 
 <br>static inline bool napi_prefer_busy_poll(struct napi_struct *n)
{
	return test_bit(NAPI_STATE_PREFER_BUSY_POLL, &amp;n-&gt;state);
}
<br>
napi_struct의 state 변수가 NAPI_STATE_PREFER_BUSY_POLL이 셋팅 되어있는지 test_bit을 하게 되는 함수
]]></description><link>encyclopedia-of-networksystem/function/include-linux/napi_prefer_busy_poll()-.md/napi_prefer_busy_poll()-.md.html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/include-linux/napi_prefer_busy_poll() .md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[napi_schedule()]]></title><description><![CDATA[ 
 <br>static inline bool napi_schedule(struct napi_struct *n)
{
	if (napi_schedule_prep(n)) { // [[Encyclopedia of NetworkSystem/Function/net-core/napi_schedule_prep().md|napi_schedule_prep()]]
		__napi_schedule(n); // [[Encyclopedia of NetworkSystem/Function/net-core/__napi_schedule().md|__napi_schedule()]]
		return true;
	}

	return false;
}
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-core/napi_schedule_prep().md" data-href="Encyclopedia of NetworkSystem/Function/net-core/napi_schedule_prep().md" href="encyclopedia-of-networksystem/function/net-core/napi_schedule_prep().html" class="internal-link" target="_self" rel="noopener nofollow">napi_schedule_prep()</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-core/__napi_schedule().md" data-href="Encyclopedia of NetworkSystem/Function/net-core/__napi_schedule().md" href="encyclopedia-of-networksystem/function/net-core/__napi_schedule().html" class="internal-link" target="_self" rel="noopener nofollow">__napi_schedule()</a><br>위 코드는 네트워크 서브시스템에서 사용되는 NAPI(Network API)의 napi_schedule 함수를 정의하고 있다. 이 함수는 napi_struct 구조체를 스케줄링하여, 네트워크 패킷 처리를 위해 준비된 작업을 큐에 추가하는 역할을 한다.<br>static inline bool napi_schedule(struct napi_struct *n)
<br>
<br>static inline: 이 함수는 정적 인라인 함수로, 컴파일러가 호출하는 대신 함수의 내용을 호출 위치에 삽입하려고 시도한다. 이는 성능을 최적화하기 위해 사용된다.
<br>bool: 함수는 boolean 값을 반환한다.
<br>napi_schedule: 함수 이름이다.
<br>struct napi_struct *n: 함수는 napi_struct 구조체에 대한 포인터 n을 인자로 받는다.
<br>{
	if (napi_schedule_prep(n)) {
<br>
<br>if (napi_schedule_prep(n)): napi_schedule_prep 함수는 NAPI 구조체 n이 스케줄링 가능한지 여부를 확인한다. 스케줄링이 가능하면 true를 반환하고, 그렇지 않으면 false를 반환한다.
<br>        __napi_schedule(n);
        return true;
    }
<br>
<br>__napi_schedule(n): napi_schedule_prep 함수가 true를 반환하면, __napi_schedule 함수를 호출하여 실제로 NAPI 스케줄링을 수행한다. 이는 NAPI 구조체를 활성화하고, 네트워크 패킷 처리를 위한 작업을 큐에 추가한다.
<br>return true;: NAPI 스케줄링이 성공적으로 수행되었음을 나타내기 위해 true를 반환한다.
<br>    return false;
}
<br>
<br>return false;: napi_schedule_prep 함수가 false를 반환하면, 즉 NAPI 스케줄링이 가능하지 않다면 false를 반환한다.
<br><br>napi_schedule 함수는 napi_struct 구조체가 스케줄링 가능한지 확인하고, 가능하다면 실제 스케줄링을 수행하여 true를 반환한다. 그렇지 않으면 false를 반환한다. 이 함수는 네트워크 인터럽트를 처리하고, 네트워크 패킷 처리를 효율적으로 관리하기 위해 사용된다.]]></description><link>encyclopedia-of-networksystem/function/include-linux/napi_schedule().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/include-linux/napi_schedule().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[netif_elide_gro()]]></title><description><![CDATA[ 
 <br>static inline bool netif_elide_gro(const struct net_device *dev) // [[Encyclopedia of NetworkSystem/Struct/include-linux/net_device.md|net_device]]
{
	if (!(dev-&gt;features &amp; NETIF_F_GRO) || dev-&gt;xdp_prog)
		return true;
	return false;
}
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Struct/include-linux/net_device.md" data-href="Encyclopedia of NetworkSystem/Struct/include-linux/net_device.md" href="encyclopedia-of-networksystem/struct/include-linux/net_device.html" class="internal-link" target="_self" rel="noopener nofollow">net_device</a><br>
<br>GRO 기능 확인:
<br>
<br>!(dev-&gt;features &amp; NETIF_F_GRO): 네트워크 장치의 기능 비트마스크에서 GRO 기능이 활성화되지 않은 경우 true를 반환한다. 이는 dev-&gt;features 필드에서 NETIF_F_GRO 플래그를 확인하는 것이다.
<br>
<br>XDP 프로그램 확인:
<br>
<br>dev-&gt;xdp_prog: 네트워크 장치에 XDP(eXpress Data Path) 프로그램이 설정되어 있는 경우 true를 반환한다. 이는 GRO를 사용할 수 없는 상태를 의미한다.
<br>
<br>결과 반환:
<br>
<br>위의 조건 중 하나라도 만족하면 true를 반환하여 GRO를 생략하도록 한다.
<br>두 조건 모두 만족하지 않으면 false를 반환하여 GRO를 사용하도록 한다.
]]></description><link>encyclopedia-of-networksystem/function/include-linux/netif_elide_gro().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/include-linux/netif_elide_gro().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[netif_napi_add()]]></title><description><![CDATA[ 
 <br>/* Default NAPI poll() weight
 * Device drivers are strongly advised to not use bigger value
 */
 
#define NAPI_POLL_WEIGHT 64

/**
 * netif_napi_add() - initialize a NAPI context
 * @dev:  network device
 * @napi: NAPI context
 * @poll: polling function
 *
 * netif_napi_add() must be used to initialize a NAPI context prior to calling
 * *any* of the other NAPI-related functions.
 */
static inline void
netif_napi_add(struct net_device *dev, struct napi_struct *napi,
	       int (*poll)(struct napi_struct *, int)) // [[Encyclopedia of NetworkSystem/Struct/include-linux/napi_struct.md|napi_struct]]
{
	netif_napi_add_weight(dev, napi, poll, NAPI_POLL_WEIGHT);
	// [[Encyclopedia of NetworkSystem/Function/net-core/netif_napi_add_weight().md|netif_napi_add_weight()]]
}
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Struct/include-linux/napi_struct.md" data-href="Encyclopedia of NetworkSystem/Struct/include-linux/napi_struct.md" href="encyclopedia-of-networksystem/struct/include-linux/napi_struct.html" class="internal-link" target="_self" rel="noopener nofollow">napi_struct</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-core/netif_napi_add_weight().md" data-href="Encyclopedia of NetworkSystem/Function/net-core/netif_napi_add_weight().md" href="encyclopedia-of-networksystem/function/net-core/netif_napi_add_weight().html" class="internal-link" target="_self" rel="noopener nofollow">netif_napi_add_weight()</a><br>
<br>netif_napi_add_weight(dev, napi, poll, NAPI_POLL_WEIGHT); 가 안에 있다<br>
a. NAPI_POLL_WEIGHT == 64
<br>if (WARN_ON(test_and_set_bit(NAPI_STATE_LISTED, &amp;napi-&gt;state)))<br>
a. 이미 napi에 list가 있는 상태면 넘어간다
<br>list의 초기값 설정, timer 초기값 설정
<br>init_gro_hash(napi);<br>

<br>        static void `init_gro_hash(`struct napi_struct *napi)
        {
        	int i;
        
        	for (i = 0; i &lt; GRO_HASH_BUCKETS; i++) {
        		INIT_LIST_HEAD(&amp;napi-&gt;gro_hash[i].list);
        		napi-&gt;gro_hash[i].count = 0;
        	}
        	napi-&gt;gro_bitmask = 0;
        }
<br>1. gro 준비
2. gro_hash_bucket의 개수만큼 list를 초기화 해준다
<br>
<br>napi 에 skb가 있는 이유<br>
a. napi_struct를 이어서 poll list를 만든다 ?
<br>napi_kthread_create(napi)<br>

<br>        static int napi_kthread_create(struct napi_struct *n)
        {
        	int err = 0;
        
        	/* Create and wake up the kthread once to put it in
        	 * TASK_INTERRUPTIBLE mode to avoid the blocked task
        	 * warning and work with loadavg.
        	 */
        	n-&gt;thread = kthread_run(napi_threaded_poll, n, "napi/%s-%d",
        				n-&gt;dev-&gt;name, n-&gt;napi_id);
        	if (IS_ERR(n-&gt;thread)) {
        		err = PTR_ERR(n-&gt;thread);
        		pr_err("kthread_run failed with err %d\\n", err);
        		n-&gt;thread = NULL;
        	}
        
        	return err;
        }
<br>1. kthread를 만들면서 깨우고 interruptible mode로 설정한다
2. blocked task warning을 피하고 loadavg를 사용하기 위해 한다 ????
   
<br>
<br>netif_napi_set_irq(napi, -1);<br>
a. napi_struct에 있는 irq 변수를 -1로 설정한다<br>
b. enum 으로 설정되어 있을 것 같은데 -1로 설정한 것 보아 irq disable 한거랑 연관있을 듯?<br>
c. napi-&gt;irq = irq;
<br>
<br>Types of NAPI state
  enum {
  	NAPI_STATE_SCHED,		/* Poll is scheduled */
  	NAPI_STATE_MISSED,		/* reschedule a napi */
  	NAPI_STATE_DISABLE,		/* Disable pending */
  	NAPI_STATE_NPSVC,		/* Netpoll - don't dequeue from poll_list */
  	NAPI_STATE_LISTED,		/* NAPI added to system lists */
  	NAPI_STATE_NO_BUSY_POLL,	/* Do not add in napi_hash, no busy polling */
  	NAPI_STATE_IN_BUSY_POLL,	/* sk_busy_loop() owns this NAPI */
  	NAPI_STATE_PREFER_BUSY_POLL,	/* prefer busy-polling over softirq processing*/
  	NAPI_STATE_THREADED,		/* The poll is performed inside its own thread*/
  	NAPI_STATE_SCHED_THREADED,	/* Napi is currently scheduled in threaded mode */
  };


]]></description><link>encyclopedia-of-networksystem/function/include-linux/netif_napi_add().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/include-linux/netif_napi_add().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[netif_tx_start_all_queues()]]></title><description><![CDATA[ 
 <br>static inline void netif_tx_start_all_queues(struct net_device *dev)
{
	unsigned int i;

	for (i = 0; i &lt; dev-&gt;num_tx_queues; i++) {
		struct netdev_queue *txq = netdev_get_tx_queue(dev, i);
		netif_tx_start_queue(txq);
	}
}
]]></description><link>encyclopedia-of-networksystem/function/include-linux/netif_tx_start_all_queues().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/include-linux/netif_tx_start_all_queues().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[NF_HOOK()]]></title><description><![CDATA[ 
 <br>static inline int
NF_HOOK(uint8_t pf, unsigned int hook, struct net *net, struct sock *sk, struct sk_buff *skb,
	struct net_device *in, struct net_device *out,
	int (*okfn)(struct net *, struct sock *, struct sk_buff *))
{
	int ret = nf_hook(pf, hook, net, sk, skb, in, out, okfn);
	if (ret == 1)
		ret = okfn(net, sk, skb);
	return ret;
}
<br>
여기는 CONFIG_NETFILTER 조건이 true일 때 활성화 된다.<br>
우선 nf_hook()함수를 호출해주고 만약 ret이 1이라면 그제서야 okfn()을 실행하게 된다.
<br>
<br>pf : protocol family 
<br>hook : 후킹 값 중 하나 (NF_INET_PRE_ROUTING, .... )
<br>net : network namespace
<br>sk : socket
<br>in : 입력 네트워크 장치
<br>out : 출력 네트워크 장치
<br>okfn : 훅이 종료되면 호출될 함수의 포인터
<br>넷필터 훅을 등록하는 매크로이다.<br>
nf_hook() 함수를 실행하고 리턴값이 있으면 okfn() 함수를 실행한다.<br>
okfn은 call back function으로 ip_rcv_finish() 함수이다.<br>넷필터 훅은 리눅스 네트워크 스택에서 패킷이 지나가는 여러 시점에 연결되어 네트워크 모듈이 패킷을 처리할 수 있게 하는 인터페이스이다. 이 훅을 통해 패킷이 들어올 때, 라우팅될 때, 나갈 때 등을 감지할 수 있으며, 이 시점에 패킷을 허용, 수정, 차단, 또는 다른 경로로 전송하는 등의 작업을 할 수 있다. 주로 방화벽, 패킷 필터링, NAT(Network Address Transition)에서 사용된다.<br>
ip_rcv 과정에서 넷필터 훅을 거치는 이유는 패킷을 수신할 때, 방화벽 규칙, NAT(Network Address Transition), 패킷 필터링 등을 적용하기 위함이다. <br><a data-href="nf_hook()_" href="encyclopedia-of-networksystem/function/include-linux/nf_hook()_.html" class="internal-link" target="_self" rel="noopener nofollow">nf_hook()_</a><br>
<a data-href="ip_rcv_finish()" href="encyclopedia-of-networksystem/function/net-ipv4/ip_rcv_finish().html" class="internal-link" target="_self" rel="noopener nofollow">ip_rcv_finish()</a>]]></description><link>encyclopedia-of-networksystem/function/include-linux/nf_hook().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/include-linux/NF_HOOK().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[nf_hook()_]]></title><description><![CDATA[ 
 <br>/**
* nf_hook - call a netfilter hook
*
* Returns 1 if the hook has allowed the packet to pass. The function
* okfn must be invoked by the caller in this case. Any other return
* value indicates the packet has been consumed by the hook.
*/
static inline int nf_hook(u_int8_t pf, unsigned int hook, struct net *net,
				struct sock *sk, struct sk_buff *skb,
				struct net_device *indev, struct net_device *outdev,
				int (*okfn)(struct net *, struct sock *, struct sk_buff *))
{
	struct nf_hook_entries *hook_head = NULL;
	int ret = 1;
  
#ifdef CONFIG_JUMP_LABEL
	if (__builtin_constant_p(pf) &amp;&amp;
		__builtin_constant_p(hook) &amp;&amp;
		!static_key_false(&amp;nf_hooks_needed[pf][hook]))
		return 1;
#endif
	  
	rcu_read_lock();
	switch (pf) {
	case NFPROTO_IPV4:
		hook_head = rcu_dereference(net-&gt;nf.hooks_ipv4[hook]);
		break;
	case NFPROTO_IPV6:
		hook_head = rcu_dereference(net-&gt;nf.hooks_ipv6[hook]);
		break;
	case NFPROTO_ARP:
#ifdef CONFIG_NETFILTER_FAMILY_ARP
		if (WARN_ON_ONCE(hook &gt;= ARRAY_SIZE(net-&gt;nf.hooks_arp)))
			break;
		hook_head = rcu_dereference(net-&gt;nf.hooks_arp[hook]);
#endif
		break;
	case NFPROTO_BRIDGE:
#ifdef CONFIG_NETFILTER_FAMILY_BRIDGE
		hook_head = rcu_dereference(net-&gt;nf.hooks_bridge[hook]);
#endif
		break;
	default:
		WARN_ON_ONCE(1);
		break;
}
  
if (hook_head) {
struct nf_hook_state state;
  
nf_hook_state_init(&amp;state, hook, pf, indev, outdev,
sk, net, okfn);
  
ret = nf_hook_slow(skb, &amp;state, hook_head, 0);
}
rcu_read_unlock();
  
return ret;
}
<br>
주어진 protocol flag를 통해서 해당하는 hook_head를 가져오고, nf_hook_state타입의 state를 초기화 한 뒤, 이를 가지고 nf_hook_slow()함수를  실행하게 된다. 여기서 초기화하는 함수는 간단하다. nf_hook_state타입의 변수에 hook int와 pf, indev, outdev, sk, net, okfn 등을 주어진 parameter에서 가져와서 설정하게 된다.<br>
그리고 마지막으로 nf_hook_slow()함수에서 나온 결과를 반환하게 된다.
<br><br>
CONFIG_JUMP_LABEL이 활성화되어 있는 경우에 실행되는 코드. return value가 1이면 hook이 pkt<br>
을 지나가게 허용했다는 뜻이다. 아래의 추가적인 작업 없이 빠르게 처리하는 경우로 보인다. 
<br>
protocol family 에 맞는 hook을 실행해주기 위해 switch문을 사용한다.<br>
ipv4, ipv6, arp, bridge, default로 나누어져있다.<br>
각자 networking namespace에 저장되어있는 hook list를 코드의 가장 위에 있던 hook_head에<br>
넣어준다.<br>
wtf is networking namespace net??
<br>
hook head에 protocol에 맞는  hook list를 저장하는 데 성공하면 nf_hook_state_init()를 실행<br>
이후 nf_hook_slow()를 실행하고 그 결과값을 리턴한다. pkt이 hook을 지나가는데 성공하면 1을<br>
리턴하고 그 외는 hook 이 pkt을 consume 했다는 의미이다. 
<br><a data-href="nf_hook_slow()" href="encyclopedia-of-networksystem/function/net-netfilter/nf_hook_slow().html" class="internal-link" target="_self" rel="noopener nofollow">nf_hook_slow()</a><br>struct nf_hook_entries {
	u16				num_hook_entries;
	/* padding */
	struct nf_hook_entry		hooks[];

	/* trailer: pointers to original orig_ops of each hook,
	 * followed by rcu_head and scratch space used for freeing
	 * the structure via call_rcu.
	 *
	 *   This is not part of struct nf_hook_entry since its only
	 *   needed in slow path (hook register/unregister):
	 * const struct nf_hook_ops     *orig_ops[]
	 *
	 *   For the same reason, we store this at end -- its
	 *   only needed when a hook is deleted, not during
	 *   packet path processing:
	 * struct nf_hook_entries_rcu_head     head
	 */
};
]]></description><link>encyclopedia-of-networksystem/function/include-linux/nf_hook()_.html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/include-linux/nf_hook()_.md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[skb_copy_datagram_msg()]]></title><description><![CDATA[ 
 <br>static inline int skb_copy_datagram_msg(const struct sk_buff *from, int offset,
					struct msghdr *msg, int size)
{
	return skb_copy_datagram_iter(from, offset, &amp;msg-&gt;msg_iter, size);
}
<br><a data-href="skb_copy_datagram_iter()" href="encyclopedia-of-networksystem/function/net-core/skb_copy_datagram_iter().html" class="internal-link" target="_self" rel="noopener nofollow">skb_copy_datagram_iter()</a>]]></description><link>encyclopedia-of-networksystem/function/include-linux/skb_copy_datagram_msg().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/include-linux/skb_copy_datagram_msg().md</guid><pubDate>Tue, 08 Oct 2024 08:24:56 GMT</pubDate></item><item><title><![CDATA[skb_propagate_pfmemalloc()]]></title><description><![CDATA[ 
 <br>/**
 *	skb_propagate_pfmemalloc - Propagate pfmemalloc if skb is allocated after RX page
 *	@page: The page that was allocated from skb_alloc_page
 *	@skb: The skb that may need pfmemalloc set
 */
static inline void skb_propagate_pfmemalloc(const struct page *page,
					    struct sk_buff *skb)
{
	if (page_is_pfmemalloc(page))
		skb-&gt;pfmemalloc = true;
}
]]></description><link>encyclopedia-of-networksystem/function/include-linux/skb_propagate_pfmemalloc().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/include-linux/skb_propagate_pfmemalloc().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[skb_record_rx_queue()]]></title><description><![CDATA[ 
 <br>static inline void skb_record_rx_queue(struct sk_buff *skb, u16 rx_queue)
{
	skb-&gt;queue_mapping = rx_queue + 1;
}
<br>
skb에 해당하는 rx_queue를 기록하는 간단한 인라인 함수이다.
]]></description><link>encyclopedia-of-networksystem/function/include-linux/skb_record_rx_queue().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/include-linux/skb_record_rx_queue().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[dst_input()]]></title><description><![CDATA[ 
 <br>static inline int dst_input(struct sk_buff *skb)
{
	return INDIRECT_CALL_INET(skb_dst(skb)-&gt;input,
				  ip6_input, ip_local_deliver, skb);
}
<br>
간단하게 INDIRECT_CALL_INET 함수 매크로를 호출하는 코드이다. 여기서도 ip 버전에 따라서 간접적으로 함수 호출이 이루어지게 된다. 여기서 중요하게 봐야할 것은 dst_input함수 그자체이다. dst_output함수도 존재하며, 똑같이 INDIRECT_CALL_INET 함수 매크로를 호출하게 된다. 둘의 차이점은 dst_entry라는 구조체를 바탕으로 처리되는 것들이다.<br>
dst_entry는 패킷을 처리하는데 네트워크 경로에 대한 정보를 저장하는 구조체이다. skb 구조체 안에 포인터로 가르키고 있으며, input과 output 함수 포인터를 가지고 있다. 각각 패킷이 입력될 때와 출력될 때 호출되는 함수들이다.
<br><a data-href="ip_local_deliver()" href="encyclopedia-of-networksystem/function/net-ipv4/ip_local_deliver().html" class="internal-link" target="_self" rel="noopener nofollow">ip_local_deliver()</a>]]></description><link>encyclopedia-of-networksystem/function/include-net/dst_input().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/include-net/dst_input().md</guid><pubDate>Tue, 08 Oct 2024 08:24:56 GMT</pubDate></item><item><title><![CDATA[fib_lookup()]]></title><description><![CDATA[ 
 <br>static inline int fib_lookup(struct net *net, const struct flowi4 *flp,
                 struct fib_result *res, unsigned int flags)
{
    struct fib_table *tb;
    int err = -ENETUNREACH;
  
    rcu_read_lock();
  
    tb = fib_get_table(net, RT_TABLE_MAIN);
    if (tb)
        err = fib_table_lookup(tb, flp, res, flags | FIB_LOOKUP_NOREF);
  
    if (err == -EAGAIN)
        err = -ENETUNREACH;
  
    rcu_read_unlock();
  
    return err;
}
<br>
주어진 flowi4 구조체와 fib_result구조체를 세팅하는 함수. 주어진 net을 parameter로 사용하여 fib_get_table()함수를 호출하고, 여기서 fib_table을 얻게 된다.<br>
테이블을 얻는 fib_get_table()함수의 로직은 간단하다. 인자로 RT_TABLE_MAIN을 가지고 들어왔는데, 함수 내부에서는 RT_TABLE_LOCAL 여부를 바탕으로 &amp;net-&gt;ipv4.fib_table_hash[]에서 index를 TABLE_LOCAL_INDEX 혹은 TABLE_MAIN_INDEX로 집어 넣게 된다.<br>
그 후 해당 포인터를 가지고 tb_hlist를 가져와서 그 첫번째 entry를 가져와서 여기에 해당하는 구조체를 container_of함수로 불러오고, 이를 반환하게 된다. 따라서 fib_table타입의 결과값을 얻을 수 있게 된다.
이 테이블을 가지고 fib_table_lookup()함수를 호출하여 주어진 테이블을 lookup하여 포워딩을 위한 fib를 찾게 된다.
<br><a data-href="fib_table_lookup()" href="encyclopedia-of-networksystem/function/net-ipv4/fib_table_lookup().html" class="internal-link" target="_self" rel="noopener nofollow">fib_table_lookup()</a>]]></description><link>encyclopedia-of-networksystem/function/include-net/fib_lookup().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/include-net/fib_lookup().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[gro_normal_list()]]></title><description><![CDATA[ 
 <br>/* Pass the currently batched GRO_NORMAL SKBs up to the stack. */
static inline void gro_normal_list(struct napi_struct *napi)
{
	if (!napi-&gt;rx_count)
		return;
	netif_receive_skb_list_internal(&amp;napi-&gt;rx_list); // [[Encyclopedia of NetworkSystem/Function/net-core/netif_receive_skb_list_internal().md|netif_receive_skb_list_internal()]]
	INIT_LIST_HEAD(&amp;napi-&gt;rx_list);
	napi-&gt;rx_count = 0;
}
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-core/netif_receive_skb_list_internal().md" data-href="Encyclopedia of NetworkSystem/Function/net-core/netif_receive_skb_list_internal().md" href="encyclopedia-of-networksystem/function/net-core/netif_receive_skb_list_internal().html" class="internal-link" target="_self" rel="noopener nofollow">netif_receive_skb_list_internal()</a><br>
napi-&gt;rx_list를 통째로 parameter로 넘기면서, napi-&gt;rx_list와 napi-&gt;rx_count를 초기화 하고 있다. 따라서, netif_receive_skb_list_internal에서 해당 리스트에 있는 skb들은 완전히 스택으로 넘어가서 처리되는 것으로 보인다.
]]></description><link>encyclopedia-of-networksystem/function/include-net/gro_normal_list().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/include-net/gro_normal_list().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[gro_normal_one()]]></title><description><![CDATA[ 
 <br>/* Queue one GRO_NORMAL SKB up for list processing. If batch size exceeded,
 * pass the whole batch up to the stack.
 */
static inline void gro_normal_one(struct napi_struct *napi, struct sk_buff *skb, int segs)
{
	list_add_tail(&amp;skb-&gt;list, &amp;napi-&gt;rx_list);
	napi-&gt;rx_count += segs;
	if (napi-&gt;rx_count &gt;= READ_ONCE(net_hotdata.gro_normal_batch))
		gro_normal_list(napi); // [[Encyclopedia of NetworkSystem/Function/include-net/gro_normal_list().md|gro_normal_list()]]
}
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/include-net/gro_normal_list().md" data-href="Encyclopedia of NetworkSystem/Function/include-net/gro_normal_list().md" href="encyclopedia-of-networksystem/function/include-net/gro_normal_list().html" class="internal-link" target="_self" rel="noopener nofollow">gro_normal_list()</a><br>
skb→list에다가 napi→rx_list를 추가하게 되며, 합친 segment 수만큼 rx_count를 증가하게 된다. 만약 gro_normal_batch보다 크기가 커진다면 이 전체를 스택에 넘기게 된다.
<br>
해당 skb를 napi-&gt;rx_list에 새로이 올리게 된다. 여기서 segs는 skb의 cb의 count값인데, 이는 aggregated된 packet의 수 이다. 만약 이 rx_count의 종합이 net_hotdata에서 가져온 gro_normal_batch보다 크거나 같게 된다면 gro_normal_list()함수를 호출하게 된다. 그게 아니라면 그냥 넘어가게 된다. 그렇다면,<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/include-net/gro_normal_list().md" data-href="Encyclopedia of NetworkSystem/Function/include-net/gro_normal_list().md" href="encyclopedia-of-networksystem/function/include-net/gro_normal_list().html" class="internal-link" target="_self" rel="noopener nofollow">gro_normal_list()</a>
]]></description><link>encyclopedia-of-networksystem/function/include-net/gro_normal_one().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/include-net/gro_normal_one().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[include-net]]></title><description><![CDATA[ 
 ]]></description><link>encyclopedia-of-networksystem/function/include-net/include-net.html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/include-net/include-net.md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[xdp_prepare_buff()]]></title><description><![CDATA[ 
 <br>static __always_inline void
xdp_prepare_buff(struct xdp_buff *xdp, unsigned char *hard_start,
		 int headroom, int data_len, const bool meta_valid)
{
	unsigned char *data = hard_start + headroom;

	xdp-&gt;data_hard_start = hard_start;
	xdp-&gt;data = data;
	xdp-&gt;data_end = data + data_len;
	xdp-&gt;data_meta = meta_valid ? data : data + 1;
}
<br>
주어진 xdp 포인터에다가 hard_start, data, data_end, data_meta를 넣어서 준비시켜준다.
]]></description><link>encyclopedia-of-networksystem/function/include-net/xdp_prepare_buff().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/include-net/xdp_prepare_buff().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[__do_softirq()]]></title><description><![CDATA[ 
 <br>asmlinkage __visible void __softirq_entry __do_softirq(void)
{
	handle_softirqs(false); // [[Encyclopedia of NetworkSystem/Function/kernel/handle_softirqs().md|handle_softirqs()]]
}
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/kernel/handle_softirqs().md" data-href="Encyclopedia of NetworkSystem/Function/kernel/handle_softirqs().md" href="encyclopedia-of-networksystem/function/kernel/handle_softirqs().html" class="internal-link" target="_self" rel="noopener nofollow">handle_softirqs()</a>]]></description><link>encyclopedia-of-networksystem/function/kernel/__do_softirq().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/kernel/__do_softirq().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[__raise_softirq_irqoff()]]></title><description><![CDATA[ 
 <br>void __raise_softirq_irqoff(unsigned int nr)
{
	lockdep_assert_irqs_disabled();
	trace_softirq_raise(nr);
	or_softirq_pending(1UL &lt;&lt; nr); 
	// 해당하는 softirq interrupt를 비트마스크를 통해 활성화 시키면 해당 인터럽트가  
	// 실행 대기 상태가 되고, 커널이 이를 확인하여 인터럽트를 처리함. 
}
<br>enum
{
	HI_SOFTIRQ=0,
	TIMER_SOFTIRQ,
	NET_TX_SOFTIRQ,
	NET_RX_SOFTIRQ,
	BLOCK_SOFTIRQ,
	IRQ_POLL_SOFTIRQ,
	TASKLET_SOFTIRQ,
	SCHED_SOFTIRQ,
	HRTIMER_SOFTIRQ,
	RCU_SOFTIRQ,    /* Preferable RCU should always be the last softirq */

	NR_SOFTIRQS
};
<br>NET_RX_SOFTIRQ는 enum으로, 3이라는 값을 가진다. 참고로 TX는 2이다.<br>
1UL&lt;&lt;3으로, 4번째 비트를 켜게 된다. 이를 통해 CPU가 해당 값을 알아차리고<br>
jiffies를 사용하여 sotfIRQ의 timeout을 구현하게 된다. 이 때 최대 값은 MAX_SOFTIRQ_TIME이며, 이를 더하여 할당된 자원을 알 수 있게 된다.]]></description><link>encyclopedia-of-networksystem/function/kernel/__raise_softirq_irqoff().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/kernel/__raise_softirq_irqoff().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[handle_softirqs()]]></title><description><![CDATA[ 
 <br>static void handle_softirqs(bool ksirqd)
{
	unsigned long end = jiffies + MAX_SOFTIRQ_TIME;
	unsigned long old_flags = current-&gt;flags;
	int max_restart = MAX_SOFTIRQ_RESTART;
	struct softirq_action *h;
	bool in_hardirq;
	__u32 pending;
	int softirq_bit;

	/*
	 * Mask out PF_MEMALLOC as the current task context is borrowed for the
	 * softirq. A softirq handled, such as network RX, might set PF_MEMALLOC
	 * again if the socket is related to swapping.
	 */
	current-&gt;flags &amp;= ~PF_MEMALLOC;

	pending = local_softirq_pending(); //softIRQ에 대한 모든 bitmap을 불러옴

	softirq_handle_begin();
	in_hardirq = lockdep_softirq_start();
	account_softirq_enter(current);

restart:
	/* Reset the pending bitmask before enabling irqs */
	set_softirq_pending(0);

	local_irq_enable();

	h = softirq_vec;

	while ((softirq_bit = ffs(pending))) { //ffs : set 된 것 중 가장 낮은 거리를 찾음.
	// 즉 set된 모든 비트가 0이 될 때까지 아래의 코드를 반복함. 
		unsigned int vec_nr;
		int prev_count;

		h += softirq_bit - 1;

		vec_nr = h - softirq_vec;
		prev_count = preempt_count();

		kstat_incr_softirqs_this_cpu(vec_nr);

		trace_softirq_entry(vec_nr);
		h-&gt;action(h); // 실제 softIRQ가 실행되는 부분
		trace_softirq_exit(vec_nr);
		if (unlikely(prev_count != preempt_count())) {
			pr_err("huh, entered softirq %u %s %p with preempt_count %08x, exited with %08x?\n",
			       vec_nr, softirq_to_name[vec_nr], h-&gt;action,
			       prev_count, preempt_count());
			preempt_count_set(prev_count);
		}
		h++;
		pending &gt;&gt;= softirq_bit;
	}

	if (!IS_ENABLED(CONFIG_PREEMPT_RT) &amp;&amp; ksirqd)
		rcu_softirq_qs();

	local_irq_disable();

	pending = local_softirq_pending();
	if (pending) {
		if (time_before(jiffies, end) &amp;&amp; !need_resched() &amp;&amp;
		    --max_restart)
			goto restart;

		wakeup_softirqd();
	}

	account_softirq_exit(current);
	lockdep_softirq_end(in_hardirq);
	softirq_handle_end();
	current_restore_flags(old_flags, PF_MEMALLOC);
}
<br>
h→action(h)에서 함수포인터를 통해 미리 매핑되어있던 <a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-core/net_rx_action().md" data-href="Encyclopedia of NetworkSystem/Function/net-core/net_rx_action().md" href="encyclopedia-of-networksystem/function/net-core/net_rx_action().html" class="internal-link" target="_self" rel="noopener nofollow">net_rx_action</a>이 실행되게 됨.
]]></description><link>encyclopedia-of-networksystem/function/kernel/handle_softirqs().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/kernel/handle_softirqs().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[kernel]]></title><description><![CDATA[ 
 ]]></description><link>encyclopedia-of-networksystem/function/kernel/kernel.html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/kernel/kernel.md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[dma_alloc_attrs()]]></title><description><![CDATA[ 
 <br>void *dma_alloc_attrs(struct device *dev, size_t size, dma_addr_t *dma_handle,
		gfp_t flag, unsigned long attrs)
{
	const struct dma_map_ops *ops = get_dma_ops(dev);
	void *cpu_addr;

	WARN_ON_ONCE(!dev-&gt;coherent_dma_mask);

	/*
	 * DMA allocations can never be turned back into a page pointer, so
	 * requesting compound pages doesn't make sense (and can't even be
	 * supported at all by various backends).
	 */
	if (WARN_ON_ONCE(flag &amp; __GFP_COMP))
		return NULL;

	if (dma_alloc_from_dev_coherent(dev, size, dma_handle, &amp;cpu_addr))
		return cpu_addr;

	/* let the implementation decide on the zone to allocate from: */
	flag &amp;= ~(__GFP_DMA | __GFP_DMA32 | __GFP_HIGHMEM);

	if (dma_alloc_direct(dev, ops)) // [[Encyclopedia of NetworkSystem/Function/kernel-dma/dma_alloc_direct().md|dma_alloc_direct()]]
		cpu_addr = dma_direct_alloc(dev, size, dma_handle, flag, attrs);
	else if (ops-&gt;alloc)
		cpu_addr = ops-&gt;alloc(dev, size, dma_handle, flag, attrs);
	else
		return NULL;

	debug_dma_alloc_coherent(dev, size, *dma_handle, cpu_addr, attrs);
	return cpu_addr;
}
EXPORT_SYMBOL(dma_alloc_attrs);
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/kernel-dma/dma_alloc_direct().md" data-href="Encyclopedia of NetworkSystem/Function/kernel-dma/dma_alloc_direct().md" href="encyclopedia-of-networksystem/function/kernel-dma/dma_alloc_direct().html" class="internal-link" target="_self" rel="noopener nofollow">dma_alloc_direct()</a><br>
dma_alloc_direct를 통해 device가 direct allocating이 가능하면 IOMMU를 bypass로 하고 해당하는 작업을 수행. 아니라면, ops→alloc 함수를 통해 allocation 수행. alloc이라는 함수는 dma_map_ops 내부에 매핑된 함수 포인터로, dma_map_ops는 device 구조체에 있는것을 확인하였다. 따라서 ice_probe 이전에 이미 매핑이 되었다고 보았다. 이를 따라가 보았지만, 아키텍쳐 종속적인 함수들만 존재하여 우선 포기하였다.
]]></description><link>encyclopedia-of-networksystem/function/kernel-dma/dma_alloc_attrs().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/kernel-dma/dma_alloc_attrs().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[dma_alloc_direct()]]></title><description><![CDATA[ 
 <br>
/*
 * Check if the devices uses a direct mapping for streaming DMA operations.
 * This allows IOMMU drivers to set a bypass mode if the DMA mask is large
 * enough.
 */
static inline bool dma_alloc_direct(struct device *dev,
		const struct dma_map_ops *ops)
{
	return dma_go_direct(dev, dev-&gt;coherent_dma_mask, ops);
}
<br>
dma_alloc_direct()를 통해 device가 direct allocating이 가능하면 IOMMU를 bypass로 하고 해당하는 작업을 수행. 아니라면, ops→alloc 함수를 통해 allocation 수행.
]]></description><link>encyclopedia-of-networksystem/function/kernel-dma/dma_alloc_direct().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/kernel-dma/dma_alloc_direct().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[dmam_alloc_attrs()]]></title><description><![CDATA[ 
 <br>/**
 * dmam_alloc_attrs - Managed dma_alloc_attrs()
 * @dev: Device to allocate non_coherent memory for
 * @size: Size of allocation
 * @dma_handle: Out argument for allocated DMA handle
 * @gfp: Allocation flags
 * @attrs: Flags in the DMA_ATTR_* namespace.
 *
 * Managed dma_alloc_attrs().  Memory allocated using this function will be
 * automatically released on driver detach.
 *
 * RETURNS:
 * Pointer to allocated memory on success, NULL on failure.
 */
void *dmam_alloc_attrs(struct device *dev, size_t size, dma_addr_t *dma_handle,
		gfp_t gfp, unsigned long attrs)
{
	struct dma_devres *dr;
	void *vaddr;

	dr = devres_alloc(dmam_release, sizeof(*dr), gfp);
	if (!dr)
		return NULL;

	vaddr = dma_alloc_attrs(dev, size, dma_handle, gfp, attrs); // [[Encyclopedia of NetworkSystem/Function/kernel-dma/dma_alloc_attrs().md|dma_alloc_attrs()]]
	if (!vaddr) {
		devres_free(dr);
		return NULL;
	}

	dr-&gt;vaddr = vaddr;
	dr-&gt;dma_handle = *dma_handle;
	dr-&gt;size = size;
	dr-&gt;attrs = attrs;

	devres_add(dev, dr);

	return vaddr;
}
EXPORT_SYMBOL(dmam_alloc_attrs);
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/kernel-dma/dma_alloc_attrs().md" data-href="Encyclopedia of NetworkSystem/Function/kernel-dma/dma_alloc_attrs().md" href="encyclopedia-of-networksystem/function/kernel-dma/dma_alloc_attrs().html" class="internal-link" target="_self" rel="noopener nofollow">dma_alloc_attrs()</a><br>
vaddr는 dma_handle과 똑같은 physical memory를 가르키는 virtual address가 될 것이다. 쭉 함수들을 따라가다보면, dev→dma_mem을 통해 원래 device가 처음 초기화 될 때 할당받은 dma memory pool에서 필요한 크기만큼의 dma 메모리를 할당 받아 이에 대한 virtual address를 가져오게 되는 것이다. dma_get_device_base로 device가 할당받은 dma 메모리의 base address를 가져오게 되고, 앞서 bitmap을 탐색하여 찾은 새로 할당 할 dma 주소의 offset, 혹은 pageno를 가지고 필요한 dma address를 저장하고, device의 base address에 대응하는 mem→virt_base에서 오프셋을 더하여 해당하는 virtual address를 반환하게 된다
]]></description><link>encyclopedia-of-networksystem/function/kernel-dma/dmam_alloc_attrs().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/kernel-dma/dmam_alloc_attrs().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[__irq_domain_activate_irq()]]></title><description><![CDATA[ 
 <br>static int __irq_domain_activate_irq(struct irq_data *irqd, bool reserve)
{
	int ret = 0;

	if (irqd &amp;&amp; irqd-&gt;domain) {
		struct irq_domain *domain = irqd-&gt;domain;

		if (irqd-&gt;parent_data)
			ret = __irq_domain_activate_irq(irqd-&gt;parent_data,
							reserve);
		if (!ret &amp;&amp; domain-&gt;ops-&gt;activate) {
			ret = domain-&gt;ops-&gt;activate(domain, irqd, reserve);
			/* Rollback in case of error */
			if (ret &amp;&amp; irqd-&gt;parent_data)
				__irq_domain_deactivate_irq(irqd-&gt;parent_data);
		}
	}
	return ret;
}
<br>
재귀적으로 irq_data의 부모 데이터가 있다면 이를 활성화 시킴.
]]></description><link>encyclopedia-of-networksystem/function/kernel-irq/__irq_domain_activate_irq().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/kernel-irq/__irq_domain_activate_irq().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[__irq_set_trigger()]]></title><description><![CDATA[ 
 <br>int __irq_set_trigger(struct irq_desc *desc, unsigned long flags)
{
	struct irq_chip *chip = desc-&gt;irq_data.chip;
	int ret, unmask = 0;

	if (!chip || !chip-&gt;irq_set_type) {
		/*
		 * IRQF_TRIGGER_* but the PIC does not support multiple
		 * flow-types?
		 */
		pr_debug("No set_type function for IRQ %d (%s)\n",
			 irq_desc_get_irq(desc),
			 chip ? (chip-&gt;name ? : "unknown") : "unknown");
		return 0;
	}

	if (chip-&gt;flags &amp; IRQCHIP_SET_TYPE_MASKED) {
		if (!irqd_irq_masked(&amp;desc-&gt;irq_data))
			mask_irq(desc);
		if (!irqd_irq_disabled(&amp;desc-&gt;irq_data))
			unmask = 1;
	}

	/* Mask all flags except trigger mode */
	flags &amp;= IRQ_TYPE_SENSE_MASK;
	ret = chip-&gt;irq_set_type(&amp;desc-&gt;irq_data, flags);

	switch (ret) {
	case IRQ_SET_MASK_OK:
	case IRQ_SET_MASK_OK_DONE:
		irqd_clear(&amp;desc-&gt;irq_data, IRQD_TRIGGER_MASK);
		irqd_set(&amp;desc-&gt;irq_data, flags);
		fallthrough;

	case IRQ_SET_MASK_OK_NOCOPY:
		flags = irqd_get_trigger_type(&amp;desc-&gt;irq_data);
		irq_settings_set_trigger_mask(desc, flags);
		irqd_clear(&amp;desc-&gt;irq_data, IRQD_LEVEL);
		irq_settings_clr_level(desc);
		if (flags &amp; IRQ_TYPE_LEVEL_MASK) {
			irq_settings_set_level(desc);
			irqd_set(&amp;desc-&gt;irq_data, IRQD_LEVEL);
		}

		ret = 0;
		break;
	default:
		pr_err("Setting trigger mode %lu for irq %u failed (%pS)\n",
		       flags, irq_desc_get_irq(desc), chip-&gt;irq_set_type);
	}
	if (unmask)
		unmask_irq(desc);
	return ret;
}
]]></description><link>encyclopedia-of-networksystem/function/kernel-irq/__irq_set_trigger().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/kernel-irq/__irq_set_trigger().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[__setup_irq()]]></title><description><![CDATA[ 
 <br>/*
 * Internal function to register an irqaction - typically used to
 * allocate special interrupts that are part of the architecture.
 *
 * Locking rules:
 *
 * desc-&gt;request_mutex	Provides serialization against a concurrent free_irq()
 *   chip_bus_lock	Provides serialization for slow bus operations
 *     desc-&gt;lock	Provides serialization against hard interrupts
 *
 * chip_bus_lock and desc-&gt;lock are sufficient for all other management and
 * interrupt related functions. desc-&gt;request_mutex solely serializes
 * request/free_irq().
 */
 static int
__setup_irq(unsigned int irq, struct irq_desc *desc, struct irqaction *new)
{
	struct irqaction *old, **old_ptr;
	unsigned long flags, thread_mask = 0;
	int ret, nested, shared = 0;

	if (!desc)
		return -EINVAL;

	if (desc-&gt;irq_data.chip == &amp;no_irq_chip)
		return -ENOSYS;
	if (!try_module_get(desc-&gt;owner))
		return -ENODEV;

	new-&gt;irq = irq;

	/*
	 * If the trigger type is not specified by the caller,
	 * then use the default for this interrupt.
	 */
	if (!(new-&gt;flags &amp; IRQF_TRIGGER_MASK))
		new-&gt;flags |= irqd_get_trigger_type(&amp;desc-&gt;irq_data);

	/*
	 * Check whether the interrupt nests into another interrupt
	 * thread.
	 */
	nested = irq_settings_is_nested_thread(desc);
	if (nested) {
		if (!new-&gt;thread_fn) {
			ret = -EINVAL;
			goto out_mput;
		}
		/*
		 * Replace the primary handler which was provided from
		 * the driver for non nested interrupt handling by the
		 * dummy function which warns when called.
		 */
		new-&gt;handler = irq_nested_primary_handler;
	} else {
		if (irq_settings_can_thread(desc)) {
			ret = irq_setup_forced_threading(new);
			if (ret)
				goto out_mput;
		}
	}
	/*
	 * Create a handler thread when a thread function is supplied
	 * and the interrupt does not nest into another interrupt
	 * thread.
	 */
	if (new-&gt;thread_fn &amp;&amp; !nested) {
		ret = setup_irq_thread(new, irq, false); // [[Encyclopedia of NetworkSystem/Function/kernel-irq/setup_irq_thread().md|setup_irq_thread()]]
		if (ret)
			goto out_mput;
		if (new-&gt;secondary) {
			ret = setup_irq_thread(new-&gt;secondary, irq, true); // [[Encyclopedia of NetworkSystem/Function/kernel-irq/setup_irq_thread().md|setup_irq_thread()]]
			if (ret)
				goto out_thread;
		}
	}

	/*
	 * Drivers are often written to work w/o knowledge about the
	 * underlying irq chip implementation, so a request for a
	 * threaded irq without a primary hard irq context handler
	 * requires the ONESHOT flag to be set. Some irq chips like
	 * MSI based interrupts are per se one shot safe. Check the
	 * chip flags, so we can avoid the unmask dance at the end of
	 * the threaded handler for those.
	 */
	if (desc-&gt;irq_data.chip-&gt;flags &amp; IRQCHIP_ONESHOT_SAFE)
		new-&gt;flags &amp;= ~IRQF_ONESHOT;

	/*
	 * Protects against a concurrent __free_irq() call which might wait
	 * for synchronize_hardirq() to complete without holding the optional
	 * chip bus lock and desc-&gt;lock. Also protects against handing out
	 * a recycled oneshot thread_mask bit while it's still in use by
	 * its previous owner.
	 */
	mutex_lock(&amp;desc-&gt;request_mutex); // [[Encyclopedia of NetworkSystem/Function/kernel-locking/mutex_lock().md|mutex_lock()]]

	/*
	 * Acquire bus lock as the irq_request_resources() callback below
	 * might rely on the serialization or the magic power management
	 * functions which are abusing the irq_bus_lock() callback,
	 */
	chip_bus_lock(desc); // [[Encyclopedia of NetworkSystem/Function/kernel-irq/chip_bus_lock().md|chip_bus_lock()]]

	/* First installed action requests resources. */
	if (!desc-&gt;action) {
		ret = irq_request_resources(desc);
		if (ret) {
			pr_err("Failed to request resources for %s (irq %d) on irqchip %s\n",
			       new-&gt;name, irq, desc-&gt;irq_data.chip-&gt;name);
			goto out_bus_unlock;
		}
	}
	// 만약 desc→action이 없다면 irq_request_resources()로 자원을 요청한다.
	// old 포인터를 통해 만약 해당하는 irq에 actioin이 있다면 이를 공유 할 수 있는지 확인하고, 
	// 위배하였다면 관련된 처리를 한다.

	/*
	 * The following block of code has to be executed atomically
	 * protected against a concurrent interrupt and any of the other
	 * management calls which are not serialized via
	 * desc-&gt;request_mutex or the optional bus lock.
	 */
	raw_spin_lock_irqsave(&amp;desc-&gt;lock, flags);
	old_ptr = &amp;desc-&gt;action;
	old = *old_ptr;
	if (old) {
		/*
		 * Can't share interrupts unless both agree to and are
		 * the same type (level, edge, polarity). So both flag
		 * fields must have IRQF_SHARED set and the bits which
		 * set the trigger type must match. Also all must
		 * agree on ONESHOT.
		 * Interrupt lines used for NMIs cannot be shared.
		 */
		unsigned int oldtype;

		if (desc-&gt;istate &amp; IRQS_NMI) {
			pr_err("Invalid attempt to share NMI for %s (irq %d) on irqchip %s.\n",
				new-&gt;name, irq, desc-&gt;irq_data.chip-&gt;name);
			ret = -EINVAL;
			goto out_unlock;
		}

		/*
		 * If nobody did set the configuration before, inherit
		 * the one provided by the requester.
		 */
		if (irqd_trigger_type_was_set(&amp;desc-&gt;irq_data)) {
			oldtype = irqd_get_trigger_type(&amp;desc-&gt;irq_data);
		} else {
			oldtype = new-&gt;flags &amp; IRQF_TRIGGER_MASK;
			irqd_set_trigger_type(&amp;desc-&gt;irq_data, oldtype);
		}

		if (!((old-&gt;flags &amp; new-&gt;flags) &amp; IRQF_SHARED) ||
		    (oldtype != (new-&gt;flags &amp; IRQF_TRIGGER_MASK)))
			goto mismatch;

		if ((old-&gt;flags &amp; IRQF_ONESHOT) &amp;&amp;
		    (new-&gt;flags &amp; IRQF_COND_ONESHOT))
			new-&gt;flags |= IRQF_ONESHOT;
		else if ((old-&gt;flags ^ new-&gt;flags) &amp; IRQF_ONESHOT)
			goto mismatch;

		/* All handlers must agree on per-cpuness */
		if ((old-&gt;flags &amp; IRQF_PERCPU) !=
		    (new-&gt;flags &amp; IRQF_PERCPU))
			goto mismatch;

		/* add new interrupt at end of irq queue */
		do {
			/*
			 * Or all existing action-&gt;thread_mask bits,
			 * so we can find the next zero bit for this
			 * new action.
			 */
			thread_mask |= old-&gt;thread_mask;
			old_ptr = &amp;old-&gt;next;
			old = *old_ptr;
		} while (old);
		shared = 1;
	}

	/*
	 * Setup the thread mask for this irqaction for ONESHOT. For
	 * !ONESHOT irqs the thread mask is 0 so we can avoid a
	 * conditional in irq_wake_thread().
	 */
	if (new-&gt;flags &amp; IRQF_ONESHOT) {
		/*
		 * Unlikely to have 32 resp 64 irqs sharing one line,
		 * but who knows.
		 */
		if (thread_mask == ~0UL) {
			ret = -EBUSY;
			goto out_unlock;
		}
		/*
		 * The thread_mask for the action is or'ed to
		 * desc-&gt;thread_active to indicate that the
		 * IRQF_ONESHOT thread handler has been woken, but not
		 * yet finished. The bit is cleared when a thread
		 * completes. When all threads of a shared interrupt
		 * line have completed desc-&gt;threads_active becomes
		 * zero and the interrupt line is unmasked. See
		 * handle.c:irq_wake_thread() for further information.
		 *
		 * If no thread is woken by primary (hard irq context)
		 * interrupt handlers, then desc-&gt;threads_active is
		 * also checked for zero to unmask the irq line in the
		 * affected hard irq flow handlers
		 * (handle_[fasteoi|level]_irq).
		 *
		 * The new action gets the first zero bit of
		 * thread_mask assigned. See the loop above which or's
		 * all existing action-&gt;thread_mask bits.
		 */
		new-&gt;thread_mask = 1UL &lt;&lt; ffz(thread_mask);

	} else if (new-&gt;handler == irq_default_primary_handler &amp;&amp;
		   !(desc-&gt;irq_data.chip-&gt;flags &amp; IRQCHIP_ONESHOT_SAFE)) {
		/*
		 * The interrupt was requested with handler = NULL, so
		 * we use the default primary handler for it. But it
		 * does not have the oneshot flag set. In combination
		 * with level interrupts this is deadly, because the
		 * default primary handler just wakes the thread, then
		 * the irq lines is reenabled, but the device still
		 * has the level irq asserted. Rinse and repeat....
		 *
		 * While this works for edge type interrupts, we play
		 * it safe and reject unconditionally because we can't
		 * say for sure which type this interrupt really
		 * has. The type flags are unreliable as the
		 * underlying chip implementation can override them.
		 */
		pr_err("Threaded irq requested with handler=NULL and !ONESHOT for %s (irq %d)\n",
		       new-&gt;name, irq);
		ret = -EINVAL;
		goto out_unlock;
	}
	 	if (!shared) {
		/* Setup the type (level, edge polarity) if configured: */
		if (new-&gt;flags &amp; IRQF_TRIGGER_MASK) {
			ret = __irq_set_trigger(desc,
						new-&gt;flags &amp; IRQF_TRIGGER_MASK); // [[Encyclopedia of NetworkSystem/Function/kernel-irq/__irq_set_trigger().md|__irq_set_trigger()]]

			if (ret)
				goto out_unlock;
		}

		/*
		 * Activate the interrupt. That activation must happen
		 * independently of IRQ_NOAUTOEN. request_irq() can fail
		 * and the callers are supposed to handle
		 * that. enable_irq() of an interrupt requested with
		 * IRQ_NOAUTOEN is not supposed to fail. The activation
		 * keeps it in shutdown mode, it merily associates
		 * resources if necessary and if that's not possible it
		 * fails. Interrupts which are in managed shutdown mode
		 * will simply ignore that activation request.
		 */
		ret = irq_activate(desc); // [[Encyclopedia of NetworkSystem/Function/kernel-irq/irq_activate().md|irq_activate()]]
		if (ret)
			goto out_unlock;

		desc-&gt;istate &amp;= ~(IRQS_AUTODETECT | IRQS_SPURIOUS_DISABLED | \
				  IRQS_ONESHOT | IRQS_WAITING);
		irqd_clear(&amp;desc-&gt;irq_data, IRQD_IRQ_INPROGRESS);

		if (new-&gt;flags &amp; IRQF_PERCPU) {
			irqd_set(&amp;desc-&gt;irq_data, IRQD_PER_CPU);
			irq_settings_set_per_cpu(desc);
			if (new-&gt;flags &amp; IRQF_NO_DEBUG)
				irq_settings_set_no_debug(desc);
		}

		if (noirqdebug)
			irq_settings_set_no_debug(desc);

		if (new-&gt;flags &amp; IRQF_ONESHOT)
			desc-&gt;istate |= IRQS_ONESHOT;

		/* Exclude IRQ from balancing if requested */
		if (new-&gt;flags &amp; IRQF_NOBALANCING) {
			irq_settings_set_no_balancing(desc);
			irqd_set(&amp;desc-&gt;irq_data, IRQD_NO_BALANCING); // [[Encyclopedia of NetworkSystem/Function/kernel-irq/irqd_set().md|irqd_set()]]
		}

		if (!(new-&gt;flags &amp; IRQF_NO_AUTOEN) &amp;&amp;
		    irq_settings_can_autoenable(desc)) {
			irq_startup(desc, IRQ_RESEND, IRQ_START_COND); // [[Encyclopedia of NetworkSystem/Function/kernel-irq/irq_startup().md|irq_startup()]]
		} else {
			/*
			 * Shared interrupts do not go well with disabling
			 * auto enable. The sharing interrupt might request
			 * it while it's still disabled and then wait for
			 * interrupts forever.
			 */
			WARN_ON_ONCE(new-&gt;flags &amp; IRQF_SHARED);
			/* Undo nested disables: */
			desc-&gt;depth = 1;
		}

	} else if (new-&gt;flags &amp; IRQF_TRIGGER_MASK) {
		unsigned int nmsk = new-&gt;flags &amp; IRQF_TRIGGER_MASK;
		unsigned int omsk = irqd_get_trigger_type(&amp;desc-&gt;irq_data);

		if (nmsk != omsk)
			/* hope the handler works with current  trigger mode */
			pr_warn("irq %d uses trigger mode %u; requested %u\n",
				irq, omsk, nmsk);
	}

	*old_ptr = new;
	//새로운 irq action을 해당하는 irq desc에 넣는 코드

	irq_pm_install_action(desc, new);

	/* Reset broken irq detection when installing new handler */
	desc-&gt;irq_count = 0;
	desc-&gt;irqs_unhandled = 0;

	/*
	 * Check whether we disabled the irq via the spurious handler
	 * before. Reenable it and give it another chance.
	 */
	if (shared &amp;&amp; (desc-&gt;istate &amp; IRQS_SPURIOUS_DISABLED)) {
		desc-&gt;istate &amp;= ~IRQS_SPURIOUS_DISABLED;
		__enable_irq(desc);
	}

	raw_spin_unlock_irqrestore(&amp;desc-&gt;lock, flags);
	chip_bus_sync_unlock(desc);
	mutex_unlock(&amp;desc-&gt;request_mutex);

	irq_setup_timings(desc, new);

	wake_up_and_wait_for_irq_thread_ready(desc, new);
	wake_up_and_wait_for_irq_thread_ready(desc, new-&gt;secondary);

	register_irq_proc(irq, desc);
	new-&gt;dir = NULL;
	register_handler_proc(irq, new);
	return 0;

mismatch:
	if (!(new-&gt;flags &amp; IRQF_PROBE_SHARED)) {
		pr_err("Flags mismatch irq %d. %08x (%s) vs. %08x (%s)\n",
		       irq, new-&gt;flags, new-&gt;name, old-&gt;flags, old-&gt;name);
#ifdef CONFIG_DEBUG_SHIRQ
		dump_stack();
#endif
	}
	ret = -EBUSY;

out_unlock:
	raw_spin_unlock_irqrestore(&amp;desc-&gt;lock, flags);

	if (!desc-&gt;action)
		irq_release_resources(desc);
out_bus_unlock:
	chip_bus_sync_unlock(desc);
	mutex_unlock(&amp;desc-&gt;request_mutex);

out_thread:
	if (new-&gt;thread) {
		struct task_struct *t = new-&gt;thread;

		new-&gt;thread = NULL;
		kthread_stop_put(t);
	}
	if (new-&gt;secondary &amp;&amp; new-&gt;secondary-&gt;thread) {
		struct task_struct *t = new-&gt;secondary-&gt;thread;

		new-&gt;secondary-&gt;thread = NULL;
		kthread_stop_put(t);
	}
out_mput:
	module_put(desc-&gt;owner);
	return ret;
}
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/kernel-irq/setup_irq_thread().md" data-href="Encyclopedia of NetworkSystem/Function/kernel-irq/setup_irq_thread().md" href="encyclopedia-of-networksystem/function/kernel-irq/setup_irq_thread().html" class="internal-link" target="_self" rel="noopener nofollow">setup_irq_thread()</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/kernel-locking/mutex_lock().md" data-href="Encyclopedia of NetworkSystem/Function/kernel-locking/mutex_lock().md" href="encyclopedia-of-networksystem/function/kernel-locking/mutex_lock().html" class="internal-link" target="_self" rel="noopener nofollow">mutex_lock()</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/kernel-irq/chip_bus_lock().md" data-href="Encyclopedia of NetworkSystem/Function/kernel-irq/chip_bus_lock().md" href="encyclopedia-of-networksystem/function/kernel-irq/chip_bus_lock().html" class="internal-link" target="_self" rel="noopener nofollow">chip_bus_lock()</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/kernel-irq/__irq_set_trigger().md" data-href="Encyclopedia of NetworkSystem/Function/kernel-irq/__irq_set_trigger().md" href="encyclopedia-of-networksystem/function/kernel-irq/__irq_set_trigger().html" class="internal-link" target="_self" rel="noopener nofollow">__irq_set_trigger()</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/kernel-irq/irq_activate().md" data-href="Encyclopedia of NetworkSystem/Function/kernel-irq/irq_activate().md" href="encyclopedia-of-networksystem/function/kernel-irq/irq_activate().html" class="internal-link" target="_self" rel="noopener nofollow">irq_activate()</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/kernel-irq/irqd_set().md" data-href="Encyclopedia of NetworkSystem/Function/kernel-irq/irqd_set().md" href="encyclopedia-of-networksystem/function/kernel-irq/irqd_set().html" class="internal-link" target="_self" rel="noopener nofollow">irqd_set()</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/kernel-irq/irq_startup().md" data-href="Encyclopedia of NetworkSystem/Function/kernel-irq/irq_startup().md" href="encyclopedia-of-networksystem/function/kernel-irq/irq_startup().html" class="internal-link" target="_self" rel="noopener nofollow">irq_startup()</a><br>irqaction을 등록하는 내부함수. 주로 아키텍쳐의 일부인 특별한 인터럽트들을 할당하는데 쓰인다.<br>
handler는 앞서 지정한 interrupt handler, thread_fn은 NULL인 상태임. nested의 뜻은, 인터럽트를 처리하는 중에 또다른 인터럽트를 처리할 수 있는지 유무다. 즉, 계속하여 인터럽트가 겹쳐져서 처리가 될 수 있다는 뜻이다. 그러나 nested가 불가능한 경우 이를 거부할 것이다.
<br>쓰레드화된 인터럽트가 만약 핸들러가 없다면, irq_default_primary_handler나 nested threaded의 경우 irq_nested_primary_handler, irq_forced_secondary_handler를 irq_handler로 직접 할당해주게 된다.<br>thread_fn이 NULL값으로 전달되었기 때문에, 아래의 과정은 실행되지 않는다. 그러나 software interrupt handler의 핵심 처리과정으로 보여 정리 하였다.<br>→<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/kernel-irq/setup_irq_thread().md" data-href="Encyclopedia of NetworkSystem/Function/kernel-irq/setup_irq_thread().md" href="encyclopedia-of-networksystem/function/kernel-irq/setup_irq_thread().html" class="internal-link" target="_self" rel="noopener nofollow">setup_irq_thread()</a> : *data로 들어온 irqaction 에 해당하는 irq_handler 함수를 처리하기 위한 커널 쓰레드를 생성하는 함수이다. 해당하는 irq_handler를 실행하는게 아닌 irq_thread()라는 함수를 실행함, 프로세스 이름은 “irq/{irq number}-{irq action의 이름}”으로 지정됨.<br>
⇒ <a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/kernel-irq/irq_thread().md" data-href="Encyclopedia of NetworkSystem/Function/kernel-irq/irq_thread().md" href="encyclopedia-of-networksystem/function/kernel-irq/irq_thread().html" class="internal-link" target="_self" rel="noopener nofollow">irq_thread()</a> (on new kernel thread) : Interrupt handler thread라고 주석 되어 있음. handler_fn 함수 포인터를 선언하여 기다리고 있다가 만약 하드웨어 인터럽트가 끝났다면 irq_thread_fn을 실행함. 이때, irq_thread_fn() 함수는 thread_fn을 실행하게 됨. 여기서부터 software interrupt 처리 과정이 됨.]]></description><link>encyclopedia-of-networksystem/function/kernel-irq/__setup_irq().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/kernel-irq/__setup_irq().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[chip_bus_lock()]]></title><description><![CDATA[ 
 <br>/* Inline functions for support of irq chips on slow busses */
static inline void chip_bus_lock(struct irq_desc *desc)
{
	if (unlikely(desc-&gt;irq_data.chip-&gt;irq_bus_lock))
		desc-&gt;irq_data.chip-&gt;irq_bus_lock(&amp;desc-&gt;irq_data);
}
<br>irq_bus_lock()의 잘못된 사용을 막기 위한 함수이다.]]></description><link>encyclopedia-of-networksystem/function/kernel-irq/chip_bus_lock().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/kernel-irq/chip_bus_lock().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[devm_request_threaded_irq()]]></title><description><![CDATA[ 
 <br>/**
 *	devm_request_threaded_irq - allocate an interrupt line for a managed device
 *	@dev: device to request interrupt for
 *	@irq: Interrupt line to allocate
 *	@handler: Function to be called when the IRQ occurs
 *	@thread_fn: function to be called in a threaded interrupt context. NULL
 *		    for devices which handle everything in @handler
 *	@irqflags: Interrupt type flags
 *	@devname: An ascii name for the claiming device, dev_name(dev) if NULL
 *	@dev_id: A cookie passed back to the handler function
 *
 *	Except for the extra @dev argument, this function takes the
 *	same arguments and performs the same function as
 *	request_threaded_irq().  IRQs requested with this function will be
 *	automatically freed on driver detach.
 *
 *	If an IRQ allocated with this function needs to be freed
 *	separately, devm_free_irq() must be used.
 */
int devm_request_threaded_irq(struct device *dev, unsigned int irq,
			      irq_handler_t handler, irq_handler_t thread_fn,
			      unsigned long irqflags, const char *devname,
			      void *dev_id)
{
	struct irq_devres *dr;
	int rc;

	dr = devres_alloc(devm_irq_release, sizeof(struct irq_devres),
			  GFP_KERNEL); // [[Encyclopedia of NetworkSystem/Function/include-linux/devres_alloc().md|devres_alloc()]]
	if (!dr)
		return -ENOMEM;

	if (!devname)
		devname = dev_name(dev);

	rc = request_threaded_irq(irq, handler, thread_fn, irqflags, devname,
				  dev_id); // [[Encyclopedia of NetworkSystem/Function/kernel-irq/request_threaded_irq().md|request_threaded_irq()]]
	if (rc) {
		devres_free(dr);
		return rc;
	}

	dr-&gt;irq = irq;
	dr-&gt;dev_id = dev_id;
	devres_add(dev, dr);

	return 0;
}
EXPORT_SYMBOL(devm_request_threaded_irq);
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/include-linux/devres_alloc().md" data-href="Encyclopedia of NetworkSystem/Function/include-linux/devres_alloc().md" href="encyclopedia-of-networksystem/function/include-linux/devres_alloc().html" class="internal-link" target="_self" rel="noopener nofollow">devres_alloc()</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/kernel-irq/request_threaded_irq().md" data-href="Encyclopedia of NetworkSystem/Function/kernel-irq/request_threaded_irq().md" href="encyclopedia-of-networksystem/function/kernel-irq/request_threaded_irq().html" class="internal-link" target="_self" rel="noopener nofollow">request_threaded_irq()</a><br>devres_alloc은 dma를 실행함. 여기서는 irq_devres를 DMA 설정하고 있는데, 인터럽트 번호와 해당 device id 포인터 두 개의 필드를 가진 간단한 구조체임.]]></description><link>encyclopedia-of-networksystem/function/kernel-irq/devm_request_threaded_irq().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/kernel-irq/devm_request_threaded_irq().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[irq_activate()]]></title><description><![CDATA[ 
 <br>int irq_activate(struct irq_desc *desc)
{
	struct irq_data *d = irq_desc_get_irq_data(desc);

	if (!irqd_affinity_is_managed(d))
		return irq_domain_activate_irq(d, false); // [[Encyclopedia of NetworkSystem/Function/kernel-irq/irq_domain_activate_irq().md|irq_domain_activate_irq()]]
	return 0;
}
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/kernel-irq/irq_domain_activate_irq().md" data-href="Encyclopedia of NetworkSystem/Function/kernel-irq/irq_domain_activate_irq().md" href="encyclopedia-of-networksystem/function/kernel-irq/irq_domain_activate_irq().html" class="internal-link" target="_self" rel="noopener nofollow">irq_domain_activate_irq()</a>]]></description><link>encyclopedia-of-networksystem/function/kernel-irq/irq_activate().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/kernel-irq/irq_activate().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[irq_domain_activate_irq()]]></title><description><![CDATA[ 
 <br>/**
 * irq_domain_activate_irq - Call domain_ops-&gt;activate recursively to activate
 *			     interrupt
 * @irq_data:	Outermost irq_data associated with interrupt
 * @reserve:	If set only reserve an interrupt vector instead of assigning one
 *
 * This is the second step to call domain_ops-&gt;activate to program interrupt
 * controllers, so the interrupt could actually get delivered.
 */
int irq_domain_activate_irq(struct irq_data *irq_data, bool reserve)
{
	int ret = 0;

	if (!irqd_is_activated(irq_data))
		ret = __irq_domain_activate_irq(irq_data, reserve); // [[Encyclopedia of NetworkSystem/Function/kernel-irq/__irq_domain_activate_irq().md|__irq_domain_activate_irq()]]
	if (!ret)
		irqd_set_activated(irq_data); // [[Encyclopedia of NetworkSystem/Function/include-linux/irqd_set_activated().md|irqd_set_activated()]]
	return ret;
}
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/kernel-irq/__irq_domain_activate_irq().md" data-href="Encyclopedia of NetworkSystem/Function/kernel-irq/__irq_domain_activate_irq().md" href="encyclopedia-of-networksystem/function/kernel-irq/__irq_domain_activate_irq().html" class="internal-link" target="_self" rel="noopener nofollow">__irq_domain_activate_irq()</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/include-linux/irqd_set_activated().md" data-href="Encyclopedia of NetworkSystem/Function/include-linux/irqd_set_activated().md" href="encyclopedia-of-networksystem/function/include-linux/irqd_set_activated().html" class="internal-link" target="_self" rel="noopener nofollow">irqd_set_activated()</a>]]></description><link>encyclopedia-of-networksystem/function/kernel-irq/irq_domain_activate_irq().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/kernel-irq/irq_domain_activate_irq().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[irq_startup()]]></title><description><![CDATA[ 
 <br>int irq_startup(struct irq_desc *desc, bool resend, bool force)
{
	struct irq_data *d = irq_desc_get_irq_data(desc);
	const struct cpumask *aff = irq_data_get_affinity_mask(d);
	int ret = 0;

	desc-&gt;depth = 0;

	if (irqd_is_started(d)) {
		irq_enable(desc);
	} else {
		switch (__irq_startup_managed(desc, aff, force)) {
		case IRQ_STARTUP_NORMAL:
			if (d-&gt;chip-&gt;flags &amp; IRQCHIP_AFFINITY_PRE_STARTUP)
				irq_setup_affinity(desc);
			ret = __irq_startup(desc);
			if (!(d-&gt;chip-&gt;flags &amp; IRQCHIP_AFFINITY_PRE_STARTUP))
				irq_setup_affinity(desc);
			break;
		case IRQ_STARTUP_MANAGED:
			irq_do_set_affinity(d, aff, false);
			ret = __irq_startup(desc);
			break;
		case IRQ_STARTUP_ABORT:
			irqd_set_managed_shutdown(d);
			return 0;
		}
	}
	if (resend)
		check_irq_resend(desc, false);

	return ret;
}
]]></description><link>encyclopedia-of-networksystem/function/kernel-irq/irq_startup().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/kernel-irq/irq_startup().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[irq_thread()]]></title><description><![CDATA[ 
 <br>/*
 * Interrupt handler thread
 */
static int irq_thread(void *data)
{
	struct callback_head on_exit_work;
	struct irqaction *action = data;
	struct irq_desc *desc = irq_to_desc(action-&gt;irq);
	irqreturn_t (*handler_fn)(struct irq_desc *desc,
			struct irqaction *action);

	irq_thread_set_ready(desc, action);

	sched_set_fifo(current);

	if (force_irqthreads() &amp;&amp; test_bit(IRQTF_FORCED_THREAD,
					   &amp;action-&gt;thread_flags))
		handler_fn = irq_forced_thread_fn;
	else
		handler_fn = irq_thread_fn;

	init_task_work(&amp;on_exit_work, irq_thread_dtor);
	task_work_add(current, &amp;on_exit_work, TWA_NONE);

	while (!irq_wait_for_interrupt(desc, action)) {
		irqreturn_t action_ret;

		action_ret = handler_fn(desc, action);
		if (action_ret == IRQ_WAKE_THREAD)
			irq_wake_secondary(desc, action);

		wake_threads_waitq(desc);
	}

	/*
	 * This is the regular exit path. __free_irq() is stopping the
	 * thread via kthread_stop() after calling
	 * synchronize_hardirq(). So neither IRQTF_RUNTHREAD nor the
	 * oneshot mask bit can be set.
	 */
	task_work_cancel(current, irq_thread_dtor);
	return 0;
}
<br>Interrupt handler thread라고 주석 되어 있음. handler_fn 함수 포인터를 선언하여 기다리고 있다가 만약 하드웨어 인터럽트가 끝났다면 irq_thread_fn을 실행함. 이때, irq_thread_fn() 함수는 thread_fn을 실행하게 됨. 여기서부터 software interrupt 처리 과정이 됨.]]></description><link>encyclopedia-of-networksystem/function/kernel-irq/irq_thread().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/kernel-irq/irq_thread().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[irq_to_desc()]]></title><description><![CDATA[ 
 <br>struct irq_desc *irq_to_desc(unsigned int irq)
{
	return (irq &lt; NR_IRQS) ? irq_desc + irq : NULL;
}
EXPORT_SYMBOL(irq_to_desc);
<br>irq 번호를 통해 irq_desc 구조체를 반환하는 함수. 좀더 살펴볼 여지가 있음]]></description><link>encyclopedia-of-networksystem/function/kernel-irq/irq_to_desc().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/kernel-irq/irq_to_desc().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[irqd_set()]]></title><description><![CDATA[ 
 <br>static inline void irqd_set(struct irq_data *d, unsigned int mask)
{
	__irqd_to_state(d) |= mask;
}
]]></description><link>encyclopedia-of-networksystem/function/kernel-irq/irqd_set().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/kernel-irq/irqd_set().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[request_threaded_irq()]]></title><description><![CDATA[ 
 <br>/**
 *	request_threaded_irq - allocate an interrupt line
 *	@irq: Interrupt line to allocate
 *	@handler: Function to be called when the IRQ occurs.
 *		  Primary handler for threaded interrupts.
 *		  If handler is NULL and thread_fn != NULL
 *		  the default primary handler is installed.
 *	@thread_fn: Function called from the irq handler thread
 *		    If NULL, no irq thread is created
 *	@irqflags: Interrupt type flags
 *	@devname: An ascii name for the claiming device
 *	@dev_id: A cookie passed back to the handler function
 *
 *	This call allocates interrupt resources and enables the
 *	interrupt line and IRQ handling. From the point this
 *	call is made your handler function may be invoked. Since
 *	your handler function must clear any interrupt the board
 *	raises, you must take care both to initialise your hardware
 *	and to set up the interrupt handler in the right order.
 *
 *	If you want to set up a threaded irq handler for your device
 *	then you need to supply @handler and @thread_fn. @handler is
 *	still called in hard interrupt context and has to check
 *	whether the interrupt originates from the device. If yes it
 *	needs to disable the interrupt on the device and return
 *	IRQ_WAKE_THREAD which will wake up the handler thread and run
 *	@thread_fn. This split handler design is necessary to support
 *	shared interrupts.
 *
 *	Dev_id must be globally unique. Normally the address of the
 *	device data structure is used as the cookie. Since the handler
 *	receives this value it makes sense to use it.
 *
 *	If your interrupt is shared you must pass a non NULL dev_id
 *	as this is required when freeing the interrupt.
 *
 *	Flags:
 *
 *	IRQF_SHARED		Interrupt is shared
 *	IRQF_TRIGGER_*		Specify active edge(s) or level
 *	IRQF_ONESHOT		Run thread_fn with interrupt line masked
 */
int request_threaded_irq(unsigned int irq, irq_handler_t handler,
			 irq_handler_t thread_fn, unsigned long irqflags,
			 const char *devname, void *dev_id)
{
	struct irqaction *action;
	struct irq_desc *desc;
	int retval;

	if (irq == IRQ_NOTCONNECTED)
		return -ENOTCONN;

	/*
	 * Sanity-check: shared interrupts must pass in a real dev-ID,
	 * otherwise we'll have trouble later trying to figure out
	 * which interrupt is which (messes up the interrupt freeing
	 * logic etc).
	 *
	 * Also shared interrupts do not go well with disabling auto enable.
	 * The sharing interrupt might request it while it's still disabled
	 * and then wait for interrupts forever.
	 *
	 * Also IRQF_COND_SUSPEND only makes sense for shared interrupts and
	 * it cannot be set along with IRQF_NO_SUSPEND.
	 */
	if (((irqflags &amp; IRQF_SHARED) &amp;&amp; !dev_id) ||
	    ((irqflags &amp; IRQF_SHARED) &amp;&amp; (irqflags &amp; IRQF_NO_AUTOEN)) ||
	    (!(irqflags &amp; IRQF_SHARED) &amp;&amp; (irqflags &amp; IRQF_COND_SUSPEND)) ||
	    ((irqflags &amp; IRQF_NO_SUSPEND) &amp;&amp; (irqflags &amp; IRQF_COND_SUSPEND)))
		return -EINVAL;

	desc = irq_to_desc(irq); // [[Encyclopedia of NetworkSystem/Function/kernel-irq/irq_to_desc().md|irq_to_desc()]]
	if (!desc)
		return -EINVAL;

	if (!irq_settings_can_request(desc) ||
	    WARN_ON(irq_settings_is_per_cpu_devid(desc)))
		return -EINVAL;

	if (!handler) {
		if (!thread_fn)
			return -EINVAL;
		handler = irq_default_primary_handler;
	}

	action = kzalloc(sizeof(struct irqaction), GFP_KERNEL);
	if (!action)
		return -ENOMEM;

	action-&gt;handler = handler;
	action-&gt;thread_fn = thread_fn;
	action-&gt;flags = irqflags;
	action-&gt;name = devname;
	action-&gt;dev_id = dev_id;

	retval = irq_chip_pm_get(&amp;desc-&gt;irq_data);
	if (retval &lt; 0) {
		kfree(action);
		return retval;
	}

	retval = __setup_irq(irq, desc, action); // [[Encyclopedia of NetworkSystem/Function/kernel-irq/__setup_irq().md|__setup_irq()]]

	if (retval) {
		irq_chip_pm_put(&amp;desc-&gt;irq_data);
		kfree(action-&gt;secondary);
		kfree(action);
	}

#ifdef CONFIG_DEBUG_SHIRQ_FIXME
	if (!retval &amp;&amp; (irqflags &amp; IRQF_SHARED)) {
		/*
		 * It's a shared IRQ -- the driver ought to be prepared for it
		 * to happen immediately, so let's make sure....
		 * We disable the irq to make sure that a 'real' IRQ doesn't
		 * run in parallel with our fake.
		 */
		unsigned long flags;

		disable_irq(irq);
		local_irq_save(flags);

		handler(irq, dev_id);

		local_irq_restore(flags);
		enable_irq(irq);
	}
#endif
	return retval;
}
EXPORT_SYMBOL(request_threaded_irq);
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/kernel-irq/irq_to_desc().md" data-href="Encyclopedia of NetworkSystem/Function/kernel-irq/irq_to_desc().md" href="encyclopedia-of-networksystem/function/kernel-irq/irq_to_desc().html" class="internal-link" target="_self" rel="noopener nofollow">irq_to_desc()</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/kernel-irq/__setup_irq().md" data-href="Encyclopedia of NetworkSystem/Function/kernel-irq/__setup_irq().md" href="encyclopedia-of-networksystem/function/kernel-irq/__setup_irq().html" class="internal-link" target="_self" rel="noopener nofollow">__setup_irq()</a><br>thread_fn은 NULL로 전달됨.]]></description><link>encyclopedia-of-networksystem/function/kernel-irq/request_threaded_irq().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/kernel-irq/request_threaded_irq().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[setup_irq_thread()]]></title><description><![CDATA[ 
 <br>static int
setup_irq_thread(struct irqaction *new, unsigned int irq, bool secondary)
{
	struct task_struct *t;

	if (!secondary) {
		t = kthread_create(irq_thread, new, "irq/%d-%s", irq,
				   new-&gt;name); // [[Encyclopedia of NetworkSystem/Function/kernel-irq/irq_thread().md|irq_thread()]]
	} else {
		t = kthread_create(irq_thread, new, "irq/%d-s-%s", irq,
				   new-&gt;name); // [[Encyclopedia of NetworkSystem/Function/kernel-irq/irq_thread().md|irq_thread()]]
	}

	if (IS_ERR(t))
		return PTR_ERR(t);

	/*
	 * We keep the reference to the task struct even if
	 * the thread dies to avoid that the interrupt code
	 * references an already freed task_struct.
	 */
	new-&gt;thread = get_task_struct(t);
	/*
	 * Tell the thread to set its affinity. This is
	 * important for shared interrupt handlers as we do
	 * not invoke setup_affinity() for the secondary
	 * handlers as everything is already set up. Even for
	 * interrupts marked with IRQF_NO_BALANCE this is
	 * correct as we want the thread to move to the cpu(s)
	 * on which the requesting code placed the interrupt.
	 */
	set_bit(IRQTF_AFFINITY, &amp;new-&gt;thread_flags);
	return 0;
}
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/kernel-irq/irq_thread().md" data-href="Encyclopedia of NetworkSystem/Function/kernel-irq/irq_thread().md" href="encyclopedia-of-networksystem/function/kernel-irq/irq_thread().html" class="internal-link" target="_self" rel="noopener nofollow">irq_thread()</a><br>*data로 들어온 irqaction 에 해당하는 irq_handler 함수를 처리하기 위한 커널 쓰레드를 생성하는 함수이다. 해당하는 irq_handler를 실행하는게 아닌 irq_thread()라는 함수를 실행함, 프로세스 이름은 “irq/{irq number}-{irq action의 이름}”으로 지정됨.]]></description><link>encyclopedia-of-networksystem/function/kernel-irq/setup_irq_thread().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/kernel-irq/setup_irq_thread().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[mutex_lock()]]></title><description><![CDATA[ 
 <br>/**
 * mutex_lock - acquire the mutex
 * @lock: the mutex to be acquired
 *
 * Lock the mutex exclusively for this task. If the mutex is not
 * available right now, it will sleep until it can get it.
 *
 * The mutex must later on be released by the same task that
 * acquired it. Recursive locking is not allowed. The task
 * may not exit without first unlocking the mutex. Also, kernel
 * memory where the mutex resides must not be freed with
 * the mutex still locked. The mutex must first be initialized
 * (or statically defined) before it can be locked. memset()-ing
 * the mutex to 0 is not allowed.
 *
 * (The CONFIG_DEBUG_MUTEXES .config option turns on debugging
 * checks that will enforce the restrictions and will also do
 * deadlock debugging)
 *
 * This function is similar to (but not equivalent to) down().
 */
void __sched mutex_lock(struct mutex *lock)
{
	might_sleep();

	if (!__mutex_trylock_fast(lock))
		__mutex_lock_slowpath(lock);
}
EXPORT_SYMBOL(mutex_lock);
#endif

#include "ww_mutex.h"

#ifdef CONFIG_MUTEX_SPIN_ON_OWNER
<br>__free_irq()와의 동시실행을 막기 위한 함수이다.]]></description><link>encyclopedia-of-networksystem/function/kernel-locking/mutex_lock().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/kernel-locking/mutex_lock().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[____napi_schedule()]]></title><description><![CDATA[ 
 <br>static inline void ____napi_schedule(struct softnet_data *sd,
				     struct napi_struct *napi)
{
	struct task_struct *thread;

	lockdep_assert_irqs_disabled();

	if (test_bit(NAPI_STATE_THREADED, &amp;napi-&gt;state)) {
		/* Paired with smp_mb__before_atomic() in
		 * napi_enable()/dev_set_threaded().
		 * Use READ_ONCE() to guarantee a complete
		 * read on napi-&gt;thread. Only call
		 * wake_up_process() when it's not NULL.
		 */
		thread = READ_ONCE(napi-&gt;thread);
		if (thread) {
			/* Avoid doing set_bit() if the thread is in
			 * INTERRUPTIBLE state, cause napi_thread_wait()
			 * makes sure to proceed with napi polling
			 * if the thread is explicitly woken from here.
			 */
			if (READ_ONCE(thread-&gt;__state) != TASK_INTERRUPTIBLE)
				set_bit(NAPI_STATE_SCHED_THREADED, &amp;napi-&gt;state);
			wake_up_process(thread);
			return;
		}
	}

	list_add_tail(&amp;napi-&gt;poll_list, &amp;sd-&gt;poll_list); // [[Encyclopedia of NetworkSystem/Function/include-linux/list_add_tail().md|list_add_tail()]]
	WRITE_ONCE(napi-&gt;list_owner, smp_processor_id());
	/* If not called from net_rx_action()
	 * we have to raise NET_RX_SOFTIRQ.
	 */
	if (!sd-&gt;in_net_rx_action)
		__raise_softirq_irqoff(NET_RX_SOFTIRQ); // [[Encyclopedia of NetworkSystem/Function/kernel/__raise_softirq_irqoff().md|__raise_softirq_irqoff()]]
}
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/include-linux/list_add_tail().md" data-href="Encyclopedia of NetworkSystem/Function/include-linux/list_add_tail().md" href="encyclopedia-of-networksystem/function/include-linux/list_add_tail().html" class="internal-link" target="_self" rel="noopener nofollow">list_add_tail()</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/kernel/__raise_softirq_irqoff().md" data-href="Encyclopedia of NetworkSystem/Function/kernel/__raise_softirq_irqoff().md" href="encyclopedia-of-networksystem/function/kernel/__raise_softirq_irqoff().html" class="internal-link" target="_self" rel="noopener nofollow">__raise_softirq_irqoff()</a><br>쓰레드를 새로 만들어서 napi→thread 멤버를 불러와 해당 프로세스를 실행하게 됨. task_struct는 include/linux/sched.h에 있음.<br>
<br>lockdep_assert_irqs_disabled() : NAPI 이후에는 irq 금지
<br>NAPI의 상태가 thread이면서, NAPI의 thread가 존재할 경우 진행

<br>이 함수에서만 napi polling이 깨워지기 위해서 한다고함
<br>napi_thread_wait() 이 다른곳에서 깨워지는 것을 막는다


<br>NAPI의 thread가 interruptable이 아닌 경우

<br>NAPI를 실행한다. poll을 schedule 상태로 바꾼다
<br>poll list는 NAPI_STATE_SCHED bit를 관리하는 존재에 의해서만 관리
<br>bit를 킨 존재는, bit를 끌 수도 있다.


]]></description><link>encyclopedia-of-networksystem/function/net-core/____napi_schedule().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/____napi_schedule().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[__alloc_skb()]]></title><description><![CDATA[ 
 <br>    /* 	Allocate a new skbuff. We do this ourselves so we can fill in a few
     *	'private' fields and also do memory statistics to find all the
     *	[BEEP] leaks.
     *
     */
    
    /**
     *	__alloc_skb	-	allocate a network buffer
     *	@size: size to allocate
     *	@gfp_mask: allocation mask
     *	@flags: If SKB_ALLOC_FCLONE is set, allocate from fclone cache
     *		instead of head cache and allocate a cloned (child) skb.
     *		If SKB_ALLOC_RX is set, __GFP_MEMALLOC will be used for
     *		allocations in case the data is required for writeback
     *	@node: numa node to allocate memory on
     *
     *	Allocate a new &amp;sk_buff. The returned buffer has no headroom and a
     *	tail room of at least size bytes. The object has a reference count
     *	of one. The return is the buffer. On a failure the return is %NULL.
     *
     *	Buffers may only be allocated from interrupts using a @gfp_mask of
     *	%GFP_ATOMIC.
     */
    struct sk_buff *__alloc_skb(unsigned int size, gfp_t gfp_mask,
    			    int flags, int node)
    {
    	struct kmem_cache *cache;
    	struct sk_buff *skb;
    	bool pfmemalloc;
    	u8 *data;
    
    	cache = (flags &amp; SKB_ALLOC_FCLONE)
    		? net_hotdata.skbuff_fclone_cache : net_hotdata.skbuff_cache;
    
    	if (sk_memalloc_socks() &amp;&amp; (flags &amp; SKB_ALLOC_RX))
    		gfp_mask |= __GFP_MEMALLOC;
    
    	/* Get the HEAD */
    	if ((flags &amp; (SKB_ALLOC_FCLONE | SKB_ALLOC_NAPI)) == SKB_ALLOC_NAPI &amp;&amp;
    	    likely(node == NUMA_NO_NODE || node == numa_mem_id()))
    		skb = napi_skb_cache_get();
    	else
    		skb = kmem_cache_alloc_node(cache, gfp_mask &amp; ~GFP_DMA, node);
    	if (unlikely(!skb))
    		return NULL;
    	prefetchw(skb);
    
    	/* We do our best to align skb_shared_info on a separate cache
    	 * line. It usually works because kmalloc(X &gt; SMP_CACHE_BYTES) gives
    	 * aligned memory blocks, unless SLUB/SLAB debug is enabled.
    	 * Both skb-&gt;head and skb_shared_info are cache line aligned.
    	 */
    	data = kmalloc_reserve(&amp;size, gfp_mask, node, &amp;pfmemalloc);
    	if (unlikely(!data))
    		goto nodata;
    	/* kmalloc_size_roundup() might give us more room than requested.
    	 * Put skb_shared_info exactly at the end of allocated zone,
    	 * to allow max possible filling before reallocation.
    	 */
    	prefetchw(data + SKB_WITH_OVERHEAD(size));
    
    	/*
    	 * Only clear those fields we need to clear, not those that we will
    	 * actually initialise below. Hence, don't put any more fields after
    	 * the tail pointer in struct sk_buff!
    	 */
    	memset(skb, 0, offsetof(struct sk_buff, tail));
    	__build_skb_around(skb, data, size);
    	skb-&gt;pfmemalloc = pfmemalloc;
    
    	if (flags &amp; SKB_ALLOC_FCLONE) {
    		struct sk_buff_fclones *fclones;
    
    		fclones = container_of(skb, struct sk_buff_fclones, skb1);
    
    		skb-&gt;fclone = SKB_FCLONE_ORIG;
    		refcount_set(&amp;fclones-&gt;fclone_ref, 1);
    	}
    
    	return skb;
    
    nodata:
    	kmem_cache_free(cache, skb);
    	return NULL;
    }
    EXPORT_SYMBOL(__alloc_skb);
<br>
새로운 skb를 할당하는 함수. numa local한 캐시에서 sk_buff 할당 받은게 있다면 꺼내오고 아니면 새로 할당하게 된다. 이때, sk_buff는 초기에 CPU마다 대량으로 할당되어 있고, napi_skb_cache_get()을 통해 가져다가 쓸 수 있게 된다. 이후 kmalloc_reserve()를 호출하여 실제 data를 담을 공간을 할당 받는다.
]]></description><link>encyclopedia-of-networksystem/function/net-core/__alloc_skb().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/__alloc_skb().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[__napi_alloc_skb()]]></title><description><![CDATA[ 
 <br>	/**
     *	__napi_alloc_skb - allocate skbuff for rx in a specific NAPI instance
     *	@napi: napi instance this buffer was allocated for
     *	@len: length to allocate
     *	@gfp_mask: get_free_pages mask, passed to alloc_skb and alloc_pages
     *
     *	Allocate a new sk_buff for use in NAPI receive.  This buffer will
     *	attempt to allocate the head from a special reserved region used
     *	only for NAPI Rx allocation.  By doing this we can save several
     *	CPU cycles by avoiding having to disable and re-enable IRQs.
     *
     *	%NULL is returned if there is no free memory.
     */
    struct sk_buff *__napi_alloc_skb(struct napi_struct *napi, unsigned int len,
    				 gfp_t gfp_mask)
    {
    	struct napi_alloc_cache *nc;
    	struct sk_buff *skb;
    	bool pfmemalloc;
    	void *data;
    
    	DEBUG_NET_WARN_ON_ONCE(!in_softirq());
    	len += NET_SKB_PAD + NET_IP_ALIGN;
    
    	/* If requested length is either too small or too big,
    	 * we use kmalloc() for skb-&gt;head allocation.
    	 * When the small frag allocator is available, prefer it over kmalloc
    	 * for small fragments
    	 */
    	if ((!NAPI_HAS_SMALL_PAGE_FRAG &amp;&amp; len &lt;= SKB_WITH_OVERHEAD(1024)) ||
    	    len &gt; SKB_WITH_OVERHEAD(PAGE_SIZE) ||
    	    (gfp_mask &amp; (__GFP_DIRECT_RECLAIM | GFP_DMA))) {
    		skb = __alloc_skb(len, gfp_mask, SKB_ALLOC_RX | SKB_ALLOC_NAPI,
    				  NUMA_NO_NODE); // [[Encyclopedia of NetworkSystem/Function/net-core/__alloc_skb().md|__alloc_skb()]]
    		if (!skb)
    			goto skb_fail;
    		goto skb_success;
    	}
    
    	nc = this_cpu_ptr(&amp;napi_alloc_cache);
    
    	if (sk_memalloc_socks())
    		gfp_mask |= __GFP_MEMALLOC;
    
    	if (NAPI_HAS_SMALL_PAGE_FRAG &amp;&amp; len &lt;= SKB_WITH_OVERHEAD(1024)) {
    		/* we are artificially inflating the allocation size, but
    		 * that is not as bad as it may look like, as:
    		 * - 'len' less than GRO_MAX_HEAD makes little sense
    		 * - On most systems, larger 'len' values lead to fragment
    		 *   size above 512 bytes
    		 * - kmalloc would use the kmalloc-1k slab for such values
    		 * - Builds with smaller GRO_MAX_HEAD will very likely do
    		 *   little networking, as that implies no WiFi and no
    		 *   tunnels support, and 32 bits arches.
    		 */
    		len = SZ_1K;
    
    		data = page_frag_alloc_1k(&amp;nc-&gt;page_small, gfp_mask);
    		pfmemalloc = NAPI_SMALL_PAGE_PFMEMALLOC(nc-&gt;page_small);
    	} else {
    		len = SKB_HEAD_ALIGN(len);
    
    		data = page_frag_alloc(&amp;nc-&gt;page, len, gfp_mask);
    		pfmemalloc = nc-&gt;page.pfmemalloc;
    	}
    
    	if (unlikely(!data))
    		return NULL;
    
    	skb = __napi_build_skb(data, len);
    	if (unlikely(!skb)) {
    		skb_free_frag(data);
    		return NULL;
    	}
    
    	if (pfmemalloc)
    		skb-&gt;pfmemalloc = 1;
    	skb-&gt;head_frag = 1;
    
    skb_success:
    	skb_reserve(skb, NET_SKB_PAD + NET_IP_ALIGN);
    	skb-&gt;dev = napi-&gt;dev;
    
    skb_fail:
    	return skb;
    }
    EXPORT_SYMBOL(__napi_alloc_skb);
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-core/__alloc_skb().md" data-href="Encyclopedia of NetworkSystem/Function/net-core/__alloc_skb().md" href="encyclopedia-of-networksystem/function/net-core/__alloc_skb().html" class="internal-link" target="_self" rel="noopener nofollow">__alloc_skb()</a><br>
만약 별 문제가 없다면 __alloc_skb()를 통해 skb를 새로 할당하게 되고, 이후의 과정은 __napi_build_skb등을 거치며 위의 ice_build_skb와 같아지게 된다. 즉, 사용할 skb가 할당 되어있는지 여부에 의해 사용될 함수가 결정된다고 볼 수 있다.
]]></description><link>encyclopedia-of-networksystem/function/net-core/__napi_alloc_skb().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/__napi_alloc_skb().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[__napi_build_skb()]]></title><description><![CDATA[ 
 <br>    /**
	 * __napi_build_skb - build a network buffer
	 * @data: data buffer provided by caller
	 * @frag_size: size of data
	 *
	 * Version of __build_skb() that uses NAPI percpu caches to obtain
	 * skbuff_head instead of inplace allocation.
	 *
	 * Returns a new &amp;sk_buff on success, %NULL on allocation failure.
	 */
static struct sk_buff *__napi_build_skb(void *data, unsigned int frag_size)
{
	struct sk_buff *skb;

	skb = napi_skb_cache_get();
	if (unlikely(!skb))
		return NULL;

	memset(skb, 0, offsetof(struct sk_buff, tail));
	__build_skb_around(skb, data, frag_size);

	return skb;
}
<br>
이 함수는 우선 napi_skb_cache_get()을 통해 CPU 로컬한 캐시에서 skb_cache를 가져오는 작업을 하게 된다. 이는 성능향상을 위한 작업이며, 이후 skb의 크기만큼 memset을 통해 초기화를 해주고 __build_skb_around()를 통해 마저 skb를 구성하게 된다.
]]></description><link>encyclopedia-of-networksystem/function/net-core/__napi_build_skb().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/__napi_build_skb().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[__napi_poll()]]></title><description><![CDATA[ 
 <br>static int __napi_poll(struct napi_struct *n, bool *repoll)
{
	int work, weight;

	weight = n-&gt;weight;

	/* This NAPI_STATE_SCHED test is for avoiding a race
	 * with netpoll's poll_napi().  Only the entity which
	 * obtains the lock and sees NAPI_STATE_SCHED set will
	 * actually make the -&gt;poll() call.  Therefore we avoid
	 * accidentally calling -&gt;poll() when NAPI is not scheduled.
	 */
	work = 0;
	if (napi_is_scheduled(n)) {
		work = n-&gt;poll(n, weight);
		trace_napi_poll(n, work, weight);

		xdp_do_check_flushed(n);
	}

	if (unlikely(work &gt; weight))
		netdev_err_once(n-&gt;dev, "NAPI poll function %pS returned %d, exceeding its budget of %d.\n",
				n-&gt;poll, work, weight);

	if (likely(work &lt; weight))
		return work;

	/* Drivers must not modify the NAPI state if they
	 * consume the entire weight.  In such cases this code
	 * still "owns" the NAPI instance and therefore can
	 * move the instance around on the list at-will.
	 */
	if (unlikely(napi_disable_pending(n))) {
		napi_complete(n); // [[Encyclopedia of NetworkSystem/Function/include-linux/napi_complete() .md|napi_complete()]]
		return work;
	}

	/* The NAPI context has more processing work, but busy-polling
	 * is preferred. Exit early.
	 */
	if (napi_prefer_busy_poll(n)) { // [[Encyclopedia of NetworkSystem/Function/include-linux/napi_prefer_busy_poll() .md|napi_prefer_busy_poll()]]
		if (napi_complete_done(n, work)) { // [[Encyclopedia of NetworkSystem/Function/net-core/napi_complete_done().md|napi_complete_done()]]
			/* If timeout is not set, we need to make sure
			 * that the NAPI is re-scheduled.
			 */
			napi_schedule(n);
		}
		return work;
	}

	if (n-&gt;gro_bitmask) {
		/* flush too old packets
		 * If HZ &lt; 1000, flush all packets.
		 */
		napi_gro_flush(n, HZ &gt;= 1000);
	}

	gro_normal_list(n); // [[Encyclopedia of NetworkSystem/Function/include-net/gro_normal_list().md|gro_normal_list()]]

	/* Some drivers may have called napi_schedule
	 * prior to exhausting their budget.
	 */
	if (unlikely(!list_empty(&amp;n-&gt;poll_list))) {
		pr_warn_once("%s: Budget exhausted after napi rescheduled\n",
			     n-&gt;dev ? n-&gt;dev-&gt;name : "backlog");
		return work;
	}

	*repoll = true;

	return work;
}
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/include-linux/napi_complete() .md" data-href="Encyclopedia of NetworkSystem/Function/include-linux/napi_complete() .md" href="encyclopedia-of-networksystem/function/include-linux/napi_complete()-" class="internal-link" target="_self" rel="noopener nofollow">napi_complete()</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/include-linux/napi_prefer_busy_poll() .md" data-href="Encyclopedia of NetworkSystem/Function/include-linux/napi_prefer_busy_poll() .md" href="encyclopedia-of-networksystem/function/include-linux/napi_prefer_busy_poll()-" class="internal-link" target="_self" rel="noopener nofollow">napi_prefer_busy_poll()</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-core/napi_complete_done().md" data-href="Encyclopedia of NetworkSystem/Function/net-core/napi_complete_done().md" href="encyclopedia-of-networksystem/function/net-core/napi_complete_done().html" class="internal-link" target="_self" rel="noopener nofollow">napi_complete_done()</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/include-net/gro_normal_list().md" data-href="Encyclopedia of NetworkSystem/Function/include-net/gro_normal_list().md" href="encyclopedia-of-networksystem/function/include-net/gro_normal_list().html" class="internal-link" target="_self" rel="noopener nofollow">gro_normal_list()</a><br>
반드시 NAPI_STATE_SCHED라는 상태여야지만 →poll()을 호출 할 수 있도록 하였다. 이는 netpoll의 poll_napi()와 서로 경쟁하지 않게 하기 위함이다. poll은 napi_struct 구조체의 함수포인터로 실행되게 된다. 이 napi_struct는 softnet_data로부터 비롯되는데, 이는 CPU당 부여되는 구조체이다.<br>
따라서 n→poll()함수를 통해 <a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_napi_poll().md" data-href="Encyclopedia of NetworkSystem/Function/drivers-net-ethernet-intel-ice/ice_napi_poll().md" href="encyclopedia-of-networksystem/function/drivers-net-ethernet-intel-ice/ice_napi_poll().html" class="internal-link" target="_self" rel="noopener nofollow">ice_napi_poll</a>이 실행되게 된다.<br>
weight은 budget을 의미한다.
]]></description><link>encyclopedia-of-networksystem/function/net-core/__napi_poll().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/__napi_poll().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[__napi_schedule()]]></title><description><![CDATA[ 
 <br>void __napi_schedule(struct napi_struct *n)
{
	unsigned long flags;

	local_irq_save(flags);
	____napi_schedule(this_cpu_ptr(&amp;softnet_data), n); // [[Encyclopedia of NetworkSystem/Function/net-core/____napi_schedule().md|__napi_schedule()]]
	local_irq_restore(flags);
}
EXPORT_SYMBOL(__napi_schedule);
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-core/____napi_schedule().md" data-href="Encyclopedia of NetworkSystem/Function/net-core/____napi_schedule().md" href="encyclopedia-of-networksystem/function/net-core/____napi_schedule().html" class="internal-link" target="_self" rel="noopener nofollow">__napi_schedule()</a>]]></description><link>encyclopedia-of-networksystem/function/net-core/__napi_schedule().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/__napi_schedule().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[__netif_receive_skb_core()]]></title><description><![CDATA[ 
 <br>    static int __netif_receive_skb_core(struct sk_buff **pskb, bool pfmemalloc, struct packet_type **ppt_prev)
    {
    	struct packet_type *ptype, *pt_prev;
    	rx_handler_func_t *rx_handler;
    	struct sk_buff *skb = *pskb;
    	struct net_device *orig_dev;
    	bool deliver_exact = false;
    	int ret = NET_RX_DROP;
    	__be16 type;
    
    	net_timestamp_check(!READ_ONCE(net_hotdata.tstamp_prequeue), skb);
    
    	trace_netif_receive_skb(skb);
    
    	orig_dev = skb-&gt;dev;
    
    	skb_reset_network_header(skb);
    	if (!skb_transport_header_was_set(skb))
    		skb_reset_transport_header(skb);
    	skb_reset_mac_len(skb);
    
    	pt_prev = NULL;
    
    another_round:
    	skb-&gt;skb_iif = skb-&gt;dev-&gt;ifindex;
    
    	__this_cpu_inc(softnet_data.processed);
    
    	if (static_branch_unlikely(&amp;generic_xdp_needed_key)) {
    		int ret2;
    
    		migrate_disable();
    		ret2 = do_xdp_generic(rcu_dereference(skb-&gt;dev-&gt;xdp_prog),
    				      &amp;skb);
    		migrate_enable();
    
    		if (ret2 != XDP_PASS) {
    			ret = NET_RX_DROP;
    			goto out;
    		}
    	}
    
    	if (eth_type_vlan(skb-&gt;protocol)) {
    		skb = skb_vlan_untag(skb);
    		if (unlikely(!skb))
    			goto out;
    	}
    
    	if (skb_skip_tc_classify(skb))
    		goto skip_classify;
    
    	if (pfmemalloc)
    		goto skip_taps;
    
    	list_for_each_entry_rcu(ptype, &amp;net_hotdata.ptype_all, list) {
    		if (pt_prev)
    			ret = deliver_skb(skb, pt_prev, orig_dev); // [[Encyclopedia of NetworkSystem/Function/net-core/deliver_skb().md|deliver_skb()]]
    		pt_prev = ptype;
    	}
    
    	list_for_each_entry_rcu(ptype, &amp;skb-&gt;dev-&gt;ptype_all, list) {
    		if (pt_prev)
    			ret = deliver_skb(skb, pt_prev, orig_dev); // [[Encyclopedia of NetworkSystem/Function/net-core/deliver_skb().md|deliver_skb()]]
    		pt_prev = ptype;
    	}
    
    skip_taps:
    #ifdef CONFIG_NET_INGRESS
    	if (static_branch_unlikely(&amp;ingress_needed_key)) {
    		bool another = false;
    
    		nf_skip_egress(skb, true);
    		skb = sch_handle_ingress(skb, &amp;pt_prev, &amp;ret, orig_dev,
    					 &amp;another);
    		if (another)
    			goto another_round;
    		if (!skb)
    			goto out;
    
    		nf_skip_egress(skb, false);
    		if (nf_ingress(skb, &amp;pt_prev, &amp;ret, orig_dev) &lt; 0)
    			goto out;
    	}
    #endif
    	skb_reset_redirect(skb);
    skip_classify:
    	if (pfmemalloc &amp;&amp; !skb_pfmemalloc_protocol(skb))
    		goto drop;
    
    	if (skb_vlan_tag_present(skb)) {
    		if (pt_prev) {
    			ret = deliver_skb(skb, pt_prev, orig_dev); // [[Encyclopedia of NetworkSystem/Function/net-core/deliver_skb().md|deliver_skb()]]
    			pt_prev = NULL;
    		}
    		if (vlan_do_receive(&amp;skb))
    			goto another_round;
    		else if (unlikely(!skb))
    			goto out;
    	}
    
    	rx_handler = rcu_dereference(skb-&gt;dev-&gt;rx_handler);
    	if (rx_handler) {
    		if (pt_prev) {
    			ret = deliver_skb(skb, pt_prev, orig_dev); // [[Encyclopedia of NetworkSystem/Function/net-core/deliver_skb().md|deliver_skb()]]
    			pt_prev = NULL;
    		}
    		switch (rx_handler(&amp;skb)) {
    		case RX_HANDLER_CONSUMED:
    			ret = NET_RX_SUCCESS;
    			goto out;
    		case RX_HANDLER_ANOTHER:
    			goto another_round;
    		case RX_HANDLER_EXACT:
    			deliver_exact = true;
    			break;
    		case RX_HANDLER_PASS:
    			break;
    		default:
    			BUG();
    		}
    	}
    
    	if (unlikely(skb_vlan_tag_present(skb)) &amp;&amp; !netdev_uses_dsa(skb-&gt;dev)) {
    check_vlan_id:
    		if (skb_vlan_tag_get_id(skb)) {
    			/* Vlan id is non 0 and vlan_do_receive() above couldn't
    			 * find vlan device.
    			 */
    			skb-&gt;pkt_type = PACKET_OTHERHOST;
    		} else if (eth_type_vlan(skb-&gt;protocol)) {
    			/* Outer header is 802.1P with vlan 0, inner header is
    			 * 802.1Q or 802.1AD and vlan_do_receive() above could
    			 * not find vlan dev for vlan id 0.
    			 */
    			__vlan_hwaccel_clear_tag(skb);
    			skb = skb_vlan_untag(skb);
    			if (unlikely(!skb))
    				goto out;
    			if (vlan_do_receive(&amp;skb))
    				/* After stripping off 802.1P header with vlan 0
    				 * vlan dev is found for inner header.
    				 */
    				goto another_round;
    			else if (unlikely(!skb))
    				goto out;
    			else
    				/* We have stripped outer 802.1P vlan 0 header.
    				 * But could not find vlan dev.
    				 * check again for vlan id to set OTHERHOST.
    				 */
    				goto check_vlan_id;
    		}
    		/* Note: we might in the future use prio bits
    		 * and set skb-&gt;priority like in vlan_do_receive()
    		 * For the time being, just ignore Priority Code Point
    		 */
    		__vlan_hwaccel_clear_tag(skb);
    	}
    
    	type = skb-&gt;protocol;
    
    	/* deliver only exact match when indicated */
    	if (likely(!deliver_exact)) {
    		deliver_ptype_list_skb(skb, &amp;pt_prev, orig_dev, type,
    				       &amp;ptype_base[ntohs(type) &amp;
    						   PTYPE_HASH_MASK]);
    	}
    
    	deliver_ptype_list_skb(skb, &amp;pt_prev, orig_dev, type,
    			       &amp;orig_dev-&gt;ptype_specific);
    
    	if (unlikely(skb-&gt;dev != orig_dev)) {
    		deliver_ptype_list_skb(skb, &amp;pt_prev, orig_dev, type,
    				       &amp;skb-&gt;dev-&gt;ptype_specific);
    	}
    
    	if (pt_prev) {
    		if (unlikely(skb_orphan_frags_rx(skb, GFP_ATOMIC)))
    			goto drop;
    		*ppt_prev = pt_prev;
    	} else {
    drop:
    		if (!deliver_exact)
    			dev_core_stats_rx_dropped_inc(skb-&gt;dev);
    		else
    			dev_core_stats_rx_nohandler_inc(skb-&gt;dev);
    		kfree_skb_reason(skb, SKB_DROP_REASON_UNHANDLED_PROTO);
    		/* Jamal, now you will not able to escape explaining
    		 * me how you were going to use this. :-)
    		 */
    		ret = NET_RX_DROP;
    	}
    
    out:
    	/* The invariant here is that if *ppt_prev is not NULL
    	 * then skb should also be non-NULL.
    	 *
    	 * Apparently *ppt_prev assignment above holds this invariant due to
    	 * skb dereferencing near it.
    	 */
    	*pskb = skb;
    	return ret;
    }
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-core/deliver_skb().md" data-href="Encyclopedia of NetworkSystem/Function/net-core/deliver_skb().md" href="encyclopedia-of-networksystem/function/net-core/deliver_skb().html" class="internal-link" target="_self" rel="noopener nofollow">deliver_skb()</a><br>
각종 체크를 통해 패킷을 드랍하거나 스택을 올려보내는 처리를 수행한다. 중간에 모든 entry에 대하여 deliver_skb() 함수를 호출해서 스택 위로 올려보내는 역할을 수행하게 된다.
]]></description><link>encyclopedia-of-networksystem/function/net-core/__netif_receive_skb_core().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/__netif_receive_skb_core().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[__netif_receive_skb_list_core()]]></title><description><![CDATA[ 
 <br>    static void __netif_receive_skb_list_core(struct list_head *head, bool pfmemalloc)
    {
    	/* Fast-path assumptions:
    	 * - There is no RX handler.
    	 * - Only one packet_type matches.
    	 * If either of these fails, we will end up doing some per-packet
    	 * processing in-line, then handling the 'last ptype' for the whole
    	 * sublist.  This can't cause out-of-order delivery to any single ptype,
    	 * because the 'last ptype' must be constant across the sublist, and all
    	 * other ptypes are handled per-packet.
    	 */
    	/* Current (common) ptype of sublist */
    	struct packet_type *pt_curr = NULL;
    	/* Current (common) orig_dev of sublist */
    	struct net_device *od_curr = NULL;
    	struct list_head sublist;
    	struct sk_buff *skb, *next;
    
    	INIT_LIST_HEAD(&amp;sublist);
    	list_for_each_entry_safe(skb, next, head, list) {
    		struct net_device *orig_dev = skb-&gt;dev;
    		struct packet_type *pt_prev = NULL;
    
    		skb_list_del_init(skb);
    		__netif_receive_skb_core(&amp;skb, pfmemalloc, &amp;pt_prev);
    		// [[Encyclopedia of NetworkSystem/Function/net-core/__netif_receive_skb_core().md|__netif_receive_skb_core()]]
    		if (!pt_prev)
    			continue;
    		if (pt_curr != pt_prev || od_curr != orig_dev) {
    			/* dispatch old sublist */
    			__netif_receive_skb_list_ptype(&amp;sublist, pt_curr, od_curr);
    			/* start new sublist */
    			INIT_LIST_HEAD(&amp;sublist);
    			pt_curr = pt_prev;
    			od_curr = orig_dev;
    		}
    		list_add_tail(&amp;skb-&gt;list, &amp;sublist);
    	}
    
    	/* dispatch final sublist */
    	__netif_receive_skb_list_ptype(&amp;sublist, pt_curr, od_curr);
    }
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-core/__netif_receive_skb_core().md" data-href="Encyclopedia of NetworkSystem/Function/net-core/__netif_receive_skb_core().md" href="encyclopedia-of-networksystem/function/net-core/__netif_receive_skb_core().html" class="internal-link" target="_self" rel="noopener nofollow">__netif_receive_skb_core()</a><br>
head 리스트에 있는 각각의 skb 패킷을 순회하게 되고, 이를 처리하기 위해 __netif_receive_skb_core()함수를 호출하게 된다.
]]></description><link>encyclopedia-of-networksystem/function/net-core/__netif_receive_skb_list_core().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/__netif_receive_skb_list_core().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[__netif_receive_skb_list()]]></title><description><![CDATA[ 
 <br>    static void __netif_receive_skb_list(struct list_head *head)
    {
    	unsigned long noreclaim_flag = 0;
    	struct sk_buff *skb, *next;
    	bool pfmemalloc = false; /* Is current sublist PF_MEMALLOC? */
    
    	list_for_each_entry_safe(skb, next, head, list) {
    		if ((sk_memalloc_socks() &amp;&amp; skb_pfmemalloc(skb)) != pfmemalloc) {
    			struct list_head sublist;
    
    			/* Handle the previous sublist */
    			list_cut_before(&amp;sublist, head, &amp;skb-&gt;list);
    			if (!list_empty(&amp;sublist))
    				__netif_receive_skb_list_core(&amp;sublist, pfmemalloc);
    			pfmemalloc = !pfmemalloc;
    			/* See comments in __netif_receive_skb */
    			if (pfmemalloc)
    				noreclaim_flag = memalloc_noreclaim_save();
    			else
    				memalloc_noreclaim_restore(noreclaim_flag);
    		}
    	}
    	/* Handle the remaining sublist */
    	if (!list_empty(head))
    		__netif_receive_skb_list_core(head, pfmemalloc);
    		// [[Encyclopedia of NetworkSystem/Function/net-core/__netif_receive_skb_list_core().md|__netif_receive_skb_list_core()]]
    	/* Restore pflags */
    	if (pfmemalloc)
    		memalloc_noreclaim_restore(noreclaim_flag);
    }
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-core/__netif_receive_skb_list_core().md" data-href="Encyclopedia of NetworkSystem/Function/net-core/__netif_receive_skb_list_core().md" href="encyclopedia-of-networksystem/function/net-core/__netif_receive_skb_list_core().html" class="internal-link" target="_self" rel="noopener nofollow">__netif_receive_skb_list_core()</a><br>
리스트의 각각의 skb에 대하여 pfmemalloc과 관련한 처리를 수행한다. 관련하여 더 찾아보려 하였으나 네트워크 스택과 깊이 관련이 있지 않은 것으로 판단하여 멈추었다. 그후 만약 리스트가 비어있지 않다면 __netif_receive_skb_list_core()를 호출하게 된다.
]]></description><link>encyclopedia-of-networksystem/function/net-core/__netif_receive_skb_list().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/__netif_receive_skb_list().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[__release_sock()]]></title><description><![CDATA[ 
 <br>void __release_sock(struct sock *sk)
	__releases(&amp;sk-&gt;sk_lock.slock)
	__acquires(&amp;sk-&gt;sk_lock.slock)
{
	struct sk_buff *skb, *next;

	while ((skb = sk-&gt;sk_backlog.head) != NULL) {
		sk-&gt;sk_backlog.head = sk-&gt;sk_backlog.tail = NULL;

		spin_unlock_bh(&amp;sk-&gt;sk_lock.slock);

		do {
			next = skb-&gt;next;
			prefetch(next);
			DEBUG_NET_WARN_ON_ONCE(skb_dst_is_noref(skb));
			skb_mark_not_on_list(skb);
			sk_backlog_rcv(sk, skb);

			cond_resched();

			skb = next;
		} while (skb != NULL);

		spin_lock_bh(&amp;sk-&gt;sk_lock.slock); // bh: bottom-half
	}

	/*
	 * Doing the zeroing here guarantee we can not loop forever
	 * while a wild producer attempts to flood us.
	 */
	sk-&gt;sk_backlog.len = 0;
}
]]></description><link>encyclopedia-of-networksystem/function/net-core/__release_sock().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/__release_sock().md</guid><pubDate>Tue, 08 Oct 2024 08:24:56 GMT</pubDate></item><item><title><![CDATA[__sk_flush_backlog()]]></title><description><![CDATA[ 
 <br>void __sk_flush_backlog(struct sock *sk)
{
	spin_lock_bh(&amp;sk-&gt;sk_lock.slock);
	__release_sock(sk);

	if (sk-&gt;sk_prot-&gt;release_cb)
		INDIRECT_CALL_INET_1(sk-&gt;sk_prot-&gt;release_cb,
				     tcp_release_cb, sk);

	spin_unlock_bh(&amp;sk-&gt;sk_lock.slock);
}
<br><a data-href="__release_sock()" href="encyclopedia-of-networksystem/function/net-core/__release_sock().html" class="internal-link" target="_self" rel="noopener nofollow">__release_sock()</a>]]></description><link>encyclopedia-of-networksystem/function/net-core/__sk_flush_backlog().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/__sk_flush_backlog().md</guid><pubDate>Tue, 08 Oct 2024 08:24:56 GMT</pubDate></item><item><title><![CDATA[__skb_datagram_iter()]]></title><description><![CDATA[ 
 <br>static int __skb_datagram_iter(const struct sk_buff *skb, int offset,
			       struct iov_iter *to, int len, bool fault_short,
			       size_t (*cb)(const void *, size_t, void *,
					    struct iov_iter *), void *data)
{
	int start = skb_headlen(skb);
	int i, copy = start - offset, start_off = offset, n;
	struct sk_buff *frag_iter;

	/* Copy header. */
	if (copy &gt; 0) {
		if (copy &gt; len)
			copy = len;
		n = INDIRECT_CALL_1(cb, simple_copy_to_iter,
				    skb-&gt;data + offset, copy, data, to);
		offset += n;
		if (n != copy)
			goto short_copy;
		if ((len -= copy) == 0)
			return 0;
	}

	/* Copy paged appendix. Hmm... why does this look so complicated? */
	for (i = 0; i &lt; skb_shinfo(skb)-&gt;nr_frags; i++) {
		int end;
		const skb_frag_t *frag = &amp;skb_shinfo(skb)-&gt;frags[i];

		WARN_ON(start &gt; offset + len);

		end = start + skb_frag_size(frag);
		if ((copy = end - offset) &gt; 0) {
			struct page *page = skb_frag_page(frag);
			u8 *vaddr = kmap(page);

			if (copy &gt; len)
				copy = len;
			n = INDIRECT_CALL_1(cb, simple_copy_to_iter,
					vaddr + skb_frag_off(frag) + offset - start,
					copy, data, to);
			kunmap(page);
			offset += n;
			if (n != copy)
				goto short_copy;
			if (!(len -= copy))
				return 0;
		}
		start = end;
	}

	skb_walk_frags(skb, frag_iter) {
		int end;

		WARN_ON(start &gt; offset + len);

		end = start + frag_iter-&gt;len;
		if ((copy = end - offset) &gt; 0) {
			if (copy &gt; len)
				copy = len;
			if (__skb_datagram_iter(frag_iter, offset - start,
						to, copy, fault_short, cb, data))
				goto fault;
			if ((len -= copy) == 0)
				return 0;
			offset += copy;
		}
		start = end;
	}
	if (!len)
		return 0;

	/* This is not really a user copy fault, but rather someone
	 * gave us a bogus length on the skb.  We should probably
	 * print a warning here as it may indicate a kernel bug.
	 */

fault:
	iov_iter_revert(to, offset - start_off);
	return -EFAULT;

short_copy:
	if (fault_short || iov_iter_count(to))
		goto fault;

	return 0;
}
<br>
copy_from_iter, copy_page_from_iter, skb_copy_datagram_from_iter의 함수들을 통해 각각 skb, frags, frag list에서 데이터를 복사해가는 모습을 볼 수 있다. 또한 frag list 에서 다시 재귀적으로 skb_copy_datagram_iter()를 호출하는 것을 볼 수 있었다.
]]></description><link>encyclopedia-of-networksystem/function/net-core/__skb_datagram_iter().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/__skb_datagram_iter().md</guid><pubDate>Wed, 06 Nov 2024 13:41:30 GMT</pubDate></item><item><title><![CDATA[`__netif_receive_skb_list()]]></title><description><![CDATA[ 
 <br>static void __netif_receive_skb_list(struct list_head *head)
{
	unsigned long noreclaim_flag = 0;
	struct sk_buff *skb, *next;
	bool pfmemalloc = false; /* Is current sublist PF_MEMALLOC? */

	list_for_each_entry_safe(skb, next, head, list) {
		if ((sk_memalloc_socks() &amp;&amp; skb_pfmemalloc(skb)) != pfmemalloc) {
			struct list_head sublist;

			/* Handle the previous sublist */
			list_cut_before(&amp;sublist, head, &amp;skb-&gt;list);
			if (!list_empty(&amp;sublist))
				__netif_receive_skb_list_core(&amp;sublist, pfmemalloc);
			pfmemalloc = !pfmemalloc;
			/* See comments in __netif_receive_skb */
			if (pfmemalloc)
				noreclaim_flag = memalloc_noreclaim_save();
			else
				memalloc_noreclaim_restore(noreclaim_flag);
		}
	}
	/* Handle the remaining sublist */
	if (!list_empty(head))
		__netif_receive_skb_list_core(head, pfmemalloc);
	/* Restore pflags */
	if (pfmemalloc)
		memalloc_noreclaim_restore(noreclaim_flag);
}
]]></description><link>encyclopedia-of-networksystem/function/net-core/`__netif_receive_skb_list().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/`__netif_receive_skb_list().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[deliver_skb()]]></title><description><![CDATA[ 
 <br>static inline int deliver_skb(struct sk_buff *skb,
				  struct packet_type *pt_prev,
				  struct net_device *orig_dev)
{
	if (unlikely(skb_orphan_frags_rx(skb, GFP_ATOMIC)))
		return -ENOMEM;
	refcount_inc(&amp;skb-&gt;users);
	return pt_prev-&gt;func(skb, skb-&gt;dev, pt_prev, orig_dev);
}
<br>
skb_orphan_frags_rx()함수의 리턴값이 true이면 -ENOMEM을 리턴한다. 그게 아니라면 refcount_inc(&amp;skb-&gt;users)를 통해 참조 카운터를 증가시키고, pt_prev-&gt;func()를 통해 해당하는 패킷타입에 알맞은 처리 함수를 실행해주게 된다.
<br>
함수 포인터 매핑부터 살펴보자면, 우선 net/ipv4/af_inet.c에서 inet_init()함수에서 dev_add_pack(&amp;ip_packet_type)함수를 호출한다. 이는 네트워크 스택에다가 다루어져야 할 패킷 타입들에 대한 handler_function을 매핑하는 함수이다. net_hotdata혹은 받은 packet_type이 가르키고 있는 net_dev의 ptype_all 리스트에 이를 추가하게 된다. 여기서는 dev에 해당하는 포인터가 위의 ip_packet_type을 선언하고 초기화하는 과정에서 NULL값으로 셋팅 될 것이므로, net_hotdata-&gt;ptype_all에 저장 될 것이다. 이 때 func에 매핑되는 함수는 ip_rcv이고, list_func에 매핑되는 함수는ip_list_rcv이다.
<br><a data-href="ip_rcv()" href="encyclopedia-of-networksystem/function/net-ipv4/ip_rcv().html" class="internal-link" target="_self" rel="noopener nofollow">ip_rcv()</a>]]></description><link>encyclopedia-of-networksystem/function/net-core/deliver_skb().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/deliver_skb().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[dev_add_pack()]]></title><description><![CDATA[ 
 <br>/**
* dev_add_pack - add packet handler
* @pt: packet type declaration
*
* Add a protocol handler to the networking stack. The passed &amp;packet_type
* is linked into kernel lists and may not be freed until it has been
* removed from the kernel lists.
*
* This call does not sleep therefore it can not
* guarantee all CPU's that are in middle of receiving packets
* will see the new packet type (until the next received packet).
*/
  
void dev_add_pack(struct packet_type *pt)
{
	struct list_head *head = ptype_head(pt);
	  
	spin_lock(&amp;ptype_lock);
	list_add_rcu(&amp;pt-&gt;list, head);
	spin_unlock(&amp;ptype_lock);
}
EXPORT_SYMBOL(dev_add_pack);
<br>
역할이 패킷 핸들러가 맞았다. 네트워크 스택에다가 프로토콜 별로 적용할 수 있는 패킷 핸들럴를 추가하는 역할을 하게 된다.
<br>
ptype_head함수가 어떤 것인지는 __netif_receive_skb_core()함수 분석 노트에서 같이 다루었다.<br>
<a data-href="`__netif_receive_skb_core() 함수에 대한 로직 분석과 고찰 및 의의" href="김기수/산협-프로젝트-2/백서-제작용/`__netif_receive_skb_core()-함수에-대한-로직-분석과-고찰-및-의의.html" class="internal-link" target="_self" rel="noopener nofollow">`__netif_receive_skb_core() 함수에 대한 로직 분석과 고찰 및 의의</a><br>
따라서 패킷 핸들러의 성격에 따라 4가지의 저장소로 가게 되는 것이다. 해당 list_head에 이 패킷핸들러를 추가하게 된다.
]]></description><link>encyclopedia-of-networksystem/function/net-core/dev_add_pack().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/dev_add_pack().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[dev_gro_receive()]]></title><description><![CDATA[ 
 <br>    static enum gro_result dev_gro_receive(struct napi_struct *napi, struct sk_buff *skb) // [[Encyclopedia of NetworkSystem/Struct/include-linux/gro_result.md|gro_result]] [[Encyclopedia of NetworkSystem/Struct/include-linux/sk_buff.md|skbuff]]
    {
    	// 변수 선언 및 초기화
    	u32 bucket = skb_get_hash_raw(skb) &amp; (GRO_HASH_BUCKETS - 1); // 나머지 계산
    	struct gro_list *gro_list = &amp;napi-&gt;gro_hash[bucket];
    	struct list_head *head = &amp;net_hotdata.offload_base;
    	struct packet_offload *ptype; // [[Encyclopedia of NetworkSystem/Struct/include-linux/packet_offload.md|packet_offload]]
    	__be16 type = skb-&gt;protocol;
    	struct sk_buff *pp = NULL;
    	enum gro_result ret;
    	int same_flow;
    	
    	// GRO 비활성화 조건 확인
    	if (netif_elide_gro(skb-&gt;dev)) // [[Encyclopedia of NetworkSystem/Function/include-linux/netif_elide_gro().md|netif_elide_gro()]]
    		goto normal;
    
    	// GRO 리스트 준비
    	gro_list_prepare(&amp;gro_list-&gt;list, skb); // [[Encyclopedia of NetworkSystem/Function/net-core/gro_list_prepare().md|gro_list_prepare()]]
    	// gro list 안에 있는 skb 속 control block안에 있는 same flow 변수들을 설정해준다
    	
    	// RCU(Read-Copy-Update) 읽기 잠금
    	rcu_read_lock();
    	list_for_each_entry_rcu(ptype, head, list) {
    		if (ptype-&gt;type == type &amp;&amp; ptype-&gt;callbacks.gro_receive)
    		// list에 있는 pkt들의 protocol이 입력받은 skb와 동일한지 + gro_receive 인지 확인
    			goto found_ptype;
    	}
    	rcu_read_unlock();
    	goto normal;
    
    found_ptype:
    	// 패킷 헤더 설정
    	skb_set_network_header(skb, skb_gro_offset(skb));
    	skb_reset_mac_len(skb);
    	BUILD_BUG_ON(sizeof_field(struct napi_gro_cb, zeroed) != sizeof(u32));
    	BUILD_BUG_ON(!IS_ALIGNED(offsetof(struct napi_gro_cb, zeroed),
    					sizeof(u32))); /* Avoid slow unaligned acc */
    	*(u32 *)&amp;NAPI_GRO_CB(skb)-&gt;zeroed = 0;
    	NAPI_GRO_CB(skb)-&gt;flush = skb_has_frag_list(skb); // [[Encyclopedia of NetworkSystem/Struct/include-net/napi_gro_cb.md|NAPI_GRO_CB]]
    	NAPI_GRO_CB(skb)-&gt;is_atomic = 1;
    	NAPI_GRO_CB(skb)-&gt;count = 1;
    	if (unlikely(skb_is_gso(skb))) {
    		NAPI_GRO_CB(skb)-&gt;count = skb_shinfo(skb)-&gt;gso_segs;
    		// gso로 생성된 skb의 경우, count 변수에 gso_segment 개수를 설정해준다. 
    		/* Only support TCP and non DODGY users. */
    		if (!skb_is_gso_tcp(skb) ||
    		    (skb_shinfo(skb)-&gt;gso_type &amp; SKB_GSO_DODGY))
    			NAPI_GRO_CB(skb)-&gt;flush = 1;
    	}
    
    	/* Setup for GRO checksum validation */
    	// GRO 체크섬 검증 설정
    	switch (skb-&gt;ip_summed) {
    	case CHECKSUM_COMPLETE:
    		NAPI_GRO_CB(skb)-&gt;csum = skb-&gt;csum;
    		NAPI_GRO_CB(skb)-&gt;csum_valid = 1;
    		break;
    	case CHECKSUM_UNNECESSARY:
    		NAPI_GRO_CB(skb)-&gt;csum_cnt = skb-&gt;csum_level + 1;
    		break;
    	}
    	
    	// 패킷 처리
    	pp = INDIRECT_CALL_INET(ptype-&gt;callbacks.gro_receive,
    				ipv6_gro_receive, inet_gro_receive,
    				&amp;gro_list-&gt;list, skb); // [[Encyclopedia of NetworkSystem/Function/net-ipv6/ipv6_gro_receive().md|ipv6_gro_receive()]] [[Encyclopedia of NetworkSystem/Function/net-ipv4/inet_gro_receive().md|inet_gro_receive()]] 
    
    	rcu_read_unlock();
    
    	if (PTR_ERR(pp) == -EINPROGRESS) {
    		ret = GRO_CONSUMED;
    		goto ok;
    	}
    
    	same_flow = NAPI_GRO_CB(skb)-&gt;same_flow;
    	ret = NAPI_GRO_CB(skb)-&gt;free ? GRO_MERGED_FREE : GRO_MERGED;
    
    	if (pp) {
    		skb_list_del_init(pp);
	    	napi_gro_complete(napi, pp); // [[Encyclopedia of NetworkSystem/Function/net-core/napi_gro_complete().md|napi_gro_complete()]]
    		gro_list-&gt;count--;
    	}
    
    	if (same_flow)
    		goto ok;
    
    	if (NAPI_GRO_CB(skb)-&gt;flush)
    		goto normal;
    
    	if (unlikely(gro_list-&gt;count &gt;= MAX_GRO_SKBS))
    		gro_flush_oldest(napi, &amp;gro_list-&gt;list);
    	else
    		gro_list-&gt;count++;
    
    	/* Must be called before setting NAPI_GRO_CB(skb)-&gt;{age|last} */
    	gro_try_pull_from_frag0(skb);
    	NAPI_GRO_CB(skb)-&gt;age = jiffies;
    	NAPI_GRO_CB(skb)-&gt;last = skb;
    	if (!skb_is_gso(skb))
    		skb_shinfo(skb)-&gt;gso_size = skb_gro_len(skb);
    	list_add(&amp;skb-&gt;list, &amp;gro_list-&gt;list);
    	ret = GRO_HELD;
    ok:
    	if (gro_list-&gt;count) {
    		if (!test_bit(bucket, &amp;napi-&gt;gro_bitmask))
    			__set_bit(bucket, &amp;napi-&gt;gro_bitmask);
    	} else if (test_bit(bucket, &amp;napi-&gt;gro_bitmask)) {
    		__clear_bit(bucket, &amp;napi-&gt;gro_bitmask);
    	}
    
    	return ret;
    
    normal:
    	ret = GRO_NORMAL;
    	gro_try_pull_from_frag0(skb);
    	goto ok;
    }
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Struct/include-linux/gro_result.md" data-href="Encyclopedia of NetworkSystem/Struct/include-linux/gro_result.md" href="encyclopedia-of-networksystem/struct/include-linux/gro_result.html" class="internal-link" target="_self" rel="noopener nofollow">gro_result</a>&nbsp;<br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Struct/include-linux/sk_buff.md" data-href="Encyclopedia of NetworkSystem/Struct/include-linux/sk_buff.md" href="encyclopedia-of-networksystem/struct/include-linux/sk_buff.html" class="internal-link" target="_self" rel="noopener nofollow">skbuff</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Struct/include-linux/packet_offload.md" data-href="Encyclopedia of NetworkSystem/Struct/include-linux/packet_offload.md" href="encyclopedia-of-networksystem/struct/include-linux/packet_offload.html" class="internal-link" target="_self" rel="noopener nofollow">packet_offload</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/include-linux/netif_elide_gro().md" data-href="Encyclopedia of NetworkSystem/Function/include-linux/netif_elide_gro().md" href="encyclopedia-of-networksystem/function/include-linux/netif_elide_gro().html" class="internal-link" target="_self" rel="noopener nofollow">netif_elide_gro()</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-core/gro_list_prepare().md" data-href="Encyclopedia of NetworkSystem/Function/net-core/gro_list_prepare().md" href="encyclopedia-of-networksystem/function/net-core/gro_list_prepare().html" class="internal-link" target="_self" rel="noopener nofollow">gro_list_prepare()</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Struct/include-net/napi_gro_cb.md" data-href="Encyclopedia of NetworkSystem/Struct/include-net/napi_gro_cb.md" href="encyclopedia-of-networksystem/struct/include-net/napi_gro_cb.html" class="internal-link" target="_self" rel="noopener nofollow">NAPI_GRO_CB</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-ipv6/ipv6_gro_receive().md" data-href="Encyclopedia of NetworkSystem/Function/net-ipv6/ipv6_gro_receive().md" href="encyclopedia-of-networksystem/function/net-ipv6/ipv6_gro_receive().html" class="internal-link" target="_self" rel="noopener nofollow">ipv6_gro_receive()</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-ipv4/inet_gro_receive().md" data-href="Encyclopedia of NetworkSystem/Function/net-ipv4/inet_gro_receive().md" href="encyclopedia-of-networksystem/function/net-ipv4/inet_gro_receive().html" class="internal-link" target="_self" rel="noopener nofollow">inet_gro_receive()</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-core/napi_gro_complete().md" data-href="Encyclopedia of NetworkSystem/Function/net-core/napi_gro_complete().md" href="encyclopedia-of-networksystem/function/net-core/napi_gro_complete().html" class="internal-link" target="_self" rel="noopener nofollow">napi_gro_complete()</a><br>
실질적으로 gro를 처리하는 함수. 인수로 받아온 skb 포인터에다가 패킷 정보를 쌓기 시작함. 코드 중간에 pp = INDIRECT_CALL_INET()이라는 함수가 호출되는데 여기서 IPv4와 IPv6로 나뉘어 콜백이 되게 됨. gro_list도 함께 전달해주며, 이는 &amp;napi→gro_hash[bucket]에서 가져온 결과임. 더 깊은 호출은 net/ipv4/af_inet.c의 inet_gro_receive() 함수를 보면 됨.
<br>
만약 합친 return이 있다면(패킷 병합이 이루어짐. 아니라면 NULL반환, -&gt; tcp_gro_receive()함수에서 보면 pp는 NULL로 initialize 됨) 따라서 if(pp)라면, 여기서 gro_list-&gt;count--를 하는 이유는 skb_list_del_init(pp) 를 통해 gro_list에서 해당 패킷을 dequeue하기 때문이다. 더이상 napi-&gt;gro_hash array에 있지 않기 때문에 이를 위해서 gro_list-&gt;count--를 하게 되는 것이다.
만약 합쳐진거라면 same_flow가 true이므로 바로 ok라벨로 가게 된다. 만약 아니라면, 새로 gro_list에 추가되어야 하므로 우선 gro_list가 가득 찼다면 오래된 패킷을 드랍한다. 아니면 list_add()함수를 통해 해당 패킷을 추가하게 된다.
<br>
<br>해시 계산 및 리스트 설정:

<br>패킷의 해시 값을 계산하여 적절한 gro_list를 선택한다.
<br>gro_list_prepare로 리스트를 준비한다.


<br>GRO 비활성화 조건 확인:

<br>netif_elide_gro를 사용해 GRO를 비활성화할지 결정한다.


<br>패킷 타입 확인 및 처리:

<br>rcu_read_lock을 사용해 RCU 읽기 잠금을 설정하고, packet_offload 리스트를 통해 패킷 타입을 확인한다.
<br>해당 타입을 찾으면 해당 콜백 함수를 호출하여 패킷을 처리한다.


<br>패킷 헤더 및 체크섬 설정:

<br>패킷의 네트워크 헤더와 MAC 길이를 설정하고, 체크섬 검증을 설정한다.


<br>GRO 수신 처리:

<br>ptype-&gt;callbacks.gro_receive 함수를 호출하여 패킷을 처리한다.
<br>패킷이 동일한 흐름인지 확인하고, 병합 결과에 따라 적절한 처리를 수행한다.


<br>패킷 리스트 관리:

<br>병합된 패킷 리스트를 관리하고, 필요시 오래된 패킷을 플러시한다.
<br>새로운 패킷을 리스트에 추가하고, 타이머를 설정한다.


<br>결과 반환:

<br>최종적으로 처리 결과를 반환한다.


]]></description><link>encyclopedia-of-networksystem/function/net-core/dev_gro_receive().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/dev_gro_receive().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[do_xdp_generic()]]></title><description><![CDATA[ 
 ]]></description><link>encyclopedia-of-networksystem/function/net-core/do_xdp_generic().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/do_xdp_generic().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[enqueue_to_backlog()]]></title><description><![CDATA[ 
 <br>static int enqueue_to_backlog(struct sk_buff *skb, int cpu,
			      unsigned int *qtail)
{
	enum skb_drop_reason reason;
	struct softnet_data *sd;
	unsigned long flags;
	unsigned int qlen;

	reason = SKB_DROP_REASON_NOT_SPECIFIED;
	sd = &amp;per_cpu(softnet_data, cpu);
2
	rps_lock_irqsave(sd, &amp;flags);
	if (!netif_running(skb-&gt;dev))
		goto drop;
	qlen = skb_queue_len(&amp;sd-&gt;input_pkt_queue);
	if (qlen &lt;= READ_ONCE(net_hotdata.max_backlog) &amp;&amp;
	    !skb_flow_limit(skb, qlen)) {
		if (qlen) {
enqueue:
			__skb_queue_tail(&amp;sd-&gt;input_pkt_queue, skb);
			input_queue_tail_incr_save(sd, qtail);
			rps_unlock_irq_restore(sd, &amp;flags);
			return NET_RX_SUCCESS;
		}

		/* Schedule NAPI for backlog device
		 * We can use non atomic operation since we own the queue lock
		 */
		if (!__test_and_set_bit(NAPI_STATE_SCHED, &amp;sd-&gt;backlog.state))
			napi_schedule_rps(sd);
		goto enqueue;
	}
	reason = SKB_DROP_REASON_CPU_BACKLOG;

drop:
	sd-&gt;dropped++;
	rps_unlock_irq_restore(sd, &amp;flags);

	dev_core_stats_rx_dropped_inc(skb-&gt;dev);
	kfree_skb_reason(skb, reason);
	return NET_RX_DROP;
}
<br><a data-href="napi_schedule_rps()" href="encyclopedia-of-networksystem/function/net-core/napi_schedule_rps().html" class="internal-link" target="_self" rel="noopener nofollow">napi_schedule_rps()</a><br>enqueue_to_backlog()가 해당 skb가 처리되어야 할 cpu로 보내는 주요 역할을 하는 함수이다. enqueue_to_backlog()에서는 가야할 cpu의 sd→input_pck_queue에 tail에 덧붙이는 작업만 하여, 적절한 cpu에서 처리 될 수 있도록 한다. 그런데 만약, 그 cpu의 input_pck_queue가 비어있는 상황이면, __napi_schedule_rps()를 실행하여, 해당 cpu의 input_pck_queue를 새롭게 스케줄링 한다.<br>이 함수에서부터의 주요 관계도 그림은 아래 링크와 같다.<br>
<a data-href="RPS-IPI 관계도" href="excalidraw/rps-ipi-관계도.html" class="internal-link" target="_self" rel="noopener nofollow">RPS-IPI 관계도</a>]]></description><link>encyclopedia-of-networksystem/function/net-core/enqueue_to_backlog().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/enqueue_to_backlog().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[get_rps_cpu()]]></title><description><![CDATA[ 
 <br><img alt="Pasted image 20240811235306.png" src="lib/media/pasted-image-20240811235306.png"><br>static int get_rps_cpu(struct net_device *dev, struct sk_buff *skb,
		       struct rps_dev_flow **rflowp)
{
	const struct rps_sock_flow_table *sock_flow_table;
	struct netdev_rx_queue *rxqueue = dev-&gt;_rx;
	struct rps_dev_flow_table *flow_table;
	struct rps_map *map;
	int cpu = -1;
	u32 tcpu;
	u32 hash;

	if (skb_rx_queue_recorded(skb)) {
		u16 index = skb_get_rx_queue(skb);

		if (unlikely(index &gt;= dev-&gt;real_num_rx_queues)) {
			WARN_ONCE(dev-&gt;real_num_rx_queues &gt; 1,
				  "%s received packet on queue %u, but number "
				  "of RX queues is %u\n",
				  dev-&gt;name, index, dev-&gt;real_num_rx_queues);
			goto done;
		}
		rxqueue += index;
	}

	/* Avoid computing hash if RFS/RPS is not active for this rxqueue */

	flow_table = rcu_dereference(rxqueue-&gt;rps_flow_table);
	map = rcu_dereference(rxqueue-&gt;rps_map);
	if (!flow_table &amp;&amp; !map)
		goto done;

	skb_reset_network_header(skb);
	hash = skb_get_hash(skb);
	if (!hash)
		goto done;

	sock_flow_table = rcu_dereference(net_hotdata.rps_sock_flow_table);
	if (flow_table &amp;&amp; sock_flow_table) {
		struct rps_dev_flow *rflow;
		u32 next_cpu;
		u32 ident;

		/* First check into global flow table if there is a match.
		 * This READ_ONCE() pairs with WRITE_ONCE() from rps_record_sock_flow().
		 */
		ident = READ_ONCE(sock_flow_table-&gt;ents[hash &amp; sock_flow_table-&gt;mask]);
		if ((ident ^ hash) &amp; ~net_hotdata.rps_cpu_mask)
			goto try_rps;

		next_cpu = ident &amp; net_hotdata.rps_cpu_mask;

		/* OK, now we know there is a match,
		 * we can look at the local (per receive queue) flow table
		 */
		rflow = &amp;flow_table-&gt;flows[hash &amp; flow_table-&gt;mask];
		tcpu = rflow-&gt;cpu;

		/*
		 * If the desired CPU (where last recvmsg was done) is
		 * different from current CPU (one in the rx-queue flow
		 * table entry), switch if one of the following holds:
		 *   - Current CPU is unset (&gt;= nr_cpu_ids).
		 *   - Current CPU is offline.
		 *   - The current CPU's queue tail has advanced beyond the
		 *     last packet that was enqueued using this table entry.
		 *     This guarantees that all previous packets for the flow
		 *     have been dequeued, thus preserving in order delivery.
		 */
		if (unlikely(tcpu != next_cpu) &amp;&amp;
		    (tcpu &gt;= nr_cpu_ids || !cpu_online(tcpu) ||
		     ((int)(per_cpu(softnet_data, tcpu).input_queue_head -
		      rflow-&gt;last_qtail)) &gt;= 0)) {
			tcpu = next_cpu;
			rflow = set_rps_cpu(dev, skb, rflow, next_cpu);
		}

		if (tcpu &lt; nr_cpu_ids &amp;&amp; cpu_online(tcpu)) {
			*rflowp = rflow;
			cpu = tcpu;
			goto done;
		}
	}

try_rps:

	if (map) {
		tcpu = map-&gt;cpus[reciprocal_scale(hash, map-&gt;len)];
		if (cpu_online(tcpu)) {
			cpu = tcpu;
			goto done;
		}
	}

done:
	return cpu;
}
<br><a data-href="netdev_rx_queue" href="encyclopedia-of-networksystem/struct/include-net/netdev_rx_queue.html" class="internal-link" target="_self" rel="noopener nofollow">netdev_rx_queue</a><br>
<a data-href="rps_map" href="encyclopedia-of-networksystem/struct/include-net/rps_map.html" class="internal-link" target="_self" rel="noopener nofollow">rps_map</a><br>
<a data-href="rps_dev_flow_table" href="황재훈/research-intern/081324/rps_dev_flow_table.html" class="internal-link" target="_self" rel="noopener nofollow">rps_dev_flow_table</a><br>get_rps_cpu 함수는 netif_receive_skb 에서 호출된 함수로 네트워크 스택에서 패킷을 처리할 CPU를 결정하는 데 사용된다. 이 함수는 특정 네트워크 장치에서 수신된 패킷(skb)을 기반으로 가장 적합한 CPU를 선택한다.<br>static int get_rps_cpu(struct net_device *dev, struct sk_buff *skb,
                       struct rps_dev_flow **rflowp)
{
    const struct rps_sock_flow_table *sock_flow_table;
    struct netdev_rx_queue *rxqueue = dev-&gt;_rx;
    struct rps_dev_flow_table *flow_table;
    struct rps_map *map;
    int cpu = -1;
    u32 tcpu;
    u32 hash;
<br> 함수는 네트워크 장치(dev)와 패킷(skb)을 기반으로 여러 데이터 구조를 초기화한다. cpu는 기본적으로 -1로 초기화되어 유효하지 않은 CPU를 의미한다. 이후 skb가 기록된 RX 큐를 사용해야 하는지 확인한다. RX 큐는 패킷이 수신된 큐를 나타내며, rxqueue 변수에 해당 큐를 참조한다.<br>    if (skb_rx_queue_recorded(skb)) {
        u16 index = skb_get_rx_queue(skb);

        if (unlikely(index &gt;= dev-&gt;real_num_rx_queues)) {
            WARN_ONCE(dev-&gt;real_num_rx_queues &gt; 1,
                      "%s received packet on queue %u, but number "
                      "of RX queues is %u\n",
                      dev-&gt;name, index, dev-&gt;real_num_rx_queues);
            goto done;
        }
        rxqueue += index;
    }
<br>skb가 유효한 RX 큐에서 수신된 경우인지 검사한다. RX 큐의 인덱스가 올바른지 확인하고, 그렇지 않으면 경고를 출력하고 종료한다.<br>    flow_table = rcu_dereference(rxqueue-&gt;rps_flow_table);
    map = rcu_dereference(rxqueue-&gt;rps_map);
    if (!flow_table &amp;&amp; !map)
        goto done;

    skb_reset_network_header(skb);
    hash = skb_get_hash(skb);
    if (!hash)
        goto done;
<br>rps_flow_table과 rps_map이 설정되어 있는지 확인한다. 둘 다 설정되지 않았다면 RPS가 비활성화된 것으로 판단하고 종료한다. 이후 패킷의 해시 값을 계산한다. 해시 값은 RPS/RFS에서 패킷 흐름을 결정하는 데 사용된다.<br>    sock_flow_table = rcu_dereference(net_hotdata.rps_sock_flow_table);
    if (flow_table &amp;&amp; sock_flow_table) {
        struct rps_dev_flow *rflow;
        u32 next_cpu;
        u32 ident;

        ident = READ_ONCE(sock_flow_table-&gt;ents[hash &amp; sock_flow_table-&gt;mask]);
        if ((ident ^ hash) &amp; ~net_hotdata.rps_cpu_mask)
            goto try_rps;

        next_cpu = ident &amp; net_hotdata.rps_cpu_mask;

        rflow = &amp;flow_table-&gt;flows[hash &amp; flow_table-&gt;mask];
        tcpu = rflow-&gt;cpu;
<br>sock_flow_table에서 현재 패킷의 해시 값과 일치하는 항목이 있는지 확인합니다. next_cpu에 일치하는 CPU를 저장한다. 로컬 흐름 테이블에서 패킷이 지정된 CPU와 일치하는지 확인한다. 만약 지정된 CPU가 다르거나 오프라인 상태이거나 큐에 더 이상 패킷이 없다면, CPU를 새로 설정한다.<br>        if (unlikely(tcpu != next_cpu) &amp;&amp;
            (tcpu &gt;= nr_cpu_ids || !cpu_online(tcpu) ||
             ((int)(per_cpu(softnet_data, tcpu).input_queue_head -
              rflow-&gt;last_qtail)) &gt;= 0)) {
            tcpu = next_cpu;
            rflow = set_rps_cpu(dev, skb, rflow, next_cpu);
        }

        if (tcpu &lt; nr_cpu_ids &amp;&amp; cpu_online(tcpu)) {
            *rflowp = rflow;
            cpu = tcpu;
            goto done;
        }
    }
<br>CPU가 현재 유효하고 온라인 상태인지 확인한다. 만약 유효하다면, cpu 변수를 업데이트하고 종료한다.<br>try_rps:

    if (map) {
        tcpu = map-&gt;cpus[reciprocal_scale(hash, map-&gt;len)];
        if (cpu_online(tcpu)) {
            cpu = tcpu;
            goto done;
        }
    }

done:
    return cpu;
}
<br>&nbsp;만약 위에서 CPU를 찾지 못했을 경우, RPS 맵을 사용하여 CPU를 결정한다. map에서 계산된 tcpu가 온라인 상태라면 해당 CPU를 사용한다. 최종적으로 결정된 CPU 값을 반환합니다. 유효한 CPU를 찾지 못했을 경우, -1을 반환한다.]]></description><link>encyclopedia-of-networksystem/function/net-core/get_rps_cpu().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/get_rps_cpu().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate><enclosure url="lib/media/pasted-image-20240811235306.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240811235306.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[gro_list_prepare()]]></title><description><![CDATA[ 
 <br>static void gro_list_prepare(const struct list_head *head, const struct sk_buff *skb)
{
    unsigned int maclen = skb-&gt;dev-&gt;hard_header_len;
    u32 hash = skb_get_hash_raw(skb);
    struct sk_buff *p;
    
    list_for_each_entry(p, head, list) {
    	unsigned long diffs;
    
    	NAPI_GRO_CB(p)-&gt;flush = 0;
    
    	if (hash != skb_get_hash_raw(p)) {
    		NAPI_GRO_CB(p)-&gt;same_flow = 0;
    		continue;
    	} // hash 값이 다르면 same_flow가 아니라고 판단하고 나간다. 
    		
    	// hash 값이 같은 경우: diff 값에 따라 pkt의 same_flow 값을 정한다. 
    	// diff 변수의 값을 통해 same_flow인지 결정한다.
    	// skb-&gt;dev, vlan_all, metadata, mac header가 모두 동일해야 한다. 
    	diffs = (unsigned long)p-&gt;dev ^ (unsigned long)skb-&gt;dev;
    	diffs |= p-&gt;vlan_all ^ skb-&gt;vlan_all;
    	diffs |= skb_metadata_differs(p, skb);
    	if (maclen == ETH_HLEN)
    		diffs |= compare_ether_header(skb_mac_header(p),
    					      skb_mac_header(skb));
    	else if (!diffs)
    		diffs = memcmp(skb_mac_header(p),
    			       skb_mac_header(skb),
    			       maclen);
    
    	/* in most common scenarions 'slow_gro' is 0
    	 * otherwise we are already on some slower paths
    	 * either skip all the infrequent tests altogether or
    	 * avoid trying too hard to skip each of them individually
    	 */
    	if (!diffs &amp;&amp; unlikely(skb-&gt;slow_gro | p-&gt;slow_gro)) {
    		diffs |= p-&gt;sk != skb-&gt;sk;
    		diffs |= skb_metadata_dst_cmp(p, skb);
    		diffs |= skb_get_nfct(p) ^ skb_get_nfct(skb);
    
    		diffs |= gro_list_prepare_tc_ext(skb, p, diffs);
    	}
    
    	NAPI_GRO_CB(p)-&gt;same_flow = !diffs;
    	// or 연산한 값들 중 하나라도 다른 것이 있으면, !diff에서 0이 되므로 same_flow가 아닌 것으로 처리 된다. 
    }
}
<br>
리스트의 각각의 entry에 대하여 skb와 헤더, 즉 메타데이터 등을 비교함으로써 같은 flow인지 확인하는 과정을 거친다. 주로 XOR 함수를 통해 그 결과를 diff에 저장하여, 만약 다르다면 1, 같다면 0이 저장 될 것이다. 이를 다시 각각의 skb에 same_flow에 !diff로 저장하게 된다.
]]></description><link>encyclopedia-of-networksystem/function/net-core/gro_list_prepare().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/gro_list_prepare().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[napi_build_skb()]]></title><description><![CDATA[ 
 <br>/**
 * napi_build_skb - build a network buffer
 * @data: data buffer provided by caller
 * @frag_size: size of data
 *
 * Version of __napi_build_skb() that takes care of skb-&gt;head_frag
 * and skb-&gt;pfmemalloc when the data is a page or page fragment.
 *
 * Returns a new &amp;sk_buff on success, %NULL on allocation failure.
 */
struct sk_buff *napi_build_skb(void *data, unsigned int frag_size)
{
	struct sk_buff *skb = __napi_build_skb(data, frag_size); // [[Encyclopedia of NetworkSystem/Function/net-core/__napi_build_skb().md|__napi_build_skb()]]

	if (likely(skb) &amp;&amp; frag_size) {
		skb-&gt;head_frag = 1;
		skb_propagate_pfmemalloc(virt_to_head_page(data), skb); // [[Encyclopedia of NetworkSystem/Function/include-linux/skb_propagate_pfmemalloc().md|skb_propagate_pfmemalloc()]]
	}

	return skb;
}
EXPORT_SYMBOL(napi_build_skb);
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-core/__napi_build_skb().md" data-href="Encyclopedia of NetworkSystem/Function/net-core/__napi_build_skb().md" href="encyclopedia-of-networksystem/function/net-core/__napi_build_skb().html" class="internal-link" target="_self" rel="noopener nofollow">__napi_build_skb()</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/include-linux/skb_propagate_pfmemalloc().md" data-href="Encyclopedia of NetworkSystem/Function/include-linux/skb_propagate_pfmemalloc().md" href="encyclopedia-of-networksystem/function/include-linux/skb_propagate_pfmemalloc().html" class="internal-link" target="_self" rel="noopener nofollow">skb_propagate_pfmemalloc()</a><br>
__napi_build_skb를 실행하여 skb 포인터를 획득하고, 이를 다시 반환한다. 이후 skb_propagate_pfmemalloc을 호출하게 된다.
]]></description><link>encyclopedia-of-networksystem/function/net-core/napi_build_skb().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/napi_build_skb().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[napi_complete_done()]]></title><description><![CDATA[ 
 <br>bool napi_complete_done(struct napi_struct *n, int work_done)
{
	unsigned long flags, val, new, timeout = 0;
	bool ret = true;

	/*
	 * 1) Don't let napi dequeue from the cpu poll list
	 *    just in case its running on a different cpu.
	 * 2) If we are busy polling, do nothing here, we have
	 *    the guarantee we will be called later.
	 */
	if (unlikely(n-&gt;state &amp; (NAPIF_STATE_NPSVC |
				 NAPIF_STATE_IN_BUSY_POLL)))
		return false;

	if (work_done) {
		if (n-&gt;gro_bitmask)
			timeout = READ_ONCE(n-&gt;dev-&gt;gro_flush_timeout);
		n-&gt;defer_hard_irqs_count = READ_ONCE(n-&gt;dev-&gt;napi_defer_hard_irqs);
	}
	if (n-&gt;defer_hard_irqs_count &gt; 0) {
		n-&gt;defer_hard_irqs_count--;
		timeout = READ_ONCE(n-&gt;dev-&gt;gro_flush_timeout);
		if (timeout)
			ret = false;
	}
	if (n-&gt;gro_bitmask) {
		/* When the NAPI instance uses a timeout and keeps postponing
		 * it, we need to bound somehow the time packets are kept in
		 * the GRO layer
		 */
		napi_gro_flush(n, !!timeout);
	}

	gro_normal_list(n); // [[Encyclopedia of NetworkSystem/Function/include-net/gro_normal_list().md|gro_normal_list()]]

	if (unlikely(!list_empty(&amp;n-&gt;poll_list))) {
		/* If n-&gt;poll_list is not empty, we need to mask irqs */
		local_irq_save(flags);
		list_del_init(&amp;n-&gt;poll_list);
		local_irq_restore(flags);
	}
	WRITE_ONCE(n-&gt;list_owner, -1);

	val = READ_ONCE(n-&gt;state);
	do {
		WARN_ON_ONCE(!(val &amp; NAPIF_STATE_SCHED));

		new = val &amp; ~(NAPIF_STATE_MISSED | NAPIF_STATE_SCHED |
			      NAPIF_STATE_SCHED_THREADED |
			      NAPIF_STATE_PREFER_BUSY_POLL);

		/* If STATE_MISSED was set, leave STATE_SCHED set,
		 * because we will call napi-&gt;poll() one more time.
		 * This C code was suggested by Alexander Duyck to help gcc.
		 */
		new |= (val &amp; NAPIF_STATE_MISSED) / NAPIF_STATE_MISSED *
						    NAPIF_STATE_SCHED;
	} while (!try_cmpxchg(&amp;n-&gt;state, &amp;val, new));

	if (unlikely(val &amp; NAPIF_STATE_MISSED)) {
		__napi_schedule(n);
		return false;
	}

	if (timeout)
		hrtimer_start(&amp;n-&gt;timer, ns_to_ktime(timeout),
			      HRTIMER_MODE_REL_PINNED);
	return ret;
}
EXPORT_SYMBOL(napi_complete_done);
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/include-net/gro_normal_list().md" data-href="Encyclopedia of NetworkSystem/Function/include-net/gro_normal_list().md" href="encyclopedia-of-networksystem/function/include-net/gro_normal_list().html" class="internal-link" target="_self" rel="noopener nofollow">gro_normal_list()</a><br>
napi가 잘 되었는지 확인하고 기타 작업을 처리하는 함수. gro_bitmask와 defer_hard_irqs_count등을 확인하여 timeout관련 처리를 하고, 만약 gro_bitmask가 존재한다면 napi_gro_flush()를 실행하게 됨. 이후 gro_normal_list(n)을 하게 됨. 만약 NAPIF_STATE_MISSED가 있다면 다시 __napi_schedule(n)을 통해 napi가 실행 될 수 있도록 함.
]]></description><link>encyclopedia-of-networksystem/function/net-core/napi_complete_done().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/napi_complete_done().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[napi_gro_complete()]]></title><description><![CDATA[ 
 <br>static void napi_gro_complete(struct napi_struct *napi, struct sk_buff *skb)
{
	struct list_head *head = &amp;net_hotdata.offload_base;
	struct packet_offload *ptype;
	__be16 type = skb-&gt;protocol;
	int err = -ENOENT;

	BUILD_BUG_ON(sizeof(struct napi_gro_cb) &gt; sizeof(skb-&gt;cb));

	if (NAPI_GRO_CB(skb)-&gt;count == 1) {
		skb_shinfo(skb)-&gt;gso_size = 0;
		goto out;
	}

	rcu_read_lock();
	list_for_each_entry_rcu(ptype, head, list) {
		if (ptype-&gt;type != type || !ptype-&gt;callbacks.gro_complete)
			continue;

		err = INDIRECT_CALL_INET(ptype-&gt;callbacks.gro_complete,
					 ipv6_gro_complete, inet_gro_complete,
					 skb, 0); // [[Encyclopedia of NetworkSystem/Function/net-ipv6/ipv6_gro_complete().md|ipv6_gro_complete()]] [[Encyclopedia of NetworkSystem/Function/net-ipv4/inet_gro_complete().md|inet_gro_complete()]]
		break;
	}
	rcu_read_unlock();

	if (err) {
		WARN_ON(&amp;ptype-&gt;list == head);
		kfree_skb(skb);
		return;
	}

out:
	gro_normal_one(napi, skb, NAPI_GRO_CB(skb)-&gt;count); // [[Encyclopedia of NetworkSystem/Function/include-net/gro_normal_one().md|gro_normal_one()]]
}

static void __napi_gro_flush_chain(struct napi_struct *napi, u32 index,
				   bool flush_old)
{
	struct list_head *head = &amp;napi-&gt;gro_hash[index].list;
	struct sk_buff *skb, *p;

	list_for_each_entry_safe_reverse(skb, p, head, list) {
		if (flush_old &amp;&amp; NAPI_GRO_CB(skb)-&gt;age == jiffies)
			return;
		skb_list_del_init(skb);
		napi_gro_complete(napi, skb);
		napi-&gt;gro_hash[index].count--;
	}

	if (!napi-&gt;gro_hash[index].count)
		__clear_bit(index, &amp;napi-&gt;gro_bitmask);
}
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-ipv6/ipv6_gro_complete().md" data-href="Encyclopedia of NetworkSystem/Function/net-ipv6/ipv6_gro_complete().md" href="encyclopedia-of-networksystem/function/net-ipv6/ipv6_gro_complete().html" class="internal-link" target="_self" rel="noopener nofollow">ipv6_gro_complete()</a>&nbsp;<br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-ipv4/inet_gro_complete().md" data-href="Encyclopedia of NetworkSystem/Function/net-ipv4/inet_gro_complete().md" href="encyclopedia-of-networksystem/function/net-ipv4/inet_gro_complete().html" class="internal-link" target="_self" rel="noopener nofollow">inet_gro_complete()</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/include-net/gro_normal_one().md" data-href="Encyclopedia of NetworkSystem/Function/include-net/gro_normal_one().md" href="encyclopedia-of-networksystem/function/include-net/gro_normal_one().html" class="internal-link" target="_self" rel="noopener nofollow">gro_normal_one()</a><br>
IP 버전에 따라 서로 다른 gro_complet를 callback할 수 있도록 하는 함수이다. 아니라면 만약 skb→count가 1이라면 해당 gro_list는 합치지 않았으므로 out 라벨로 건너뛰어 바로 gro_normal_one()함수를 호출하게 된다. 아니라면 head를 돌면서 각각의 gro_complete를 실행하게 된다. 이후 최종적으로 gro_normal_one() 함수를 실행하게 된다.
]]></description><link>encyclopedia-of-networksystem/function/net-core/napi_gro_complete().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/napi_gro_complete().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[napi_gro_receive()]]></title><description><![CDATA[ 
 <br>gro_result_t napi_gro_receive(struct napi_struct *napi, struct sk_buff *skb)
{
	gro_result_t ret; // [[Encyclopedia of NetworkSystem/Struct/include-linux/gro_result.md|gro_result_t]]

	skb_mark_napi_id(skb, napi);
	trace_napi_gro_receive_entry(skb);

	skb_gro_reset_offset(skb, 0);

	ret = napi_skb_finish(napi, skb, dev_gro_receive(napi, skb));
	trace_napi_gro_receive_exit(ret); // [[Encyclopedia of NetworkSystem/Function/net-core/napi_skb_finish().md|napi_skb_finish()]] [[Encyclopedia of NetworkSystem/Function/net-core/dev_gro_receive().md|dev_gro_receive()]]

	return ret;
}
EXPORT_SYMBOL(napi_gro_receive);
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Struct/include-linux/gro_result.md" data-href="Encyclopedia of NetworkSystem/Struct/include-linux/gro_result.md" href="encyclopedia-of-networksystem/struct/include-linux/gro_result.html" class="internal-link" target="_self" rel="noopener nofollow">gro_result_t</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-core/napi_skb_finish().md" data-href="Encyclopedia of NetworkSystem/Function/net-core/napi_skb_finish().md" href="encyclopedia-of-networksystem/function/net-core/napi_skb_finish().html" class="internal-link" target="_self" rel="noopener nofollow">napi_skb_finish()</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-core/dev_gro_receive().md" data-href="Encyclopedia of NetworkSystem/Function/net-core/dev_gro_receive().md" href="encyclopedia-of-networksystem/function/net-core/dev_gro_receive().html" class="internal-link" target="_self" rel="noopener nofollow">dev_gro_receive()</a><br>
dev_gro_receive() 함수에서 skb와 napi를 통해 실질적으로 gro를 처리하게 됨. 전후로 기타작업을 하는 함수들이 호출되고 있고, napi_skb_finish()함수를 dev_gro_receive() 함수의 return 값을 인수로 하여 호출하게 됨. 
]]></description><link>encyclopedia-of-networksystem/function/net-core/napi_gro_receive().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/napi_gro_receive().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[napi_poll()]]></title><description><![CDATA[ 
 <br>static int napi_poll(struct napi_struct *n, struct list_head *repoll)
{
	bool do_repoll = false;
	void *have;
	int work;

	list_del_init(&amp;n-&gt;poll_list); // [[Encyclopedia of NetworkSystem/Function/include-linux/list_del_init() .md|list_del_init()]]

	have = netpoll_poll_lock(n);

	work = __napi_poll(n, &amp;do_repoll); // [[Encyclopedia of NetworkSystem/Function/net-core/__napi_poll().md|__napi_poll()]]

	if (do_repoll)
		list_add_tail(&amp;n-&gt;poll_list, repoll); // [[Encyclopedia of NetworkSystem/Function/include-linux/list_add_tail().md|list_add_tail()]]

	netpoll_poll_unlock(have);

	return work;
}
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/include-linux/list_del_init() .md" data-href="Encyclopedia of NetworkSystem/Function/include-linux/list_del_init() .md" href="encyclopedia-of-networksystem/function/include-linux/list_del_init()-" class="internal-link" target="_self" rel="noopener nofollow">list_del_init()</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-core/__napi_poll().md" data-href="Encyclopedia of NetworkSystem/Function/net-core/__napi_poll().md" href="encyclopedia-of-networksystem/function/net-core/__napi_poll().html" class="internal-link" target="_self" rel="noopener nofollow">__napi_poll()</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/include-linux/list_add_tail().md" data-href="Encyclopedia of NetworkSystem/Function/include-linux/list_add_tail().md" href="encyclopedia-of-networksystem/function/include-linux/list_add_tail().html" class="internal-link" target="_self" rel="noopener nofollow">list_add_tail()</a><br>
poll_list를 지우고, netpoll_poll_lock을 걸게 된다.(멀티 코어 환경 대응) 이후 __napi_poll() 함수를 실행시켜서 폴링을 시작하게 된다. 또한 추가적으로 다시 폴링이 필요하다면 list_add_tail을 통해 repoll 변수에 저장된 리스트를 가져오게 된다.
]]></description><link>encyclopedia-of-networksystem/function/net-core/napi_poll().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/napi_poll().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[napi_schedule_prep()]]></title><description><![CDATA[ 
 <br>bool napi_schedule_prep(struct napi_struct *n)
{
	unsigned long new, val = READ_ONCE(n-&gt;state);

	do {
		if (unlikely(val &amp; NAPIF_STATE_DISABLE))
			return false;
		new = val | NAPIF_STATE_SCHED;

		/* Sets STATE_MISSED bit if STATE_SCHED was already set
		 * This was suggested by Alexander Duyck, as compiler
		 * emits better code than :
		 * if (val &amp; NAPIF_STATE_SCHED)
		 *     new |= NAPIF_STATE_MISSED;
		 */
		new |= (val &amp; NAPIF_STATE_SCHED) / NAPIF_STATE_SCHED *
						   NAPIF_STATE_MISSED;
	} while (!try_cmpxchg(&amp;n-&gt;state, &amp;val, new));

	return !(val &amp; NAPIF_STATE_SCHED);
}
EXPORT_SYMBOL(napi_schedule_prep);
<br>
napi_schedule_prep를 통해 napi 스케쥴링이 가능한지 확인
]]></description><link>encyclopedia-of-networksystem/function/net-core/napi_schedule_prep().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/napi_schedule_prep().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[napi_schedule_rps()]]></title><description><![CDATA[ 
 <br>/*
 * After we queued a packet into sd-&gt;input_pkt_queue,
 * we need to make sure this queue is serviced soon.
 *
 * - If this is another cpu queue, link it to our rps_ipi_list,
 *   and make sure we will process rps_ipi_list from net_rx_action().
 *
 * - If this is our own queue, NAPI schedule our backlog.
 *   Note that this also raises NET_RX_SOFTIRQ.
 */
static void napi_schedule_rps(struct softnet_data *sd)
{
	struct softnet_data *mysd = this_cpu_ptr(&amp;softnet_data);

#ifdef CONFIG_RPS
	if (sd != mysd) {
		sd-&gt;rps_ipi_next = mysd-&gt;rps_ipi_list;
		mysd-&gt;rps_ipi_list = sd;

		/* If not called from net_rx_action() or napi_threaded_poll()
		 * we have to raise NET_RX_SOFTIRQ.
		 */
		if (!mysd-&gt;in_net_rx_action &amp;&amp; !mysd-&gt;in_napi_threaded_poll)
			__raise_softirq_irqoff(NET_RX_SOFTIRQ);
		return;
	}
#endif /* CONFIG_RPS */
	__napi_schedule_irqoff(&amp;mysd-&gt;backlog);
}
<br><a data-tooltip-position="top" aria-label="황재훈/Research Intern/081324/net_rx_action()" data-href="황재훈/Research Intern/081324/net_rx_action()" href="황재훈/research-intern/081324/net_rx_action().html" class="internal-link" target="_self" rel="noopener nofollow">net_rx_action()</a><br>input_pkt_queue에 pkt을 넣은 후에, 이 queue가 실행되도록 설정하는 함수이다.<br>
다른 cpu를 위한 queue일 경우, rps_ipi_list (inter process interrupt list)에 넣는다.<br>
올바른 cpu 인 경우에는, napi schedule을 실행한다.<br>static void net_rps_send_ipi(struct softnet_data *remsd)
{
#ifdef CONFIG_RPS
	while (remsd) {
		struct softnet_data *next = remsd-&gt;rps_ipi_next;
		  
		if (cpu_online(remsd-&gt;cpu))
			smp_call_function_single_async(remsd-&gt;cpu, &amp;remsd-&gt;csd);
		remsd = next;
	}
#endif
}
]]></description><link>encyclopedia-of-networksystem/function/net-core/napi_schedule_rps().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/napi_schedule_rps().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[napi_skb_finish()]]></title><description><![CDATA[ 
 <br>static gro_result_t napi_skb_finish(struct napi_struct *napi,
				    struct sk_buff *skb,
				    gro_result_t ret)
{
	switch (ret) {
	case GRO_NORMAL:
		gro_normal_one(napi, skb, 1);
		break;

	case GRO_MERGED_FREE:
		if (NAPI_GRO_CB(skb)-&gt;free == NAPI_GRO_FREE_STOLEN_HEAD)
			napi_skb_free_stolen_head(skb);
		else if (skb-&gt;fclone != SKB_FCLONE_UNAVAILABLE)
			__kfree_skb(skb);
		else
			__napi_kfree_skb(skb, SKB_CONSUMED);
		break;

	case GRO_HELD:
	case GRO_MERGED:
	case GRO_CONSUMED:
		break;
	}

	return ret;
}
<br>
위의 dev_gro_receive() 함수의 return 값을 바탕으로 switch문을 통해 뒤 처리를 하는 함수. GRO_~~ enum을 사용함.
]]></description><link>encyclopedia-of-networksystem/function/net-core/napi_skb_finish().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/napi_skb_finish().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[net_dev_init()]]></title><description><![CDATA[ 
 <br>/*
 *       This is called single threaded during boot, so no need
 *       to take the rtnl semaphore.
 */
static int __init net_dev_init(void)
{
	int i, rc = -ENOMEM;

	BUG_ON(!dev_boot_phase);

	net_dev_struct_check();

	if (dev_proc_init())
		goto out;

	if (netdev_kobject_init())
		goto out;

	for (i = 0; i &lt; PTYPE_HASH_SIZE; i++)
		INIT_LIST_HEAD(&amp;ptype_base[i]);

	if (register_pernet_subsys(&amp;netdev_net_ops))
		goto out;

	/*
	 *	Initialise the packet receive queues.
	 */

	for_each_possible_cpu(i) {
		struct work_struct *flush = per_cpu_ptr(&amp;flush_works, i);
		struct softnet_data *sd = &amp;per_cpu(softnet_data, i);

		INIT_WORK(flush, flush_backlog);

		skb_queue_head_init(&amp;sd-&gt;input_pkt_queue);
		skb_queue_head_init(&amp;sd-&gt;process_queue);
#ifdef CONFIG_XFRM_OFFLOAD
		skb_queue_head_init(&amp;sd-&gt;xfrm_backlog);
#endif
		INIT_LIST_HEAD(&amp;sd-&gt;poll_list);
		sd-&gt;output_queue_tailp = &amp;sd-&gt;output_queue;
#ifdef CONFIG_RPS
		INIT_CSD(&amp;sd-&gt;csd, rps_trigger_softirq, sd);
		sd-&gt;cpu = i;
#endif
		INIT_CSD(&amp;sd-&gt;defer_csd, trigger_rx_softirq, sd);
		spin_lock_init(&amp;sd-&gt;defer_lock);

		init_gro_hash(&amp;sd-&gt;backlog);
		sd-&gt;backlog.poll = process_backlog;
		sd-&gt;backlog.weight = weight_p;

		if (net_page_pool_create(i))
			goto out;
	}

	dev_boot_phase = 0;

	/* The loopback device is special if any other network devices
	 * is present in a network namespace the loopback device must
	 * be present. Since we now dynamically allocate and free the
	 * loopback device ensure this invariant is maintained by
	 * keeping the loopback device as the first device on the
	 * list of network devices.  Ensuring the loopback devices
	 * is the first device that appears and the last network device
	 * that disappears.
	 */
	if (register_pernet_device(&amp;loopback_net_ops))
		goto out;

	if (register_pernet_device(&amp;default_device_ops))
		goto out;

	open_softirq(NET_TX_SOFTIRQ, net_tx_action);
	open_softirq(NET_RX_SOFTIRQ, net_rx_action);
	// net_tx_action과 net_rx_action을 softirq에 매핑
	
	rc = cpuhp_setup_state_nocalls(CPUHP_NET_DEV_DEAD, "net/dev:dead",
				       NULL, dev_cpu_dead);
	WARN_ON(rc &lt; 0);
	rc = 0;
out:
	if (rc &lt; 0) {
		for_each_possible_cpu(i) {
			struct page_pool *pp_ptr;

			pp_ptr = per_cpu(system_page_pool, i);
			if (!pp_ptr)
				continue;

			page_pool_destroy(pp_ptr);
			per_cpu(system_page_pool, i) = NULL;
		}
	}

	return rc;
}
<br>// kernel/softirq.c
void open_softirq(int nr, void (*action)(struct softirq_action *))
{
	softirq_vec[nr].action = action;
}

static struct softirq_action softirq_vec[NR_SOFTIRQS] __cacheline_aligned_in_smp;

// include/linux/interrupt.h
struct softirq_action
{
	void	(*action)(struct softirq_action *);
};

<br>RTNL semaphore:<br>리눅스 커널에서 라우팅 네트링크(RTNL) 인터페이스를 사용하는 동안 동기화 메커니즘으로 사용된다. 이는 여러 스레드나 프로세스가 네트워크 장치, 주소, 라우팅 테이블 등을 설정하거나 변경할 때 동시 접근을 조정하고, 데이터 일관성을 유지하기 위해 사용된다.<br>주요 특징<br>
<br>동기화: 여러 스레드가 동시에 네트워크 설정을 변경하는 것을 방지.
<br>보호: 네트워크 설정을 변경하는 동안 데이터 일관성을 유지.
<br>락(lock) 메커니즘: 세마포어를 사용하여 다른 프로세스가 네트워크 설정에 접근하는 것을 제어.
]]></description><link>encyclopedia-of-networksystem/function/net-core/net_dev_init().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/net_dev_init().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[net_rps_action_and_irq_enable()]]></title><description><![CDATA[ 
 <br>/*
 * net_rps_action_and_irq_enable sends any pending IPI's for rps.
 * Note: called with local irq disabled, but exits with local irq enabled.
 */
static void net_rps_action_and_irq_enable(struct softnet_data *sd)
{
#ifdef CONFIG_RPS
	struct softnet_data *remsd = sd-&gt;rps_ipi_list;

	if (remsd) {
		sd-&gt;rps_ipi_list = NULL;

		local_irq_enable();

		/* Send pending IPI's to kick RPS processing on remote cpus. */
		net_rps_send_ipi(remsd);
	} else
#endif
		local_irq_enable();
}
]]></description><link>encyclopedia-of-networksystem/function/net-core/net_rps_action_and_irq_enable().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/net_rps_action_and_irq_enable().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[net_rx_action()]]></title><description><![CDATA[ 
 <br>static __latent_entropy void net_rx_action(struct softirq_action *h)
{
	struct softnet_data *sd = this_cpu_ptr(&amp;softnet_data);
	unsigned long time_limit = jiffies +
		usecs_to_jiffies(READ_ONCE(net_hotdata.netdev_budget_usecs));
	int budget = READ_ONCE(net_hotdata.netdev_budget);
	LIST_HEAD(list);
	LIST_HEAD(repoll);

start:
	sd-&gt;in_net_rx_action = true;
	local_irq_disable();
	list_splice_init(&amp;sd-&gt;poll_list, &amp;list);
	local_irq_enable();

	for (;;) {
		struct napi_struct *n;

		skb_defer_free_flush(sd);

		if (list_empty(&amp;list)) {
			if (list_empty(&amp;repoll)) {
				sd-&gt;in_net_rx_action = false;
				barrier();
				/* We need to check if ____napi_schedule()
				 * had refilled poll_list while
				 * sd-&gt;in_net_rx_action was true.
				 */
				if (!list_empty(&amp;sd-&gt;poll_list))
					goto start;
				if (!sd_has_rps_ipi_waiting(sd))
					goto end;
			}
			break;
		}

		n = list_first_entry(&amp;list, struct napi_struct, poll_list);
		budget -= napi_poll(n, &amp;repoll); //napi_poll이 이루어지는 부분
		// [[Encyclopedia of NetworkSystem/Function/net-core/napi_poll().md|napi_poll()]]

		/* If softirq window is exhausted then punt.
		 * Allow this to run for 2 jiffies since which will allow
		 * an average latency of 1.5/HZ.
		 */
		if (unlikely(budget &lt;= 0 ||
			     time_after_eq(jiffies, time_limit))) {
			sd-&gt;time_squeeze++;
			break;
		}
	}

	local_irq_disable();

	list_splice_tail_init(&amp;sd-&gt;poll_list, &amp;list);
	list_splice_tail(&amp;repoll, &amp;list);
	list_splice(&amp;list, &amp;sd-&gt;poll_list);
	if (!list_empty(&amp;sd-&gt;poll_list))
		__raise_softirq_irqoff(NET_RX_SOFTIRQ);
	else
		sd-&gt;in_net_rx_action = false;

	net_rps_action_and_irq_enable(sd); // [[Encyclopedia of NetworkSystem/Function/net-core/net_rps_action_and_irq_enable().md|net_rps_action_and_irq_enable()]]
end:;
}

struct netdev_adjacent {
	struct net_device *dev;
	netdevice_tracker dev_tracker;

	/* upper master flag, there can only be one master device per list */
	bool master;

	/* lookup ignore flag */
	bool ignore;

	/* counter for the number of times this device was added to us */
	u16 ref_nr;

	/* private field for the users */
	void *private;

	struct list_head list;
	struct rcu_head rcu;
};
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-core/napi_poll().md" data-href="Encyclopedia of NetworkSystem/Function/net-core/napi_poll().md" href="encyclopedia-of-networksystem/function/net-core/napi_poll().html" class="internal-link" target="_self" rel="noopener nofollow">napi_poll()</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-core/net_rps_action_and_irq_enable().md" data-href="Encyclopedia of NetworkSystem/Function/net-core/net_rps_action_and_irq_enable().md" href="encyclopedia-of-networksystem/function/net-core/net_rps_action_and_irq_enable().html" class="internal-link" target="_self" rel="noopener nofollow">net_rps_action_and_irq_enable()</a><br>
jiffies를 통해 time out을 구현하였고, softnet_data 구조체에서 poll_list를 가져왔다. 이후 이러한 리스트를 바탕으로 napi_poll() 함수를 호출하여 폴링을 시작하게 된다. napi_poll 함수는 네트워크 장치의 수신 큐에서 패킷을 처리하고, 이를 위해 할당된 budget을 감소시키는 역할을 한다.
<br>--8.22 추가--<br>
napi_poll함수를 실행할 때, napi_struct를 n이라는 parameter로 넘기게 되는데, 따라서 for문을 돌 때마다 한 개씩  napi_struct가 처리되고 있음을 볼 수 있다.<br>
본 함수에서는 계속 poll_list의 첫 번째 entry만 가져오는 것 같지만 napi_poll함수 내부에서 해당 napi_struct를 poll_list에서 제거하므로, 전체 리스트를 적절하게 처리하고 있다고 볼 수 있다.
즉 budget은 하나의 CPU에서 여러개의 napi_struct를 처리하는 전체 처리양의 예산인 것이다.
<br>
이때 softnet_data의 poll_list는 ____napi_schedule()함수에서 해당 napi_struct가 추가 되는 것을 확인 할 수 있었다. 이렇게 여러 개의 ice_q_vector가 하나의 softnet_data-&gt;poll_list에 매핑되어, 그 CPU에서 softIRQ가 처리되게 되는 것이다.
이후 net_rx_action() 함수가 실행 중인지 확인하여 실행 중이라면 그냥 sd-&gt;poll_list에 추가함으로써 해당 net_rx_action()에서 처리할 수 있도록하고, 만약 꺼져 있다면 새로이 켜게 되는 것이다.
그렇다는 말은, sd-&gt;backlog도 sd-&gt;poll_list에 들어가게 된다는 의미이다. 그러나 napi_struct의 poll함수 포인터가 서로 다른 것을 가르키고 있을 것이다.

ring descriptor가 속해있는 ice_q_vector의 napi_struct인 경우 -&gt;ice_napi_poll<br>
softnet_data에 존재하는 backlog라는 이름의 napi_struct인 경우 -&gt;process_backlog<br>
따라서 각기 다른 함수가 폴링을 하게 될 것이다.

<br>jiffies:<br>리눅스 커널에서 시간을 측정하기 위해 사용되는 전역 변수이다. 시스템 부팅 시점부터 일정 주기마다 증가하는 카운터로, 타이머 인터럽트에 의해 업데이트된다. jiffies 값은 타임슬라이스 계산, 시간 제한 설정, 타이머 관리 등 다양한 시간 관련 작업에 사용된다.<br>주요 특징<br>
<br>단위: jiffies는 시스템 타이머 인터럽트 주기와 관련되어 있다.
<br>정확도: HZ 매크로를 통해 1초에 몇 번의 jiffies 업데이트가 발생하는지 결정한다.
<br>사용 예: 시간 계산, 타임아웃 설정 등.
]]></description><link>encyclopedia-of-networksystem/function/net-core/net_rx_action().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/net_rx_action().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[netif_napi_add_weight()]]></title><description><![CDATA[ 
 <br>void netif_napi_add_weight(struct net_device *dev, struct napi_struct *napi, int (*poll)(struct napi_struct *, int), int weight)
{
	if (WARN_ON(test_and_set_bit(NAPI_STATE_LISTED, &amp;napi-&gt;state)))
		return;

	INIT_LIST_HEAD(&amp;napi-&gt;poll_list);
	INIT_HLIST_NODE(&amp;napi-&gt;napi_hash_node);
	hrtimer_init(&amp;napi-&gt;timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL_PINNED);
	napi-&gt;timer.function = napi_watchdog;
	init_gro_hash(napi);
	napi-&gt;skb = NULL;
	INIT_LIST_HEAD(&amp;napi-&gt;rx_list);
	napi-&gt;rx_count = 0;
	napi-&gt;poll = poll;
	if (weight &gt; NAPI_POLL_WEIGHT)
		netdev_err_once(dev, "%s() called with weight %d\n", __func__,
				weight);
	napi-&gt;weight = weight;
	napi-&gt;dev = dev;
#ifdef CONFIG_NETPOLL
	napi-&gt;poll_owner = -1;
#endif
	napi-&gt;list_owner = -1;
	set_bit(NAPI_STATE_SCHED, &amp;napi-&gt;state);
	set_bit(NAPI_STATE_NPSVC, &amp;napi-&gt;state);
	list_add_rcu(&amp;napi-&gt;dev_list, &amp;dev-&gt;napi_list);
	napi_hash_add(napi);
	napi_get_frags_check(napi);
	/* Create kthread for this napi if dev-&gt;threaded is set.
	 * Clear dev-&gt;threaded if kthread creation failed so that
	 * threaded mode will not be enabled in napi_enable().
	 */
	if (dev-&gt;threaded &amp;&amp; napi_kthread_create(napi))
		dev-&gt;threaded = 0;
	netif_napi_set_irq(napi, -1);
}
EXPORT_SYMBOL(netif_napi_add_weight);
]]></description><link>encyclopedia-of-networksystem/function/net-core/netif_napi_add_weight().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/netif_napi_add_weight().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[netif_receive_skb_internal()]]></title><description><![CDATA[ 
 <br>static int netif_receive_skb_internal(struct sk_buff *skb)
{
	int ret;

	net_timestamp_check(READ_ONCE(net_hotdata.tstamp_prequeue), skb);

	if (skb_defer_rx_timestamp(skb))
		return NET_RX_SUCCESS;

	rcu_read_lock();
#ifdef CONFIG_RPS
	if (static_branch_unlikely(&amp;rps_needed)) {
		struct rps_dev_flow voidflow, *rflow = &amp;voidflow;
		int cpu = get_rps_cpu(skb-&gt;dev, skb, &amp;rflow);

		if (cpu &gt;= 0) {
			ret = enqueue_to_backlog(skb, cpu, &amp;rflow-&gt;last_qtail);
			rcu_read_unlock();
			return ret;
		}
	}
#endif
	ret = __netif_receive_skb(skb);
	rcu_read_unlock();
	return ret;
}
<br><a data-href="Encyclopedia of NetworkSystem/Function/net-core/get_rps_cpu()" href="encyclopedia-of-networksystem/function/net-core/get_rps_cpu().html" class="internal-link" target="_self" rel="noopener nofollow">Encyclopedia of NetworkSystem/Function/net-core/get_rps_cpu()</a><br>
<a data-href="enqueue_to_backlog()" href="encyclopedia-of-networksystem/function/net-core/enqueue_to_backlog().html" class="internal-link" target="_self" rel="noopener nofollow">enqueue_to_backlog()</a><br>
<a data-href="__netif_receive_skb()" href="__netif_receive_skb().html" class="internal-link" target="_self" rel="noopener nofollow">__netif_receive_skb()</a><br>rps를 사용하는 경우, skb를 보내야하는 cpu를 찾고 존재할 경우, enqueue_to_backlog() 함수를 실행<br>
사용하지 않는 경우에는 __netif_receive_skb() 실행한다.]]></description><link>encyclopedia-of-networksystem/function/net-core/netif_receive_skb_internal().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/netif_receive_skb_internal().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[netif_receive_skb_list_internal()]]></title><description><![CDATA[ 
 <br>void netif_receive_skb_list_internal(struct list_head *head)
{
	struct sk_buff *skb, *next;
	struct list_head sublist;

	INIT_LIST_HEAD(&amp;sublist);
	list_for_each_entry_safe(skb, next, head, list) {
		net_timestamp_check(READ_ONCE(net_hotdata.tstamp_prequeue),
				    skb);
		skb_list_del_init(skb);
		if (!skb_defer_rx_timestamp(skb))
			list_add_tail(&amp;skb-&gt;list, &amp;sublist);
	}
	list_splice_init(&amp;sublist, head);

	rcu_read_lock();
#ifdef CONFIG_RPS
	if (static_branch_unlikely(&amp;rps_needed)) {
		list_for_each_entry_safe(skb, next, head, list) {
			struct rps_dev_flow voidflow, *rflow = &amp;voidflow;
			int cpu = get_rps_cpu(skb-&gt;dev, skb, &amp;rflow);
					//[[get_rps_cpu()]]

			if (cpu &gt;= 0) {
				**/* Will be handled, remove from list */**
				skb_list_del_init(skb);
				enqueue_to_backlog(skb, cpu, &amp;rflow-&gt;last_qtail);
				// [[enqueue_to_backlog()]]
			}
		}
	}
#endif
	__netif_receive_skb_list(head); // [[Encyclopedia of NetworkSystem/Function/net-core/__netif_receive_skb_list().md|__netif_receive_skb_list()]]
	rcu_read_unlock();
}
<br><a data-tooltip-position="top" aria-label="__netif_receive_skb_list()" data-href="__netif_receive_skb_list()" href="encyclopedia-of-networksystem/function/net-core/__netif_receive_skb_list().html" class="internal-link" target="_self" rel="noopener nofollow">netif_receive_skb_list()</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-core/enqueue_to_backlog()" data-href="Encyclopedia of NetworkSystem/Function/net-core/enqueue_to_backlog()" href="encyclopedia-of-networksystem/function/net-core/enqueue_to_backlog().html" class="internal-link" target="_self" rel="noopener nofollow">enqueue_to_backlog()</a><br>
<a data-href="Encyclopedia of NetworkSystem/Function/net-core/get_rps_cpu()" href="encyclopedia-of-networksystem/function/net-core/get_rps_cpu().html" class="internal-link" target="_self" rel="noopener nofollow">Encyclopedia of NetworkSystem/Function/net-core/get_rps_cpu()</a><br>
timestamp를 확인하고 만약 rps가 설정되어 있을 경우 enqueue_to_backlog()를 통해 특정 cpu에 해당 flow를 할당하게 된다. 아니라면 __netif_receive_skb_list(head)를 호출하여 처리를 이어나가게 된다.
<br>
새로운 서브리스트를 생성하고, 주어진 napi-&gt;rx_list를 돌면서 해당 skb를 리스트에서 제거하고, sublist에 담게 된다.<br>
이 때, list_splice_init(&amp;sublsit, head)를 하게 되면, 두 리스트를 합치고, 앞의 리스트를 초기화하게 된다.<br>
만약 CONFIG_RPS이라면, 각각의 리스트에 대하여 enqueue_to_backlog()함수를 실행하게 된다.<br>
그 이후 공통적으로, __netif_receive_skb_list()함수를 실행하게 된다.
<br>
get_rps_cpu()를 통해 rps를 해서 보낼 cpu 넘버를 int 형태로 반환할 것으로 추정 된다.
]]></description><link>encyclopedia-of-networksystem/function/net-core/netif_receive_skb_list_internal().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/netif_receive_skb_list_internal().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[netif_receive_skb_list()]]></title><description><![CDATA[ 
 <br>/**
 *	netif_receive_skb_list - process many receive buffers from network
 *	@head: list of skbs to process.
 *
 *	Since return value of netif_receive_skb() is normally ignored, and
 *	wouldn't be meaningful for a list, this function returns void.
 *
 *	This function may only be called from softirq context and interrupts
 *	should be enabled.
 */
void netif_receive_skb_list(struct list_head *head)
{
	struct sk_buff *skb;

	if (list_empty(head))
		return;
	if (trace_netif_receive_skb_list_entry_enabled()) {
		list_for_each_entry(skb, head, list)
			trace_netif_receive_skb_list_entry(skb);
	}
	netif_receive_skb_list_internal(head);
	trace_netif_receive_skb_list_exit(0);
}
<br>list 가 비어있는 경우 종료<br>
trace_netif_receive_skb_list_entry_enabled() 는 trace point가 enable되어 있는지 리턴<br>
tracepoint() 들 간에 race를 막는데 필요하다.<br>
list에 있는 skb들을 순서대로  trace_netif_receive_skb_list_entry(skb) 돌린다.<br>
tracepoint을 실행한다. tracepoint에는 probe function이 연결될 수 있다(연결되어 있으면 on  상테)<br>
tracepoint이 실행될 때마다 probe 함수도 같이 실행된다. tracing performace accounting에 사용<br>
<a rel="noopener nofollow" class="external-link" href="https://www.kernel.org/doc/Documentation/trace/tracepoints.txt" target="_blank">https://www.kernel.org/doc/Documentation/trace/tracepoints.txt</a><br>
<a rel="noopener nofollow" class="external-link" href="https://blogs.oracle.com/linux/post/taming-tracepoints-in-the-linux-kernel" target="_blank">https://blogs.oracle.com/linux/post/taming-tracepoints-in-the-linux-kernel</a><br>DEFINE_EVENT(net_dev_rx_verbose_template, netif_receive_skb_list_entry,

	TP_PROTO(const struct sk_buff *skb),

	TP_ARGS(skb)
);
<br><img alt="Pasted image 20240904104737.png" src="encyclopedia-of-networksystem/function/net-core/img/pasted-image-20240904104737.png"><br>
마지막으로 해당하는 list에 대해서 netif_receive_skb_list_internal(head) 실행한다.<br><a data-href="netif_receive_skb_list_internal()" href="encyclopedia-of-networksystem/function/net-core/netif_receive_skb_list_internal().html" class="internal-link" target="_self" rel="noopener nofollow">netif_receive_skb_list_internal()</a>]]></description><link>encyclopedia-of-networksystem/function/net-core/netif_receive_skb_list().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/netif_receive_skb_list().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate><enclosure url="encyclopedia-of-networksystem/function/net-core/img/pasted-image-20240904104737.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;encyclopedia-of-networksystem/function/net-core/img/pasted-image-20240904104737.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[netif_receive_skb()]]></title><description><![CDATA[ 
 <br>int netif_receive_skb(struct sk_buff *skb)
{
	int ret;

	trace_netif_receive_skb_entry(skb);

	ret = netif_receive_skb_internal(skb);
	trace_netif_receive_skb_exit(ret);

	return ret;
}
EXPORT_SYMBOL(netif_receive_skb);
<br>네트워크에서 가져온 rx buffer를 처리한다<br>
buffer가 congestion control 이나 다른 layer에 의해 드랍될 수 있다.<br>
softirq context 에서만 콜 될 수 있다. interrupt이 허용된다.<br>trace_netif_receive_skb_entry, exit은 디버깅을 위한 코드이다?<br>NET_RX_SUCCESS : no congestion<br>
NET_RX_DROP : packet dropped<br><a data-href="netif_receive_skb_internal()" href="encyclopedia-of-networksystem/function/net-core/netif_receive_skb_internal().html" class="internal-link" target="_self" rel="noopener nofollow">netif_receive_skb_internal()</a>]]></description><link>encyclopedia-of-networksystem/function/net-core/netif_receive_skb().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/netif_receive_skb().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[process_backlog()]]></title><description><![CDATA[ 
 ]]></description><link>encyclopedia-of-networksystem/function/net-core/process_backlog().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/process_backlog().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[release_sock()]]></title><description><![CDATA[ 
 <br>void release_sock(struct sock *sk)
{
	spin_lock_bh(&amp;sk-&gt;sk_lock.slock);
	if (sk-&gt;sk_backlog.tail)
		__release_sock(sk);

	if (sk-&gt;sk_prot-&gt;release_cb)
		INDIRECT_CALL_INET_1(sk-&gt;sk_prot-&gt;release_cb,
				     tcp_release_cb, sk);

	sock_release_ownership(sk);
	if (waitqueue_active(&amp;sk-&gt;sk_lock.wq))
		wake_up(&amp;sk-&gt;sk_lock.wq);
	spin_unlock_bh(&amp;sk-&gt;sk_lock.slock);
}
]]></description><link>encyclopedia-of-networksystem/function/net-core/release_sock().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/release_sock().md</guid><pubDate>Tue, 08 Oct 2024 08:24:56 GMT</pubDate></item><item><title><![CDATA[set_rps_cpu()]]></title><description><![CDATA[ 
 <br>static struct rps_dev_flow *
set_rps_cpu(struct net_device *dev, struct sk_buff *skb,
struct rps_dev_flow *rflow, u16 next_cpu)
{
	if (next_cpu &lt; nr_cpu_ids) {
#ifdef CONFIG_RFS_ACCEL
		struct netdev_rx_queue *rxqueue;
		struct rps_dev_flow_table *flow_table;
		struct rps_dev_flow *old_rflow;
		u32 flow_id;
		u16 rxq_index;
		int rc;
		  
		/* Should we steer this flow to a different hardware queue? */
		if (!skb_rx_queue_recorded(skb) || !dev-&gt;rx_cpu_rmap ||
			!(dev-&gt;features &amp; NETIF_F_NTUPLE))
			goto out;
		rxq_index = cpu_rmap_lookup_index(dev-&gt;rx_cpu_rmap, next_cpu);
		if (rxq_index == skb_get_rx_queue(skb))
			goto out;
		  
		rxqueue = dev-&gt;_rx + rxq_index;
		flow_table = rcu_dereference(rxqueue-&gt;rps_flow_table);
		if (!flow_table)
			goto out;
		flow_id = skb_get_hash(skb) &amp; flow_table-&gt;mask;
		rc = dev-&gt;netdev_ops-&gt;ndo_rx_flow_steer(dev, skb,
							rxq_index, flow_id);
		if (rc &lt; 0)
			goto out;
		old_rflow = rflow;
		rflow = &amp;flow_table-&gt;flows[flow_id];
		rflow-&gt;filter = rc;
		if (old_rflow-&gt;filter == rflow-&gt;filter)
			old_rflow-&gt;filter = RPS_NO_FILTER;
		out:
#endif
		rflow-&gt;last_qtail =
			per_cpu(softnet_data, next_cpu).input_queue_head;
	}
  
	rflow-&gt;cpu = next_cpu;
	return rflow;
}
<br>
<img alt="Pasted image 20240816113600.png" src="lib/media/pasted-image-20240816113600.png">
위의 참고자료의 한 문단이다.<br>
rflow는 해당 receive queue가 처리되고 있는 CPU를 가르키고 있는 것이다.<br>
위를 보면 알 수 있듯이, aRFS가 켜져있을 때 작동하는 코드들이 있다.<br>
그게 아니라면 rflow-&gt;last_qtail을 설정해주고, rflow-&gt;cpu들어온 next_cpu로 설정하게 된다.
]]></description><link>encyclopedia-of-networksystem/function/net-core/set_rps_cpu().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/set_rps_cpu().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate><enclosure url="lib/media/pasted-image-20240816113600.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240816113600.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[sk_wait_data()]]></title><description><![CDATA[ 
 <br>/**
 * sk_wait_data - wait for data to arrive at sk_receive_queue
 * @sk:    sock to wait on
 * @timeo: for how long
 * @skb:   last skb seen on sk_receive_queue
 *
 * Now socket state including sk-&gt;sk_err is changed only under lock,
 * hence we may omit checks after joining wait queue.
 * We check receive queue before schedule() only as optimization;
 * it is very likely that release_sock() added new data.
 */
int sk_wait_data(struct sock *sk, long *timeo, const struct sk_buff *skb)
{
	DEFINE_WAIT_FUNC(wait, woken_wake_function);
	int rc;

	add_wait_queue(sk_sleep(sk), &amp;wait);
	sk_set_bit(SOCKWQ_ASYNC_WAITDATA, sk);
	rc = sk_wait_event(sk, timeo, skb_peek_tail(&amp;sk-&gt;sk_receive_queue) != skb, &amp;wait);
	sk_clear_bit(SOCKWQ_ASYNC_WAITDATA, sk);
	remove_wait_queue(sk_sleep(sk), &amp;wait);
	return rc;
}```
]]></description><link>encyclopedia-of-networksystem/function/net-core/sk_wait_data().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/sk_wait_data().md</guid><pubDate>Tue, 08 Oct 2024 08:24:56 GMT</pubDate></item><item><title><![CDATA[skb_copy_datagram_iter()]]></title><description><![CDATA[ 
 <br>/**
 *	skb_copy_datagram_iter - Copy a datagram to an iovec iterator.
 *	@skb: buffer to copy
 *	@offset: offset in the buffer to start copying from
 *	@to: iovec iterator to copy to
 *	@len: amount of data to copy from buffer to iovec
 */
int skb_copy_datagram_iter(const struct sk_buff *skb, int offset,
			   struct iov_iter *to, int len)
{
	trace_skb_copy_datagram_iovec(skb, len);
	return __skb_datagram_iter(skb, offset, to, len, false,
			simple_copy_to_iter, NULL);
}
<br><a data-href="__skb_datagram_iter()" href="encyclopedia-of-networksystem/function/net-core/__skb_datagram_iter().html" class="internal-link" target="_self" rel="noopener nofollow">__skb_datagram_iter()</a>]]></description><link>encyclopedia-of-networksystem/function/net-core/skb_copy_datagram_iter().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/skb_copy_datagram_iter().md</guid><pubDate>Tue, 08 Oct 2024 08:24:56 GMT</pubDate></item><item><title><![CDATA[skb_gro_receive()]]></title><description><![CDATA[ 
 <br>int skb_gro_receive(struct sk_buff *p, struct sk_buff *skb)
{
	struct skb_shared_info *pinfo, *skbinfo = skb_shinfo(skb);
	unsigned int offset = skb_gro_offset(skb);
	unsigned int headlen = skb_headlen(skb);
	unsigned int len = skb_gro_len(skb);
	unsigned int delta_truesize;
	unsigned int gro_max_size;
	unsigned int new_truesize;
	struct sk_buff *lp;
	int segs;

	/* Do not splice page pool based packets w/ non-page pool
	 * packets. This can result in reference count issues as page
	 * pool pages will not decrement the reference count and will
	 * instead be immediately returned to the pool or have frag
	 * count decremented.
	 */
	if (p-&gt;pp_recycle != skb-&gt;pp_recycle)
		return -ETOOMANYREFS;

	/* pairs with WRITE_ONCE() in netif_set_gro(_ipv4)_max_size() */
	gro_max_size = p-&gt;protocol == htons(ETH_P_IPV6) ?
			READ_ONCE(p-&gt;dev-&gt;gro_max_size) :
			READ_ONCE(p-&gt;dev-&gt;gro_ipv4_max_size);

	if (unlikely(p-&gt;len + len &gt;= gro_max_size || NAPI_GRO_CB(skb)-&gt;flush))
		return -E2BIG;

	if (unlikely(p-&gt;len + len &gt;= GRO_LEGACY_MAX_SIZE)) {
		if (NAPI_GRO_CB(skb)-&gt;proto != IPPROTO_TCP ||
		    (p-&gt;protocol == htons(ETH_P_IPV6) &amp;&amp;
		     skb_headroom(p) &lt; sizeof(struct hop_jumbo_hdr)) ||
		    p-&gt;encapsulation)
			return -E2BIG;
	}

	segs = NAPI_GRO_CB(skb)-&gt;count;
	lp = NAPI_GRO_CB(p)-&gt;last;
	pinfo = skb_shinfo(lp);

	if (headlen &lt;= offset) {
		skb_frag_t *frag;
		skb_frag_t *frag2;
		int i = skbinfo-&gt;nr_frags;
		int nr_frags = pinfo-&gt;nr_frags + i;

		if (nr_frags &gt; MAX_SKB_FRAGS)
			goto merge;

		offset -= headlen;
		pinfo-&gt;nr_frags = nr_frags;
		skbinfo-&gt;nr_frags = 0;

		frag = pinfo-&gt;frags + nr_frags;
		frag2 = skbinfo-&gt;frags + i;
		do {
			*--frag = *--frag2;
		} while (--i);

		skb_frag_off_add(frag, offset);
		skb_frag_size_sub(frag, offset);

		/* all fragments truesize : remove (head size + sk_buff) */
		new_truesize = SKB_TRUESIZE(skb_end_offset(skb));
		delta_truesize = skb-&gt;truesize - new_truesize;

		skb-&gt;truesize = new_truesize;
		skb-&gt;len -= skb-&gt;data_len;
		skb-&gt;data_len = 0;

		NAPI_GRO_CB(skb)-&gt;free = NAPI_GRO_FREE;
		goto done;
	} else if (skb-&gt;head_frag) {
		int nr_frags = pinfo-&gt;nr_frags;
		skb_frag_t *frag = pinfo-&gt;frags + nr_frags;
		struct page *page = virt_to_head_page(skb-&gt;head);
		unsigned int first_size = headlen - offset;
		unsigned int first_offset;

		if (nr_frags + 1 + skbinfo-&gt;nr_frags &gt; MAX_SKB_FRAGS)
			goto merge;

		first_offset = skb-&gt;data -
			       (unsigned char *)page_address(page) +
			       offset;

		pinfo-&gt;nr_frags = nr_frags + 1 + skbinfo-&gt;nr_frags;

		skb_frag_fill_page_desc(frag, page, first_offset, first_size);

		memcpy(frag + 1, skbinfo-&gt;frags, sizeof(*frag) * skbinfo-&gt;nr_frags);
		/* We dont need to clear skbinfo-&gt;nr_frags here */

		new_truesize = SKB_DATA_ALIGN(sizeof(struct sk_buff));
		delta_truesize = skb-&gt;truesize - new_truesize;
		skb-&gt;truesize = new_truesize;
		NAPI_GRO_CB(skb)-&gt;free = NAPI_GRO_FREE_STOLEN_HEAD;
		goto done;
	}

merge:
	/* sk ownership - if any - completely transferred to the aggregated packet */
	skb-&gt;destructor = NULL;
	skb-&gt;sk = NULL;
	delta_truesize = skb-&gt;truesize;
	if (offset &gt; headlen) {
		unsigned int eat = offset - headlen;

		skb_frag_off_add(&amp;skbinfo-&gt;frags[0], eat);
		skb_frag_size_sub(&amp;skbinfo-&gt;frags[0], eat);
		skb-&gt;data_len -= eat;
		skb-&gt;len -= eat;
		offset = headlen;
	}

	__skb_pull(skb, offset);

	if (NAPI_GRO_CB(p)-&gt;last == p)
		skb_shinfo(p)-&gt;frag_list = skb;
	else
		NAPI_GRO_CB(p)-&gt;last-&gt;next = skb;
	NAPI_GRO_CB(p)-&gt;last = skb;
	__skb_header_release(skb);
	lp = p;

done:
	NAPI_GRO_CB(p)-&gt;count += segs;
	p-&gt;data_len += len;
	p-&gt;truesize += delta_truesize;
	p-&gt;len += len;
	if (lp != p) {
		lp-&gt;data_len += len;
		lp-&gt;truesize += delta_truesize;
		lp-&gt;len += len;
	}
	NAPI_GRO_CB(skb)-&gt;same_flow = 1;
	return 0;
}
<br>
기존의 패킷에다가 skb를 새로 덧붙이게 된다. skb_shared_info에다가 frag를 붙이게 되고, 만약 여유공간이 없다면 새로 만들게 된다.
]]></description><link>encyclopedia-of-networksystem/function/net-core/skb_gro_receive().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-core/skb_gro_receive().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[`__inet_lookup_skb()]]></title><description><![CDATA[ 
 <br>static inline struct sock *__inet_lookup_skb(struct inet_hashinfo *hashinfo,
					     struct sk_buff *skb,
					     int doff,
					     const __be16 sport,
					     const __be16 dport,
					     const int sdif,
					     bool *refcounted)
{
	struct net *net = dev_net(skb_dst(skb)-&gt;dev);
	const struct iphdr *iph = ip_hdr(skb);
	struct sock *sk;

	sk = inet_steal_sock(net, skb, doff, iph-&gt;saddr, sport, iph-&gt;daddr, dport,
			     refcounted, inet_ehashfn);
	if (IS_ERR(sk))
		return NULL;
	if (sk)
		return sk;

	return __inet_lookup(net, hashinfo, skb,
			     doff, iph-&gt;saddr, sport,
			     iph-&gt;daddr, dport, inet_iif(skb), sdif,
			     refcounted);
}
<br>skb에 맞는 socket을 찾아주는 함수이다. <br>
<br>inet_steal_sock() 실행 후, sk가 존재한다면 리턴
<br>steal 에 실패한다면 __inet_lookup() 실행해서 리턴
<br><a data-href="\_\_inet_lookup()" href="/_/_inet_lookup()" class="internal-link" target="_self" rel="noopener nofollow">\_\_inet_lookup()</a>]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/`__inet_lookup_skb().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/`__inet_lookup_skb().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[`_inet_lookup_established()]]></title><description><![CDATA[ 
 <br>struct sock *__inet_lookup_established(struct net *net,
				  struct inet_hashinfo *hashinfo,
				  const __be32 saddr, const __be16 sport,
				  const __be32 daddr, const u16 hnum,
				  const int dif, const int sdif)
{
	INET_ADDR_COOKIE(acookie, saddr, daddr);
	const __portpair ports = INET_COMBINED_PORTS(sport, hnum);
	struct sock *sk;
	const struct hlist_nulls_node *node;
	/* Optimize here for direct hit, only listening connections can
	 * have wildcards anyways.
	 */
	unsigned int hash = inet_ehashfn(net, daddr, hnum, saddr, sport);
	unsigned int slot = hash &amp; hashinfo-&gt;ehash_mask;
	struct inet_ehash_bucket *head = &amp;hashinfo-&gt;ehash[slot];

begin:
	sk_nulls_for_each_rcu(sk, node, &amp;head-&gt;chain) {
		if (sk-&gt;sk_hash != hash)
			continue;
		if (likely(inet_match(net, sk, acookie, ports, dif, sdif))) {
			if (unlikely(!refcount_inc_not_zero(&amp;sk-&gt;sk_refcnt)))
				goto out;
			if (unlikely(!inet_match(net, sk, acookie,
						 ports, dif, sdif))) {
				sock_gen_put(sk);
				goto begin;
			}
			goto found;
		}
	}
	/*
	 * if the nulls value we got at the end of this lookup is
	 * not the expected one, we must restart lookup.
	 * We probably met an item that was moved to another chain.
	 */
	if (get_nulls_value(node) != slot)
		goto begin;
out:
	sk = NULL;
found:
	return sk;
}
]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/`_inet_lookup_established().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/`_inet_lookup_established().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[fib_table_lookup()]]></title><description><![CDATA[ 
 <br>/* should be called with rcu_read_lock */
int fib_table_lookup(struct fib_table *tb, const struct flowi4 *flp,
             struct fib_result *res, int fib_flags)
{
    struct trie *t = (struct trie *) tb-&gt;tb_data;
#ifdef CONFIG_IP_FIB_TRIE_STATS
    struct trie_use_stats __percpu *stats = t-&gt;stats;
#endif
    const t_key key = ntohl(flp-&gt;daddr);
    struct key_vector *n, *pn;
    struct fib_alias *fa;
    unsigned long index;
    t_key cindex;
  
    pn = t-&gt;kv;
    cindex = 0;
  
    n = get_child_rcu(pn, cindex);
    if (!n) {
        trace_fib_table_lookup(tb-&gt;tb_id, flp, NULL, -EAGAIN);
        return -EAGAIN;
    }
  
#ifdef CONFIG_IP_FIB_TRIE_STATS
    this_cpu_inc(stats-&gt;gets);
#endif
  
    /* Step 1: Travel to the longest prefix match in the trie */
    for (;;) {
        index = get_cindex(key, n);
  
        /* This bit of code is a bit tricky but it combines multiple
         * checks into a single check.  The prefix consists of the
         * prefix plus zeros for the "bits" in the prefix. The index
         * is the difference between the key and this value.  From
         * this we can actually derive several pieces of data.
         *   if (index &gt;= (1ul &lt;&lt; bits))
         *     we have a mismatch in skip bits and failed
         *   else
         *     we know the value is cindex
         *
         * This check is safe even if bits == KEYLENGTH due to the
         * fact that we can only allocate a node with 32 bits if a
         * long is greater than 32 bits.
         */
        if (index &gt;= (1ul &lt;&lt; n-&gt;bits))
            break;
  
        /* we have found a leaf. Prefixes have already been compared */
        if (IS_LEAF(n))
            goto found;
  
        /* only record pn and cindex if we are going to be chopping
         * bits later.  Otherwise we are just wasting cycles.
         */
        if (n-&gt;slen &gt; n-&gt;pos) {
            pn = n;
            cindex = index;
        }
  
        n = get_child_rcu(n, index);
        if (unlikely(!n))
            goto backtrace;
    }
  
    /* Step 2: Sort out leaves and begin backtracing for longest prefix */
    for (;;) {
        /* record the pointer where our next node pointer is stored */
        struct key_vector __rcu **cptr = n-&gt;tnode;
  
        /* This test verifies that none of the bits that differ
         * between the key and the prefix exist in the region of
         * the lsb and higher in the prefix.
         */
        if (unlikely(prefix_mismatch(key, n)) || (n-&gt;slen == n-&gt;pos))
            goto backtrace;
  
        /* exit out and process leaf */
        if (unlikely(IS_LEAF(n)))
            break;
  
        /* Don't bother recording parent info.  Since we are in
         * prefix match mode we will have to come back to wherever
         * we started this traversal anyway
         */
  
        while ((n = rcu_dereference(*cptr)) == NULL) {
backtrace:
#ifdef CONFIG_IP_FIB_TRIE_STATS
            if (!n)
                this_cpu_inc(stats-&gt;null_node_hit);
#endif
            /* If we are at cindex 0 there are no more bits for
             * us to strip at this level so we must ascend back
             * up one level to see if there are any more bits to
             * be stripped there.
             */
            while (!cindex) {
                t_key pkey = pn-&gt;key;
  
                /* If we don't have a parent then there is
                 * nothing for us to do as we do not have any
                 * further nodes to parse.
                 */
                if (IS_TRIE(pn)) {
                    trace_fib_table_lookup(tb-&gt;tb_id, flp,
                                   NULL, -EAGAIN);
                    return -EAGAIN;
                }
#ifdef CONFIG_IP_FIB_TRIE_STATS
                this_cpu_inc(stats-&gt;backtrack);
#endif
                /* Get Child's index */
                pn = node_parent_rcu(pn);
                cindex = get_index(pkey, pn);
            }
  
            /* strip the least significant bit from the cindex */
            cindex &amp;= cindex - 1;
  
            /* grab pointer for next child node */
            cptr = &amp;pn-&gt;tnode[cindex];
        }
    }
  
found:
    /* this line carries forward the xor from earlier in the function */
    index = key ^ n-&gt;key;
  
    /* Step 3: Process the leaf, if that fails fall back to backtracing */
    hlist_for_each_entry_rcu(fa, &amp;n-&gt;leaf, fa_list) {
        struct fib_info *fi = fa-&gt;fa_info;
        struct fib_nh_common *nhc;
        int nhsel, err;
  
        if ((BITS_PER_LONG &gt; KEYLENGTH) || (fa-&gt;fa_slen &lt; KEYLENGTH)) {
            if (index &gt;= (1ul &lt;&lt; fa-&gt;fa_slen))
                continue;
        }
        if (fa-&gt;fa_dscp &amp;&amp;
            inet_dscp_to_dsfield(fa-&gt;fa_dscp) != flp-&gt;flowi4_tos)
            continue;
        /* Paired with WRITE_ONCE() in fib_release_info() */
        if (READ_ONCE(fi-&gt;fib_dead))
            continue;
        if (fa-&gt;fa_info-&gt;fib_scope &lt; flp-&gt;flowi4_scope)
            continue;
        fib_alias_accessed(fa);
        err = fib_props[fa-&gt;fa_type].error;
        if (unlikely(err &lt; 0)) {
out_reject:
#ifdef CONFIG_IP_FIB_TRIE_STATS
            this_cpu_inc(stats-&gt;semantic_match_passed);
#endif
            trace_fib_table_lookup(tb-&gt;tb_id, flp, NULL, err);
            return err;
        }
        if (fi-&gt;fib_flags &amp; RTNH_F_DEAD)
            continue;
  
        if (unlikely(fi-&gt;nh)) {
            if (nexthop_is_blackhole(fi-&gt;nh)) {
                err = fib_props[RTN_BLACKHOLE].error;
                goto out_reject;
            }
  
            nhc = nexthop_get_nhc_lookup(fi-&gt;nh, fib_flags, flp,
                             &amp;nhsel);
            if (nhc)
                goto set_result;
            goto miss;
        }
  
        for (nhsel = 0; nhsel &lt; fib_info_num_path(fi); nhsel++) {
            nhc = fib_info_nhc(fi, nhsel);
  
            if (!fib_lookup_good_nhc(nhc, fib_flags, flp))
                continue;
set_result:
            if (!(fib_flags &amp; FIB_LOOKUP_NOREF))
                refcount_inc(&amp;fi-&gt;fib_clntref);
  
            res-&gt;prefix = htonl(n-&gt;key);
            res-&gt;prefixlen = KEYLENGTH - fa-&gt;fa_slen;
            res-&gt;nh_sel = nhsel;
            res-&gt;nhc = nhc;
            res-&gt;type = fa-&gt;fa_type;
            res-&gt;scope = fi-&gt;fib_scope;
            res-&gt;fi = fi;
            res-&gt;table = tb;
            res-&gt;fa_head = &amp;n-&gt;leaf;
#ifdef CONFIG_IP_FIB_TRIE_STATS
            this_cpu_inc(stats-&gt;semantic_match_passed);
#endif
            trace_fib_table_lookup(tb-&gt;tb_id, flp, nhc, err);
  
            return err;
        }
    }
miss:
#ifdef CONFIG_IP_FIB_TRIE_STATS
    this_cpu_inc(stats-&gt;semantic_match_miss);
#endif
    goto backtrace;
}
<br>
여러가지 새로 보이는 구조체가 많아서 나중에 한번 정리해야 할 것이다.
<br>
trie자료구조를 통해 라우팅 테이블을 저장하고, 관리하고, 탐색하고있었다. 이 자료구조는 문자열을 저장하고 효율적으로 탐색하기 위해 고안된 자료구조로, 따로 공부해보아야 할 것이다.<br>
해당 구조에 대한 도식은 sock 노트에 있다.
우선 key는 최종 목적지인 flp-&gt;daddr이다. key_vector타입으로는 *n과 *pn이 있는데, 이는 각각 자식노드와 부모노드를 가르키기 위한 포인터이다.<br>
가장 먼저 for문을 들어가기 전에 n = get_child_rcu(pn, cindex)를 통해 첫번째 자식 노드를 가져오게 된다. 왜냐면 위에서 cindex = 0으로 초기화 하였기 때문이다.
<br>/* To understand this stuff, an understanding of keys and all their bits is
 * necessary. Every node in the trie has a key associated with it, but not
 * all of the bits in that key are significant.
 *
 * Consider a node 'n' and its parent 'tp'.
 *
 * If n is a leaf, every bit in its key is significant. Its presence is
 * necessitated by path compression, since during a tree traversal (when
 * searching for a leaf - unless we are doing an insertion) we will completely
 * ignore all skipped bits we encounter. Thus we need to verify, at the end of
 * a potentially successful search, that we have indeed been walking the
 * correct key path.
 *
 * Note that we can never "miss" the correct key in the tree if present by
 * following the wrong path. Path compression ensures that segments of the key
 * that are the same for all keys with a given prefix are skipped, but the
 * skipped part *is* identical for each node in the subtrie below the skipped
 * bit! trie_insert() in this implementation takes care of that.
 *
 * if n is an internal node - a 'tnode' here, the various parts of its key
 * have many different meanings.
 *
 * Example:
 * _________________________________________________________________
 * | i | i | i | i | i | i | i | N | N | N | S | S | S | S | S | C |
 * -----------------------------------------------------------------
 *  31  30  29  28  27  26  25  24  23  22  21  20  19  18  17  16
 *
 * _________________________________________________________________
 * | C | C | C | u | u | u | u | u | u | u | u | u | u | u | u | u |
 * -----------------------------------------------------------------
 *  15  14  13  12  11  10   9   8   7   6   5   4   3   2   1   0
 *
 * tp-&gt;pos = 22
 * tp-&gt;bits = 3
 * n-&gt;pos = 13
 * n-&gt;bits = 4
 *
 * First, let's just ignore the bits that come before the parent tp, that is
 * the bits from (tp-&gt;pos + tp-&gt;bits) to 31. They are *known* but at this
 * point we do not use them for anything.
 *
 * The bits from (tp-&gt;pos) to (tp-&gt;pos + tp-&gt;bits - 1) - "N", above - are the
 * index into the parent's child array. That is, they will be used to find
 * 'n' among tp's children.
 *
 * The bits from (n-&gt;pos + n-&gt;bits) to (tp-&gt;pos - 1) - "S" - are skipped bits
 * for the node n.
 *
 * All the bits we have seen so far are significant to the node n. The rest
 * of the bits are really not needed or indeed known in n-&gt;key
 *
 * The bits from (n-&gt;pos) to (n-&gt;pos + n-&gt;bits - 1) - "C" - are the index into
 * n's child array, and will of course be different for each child.
 *
 * The rest of the bits, from 0 to (n-&gt;pos -1) - "u" - are completely unknown
 * at this point.
 */
<br><a rel="noopener nofollow" class="external-link" href="https://vincent.bernat.ch/en/blog/2017-ipv4-route-lookup-linux" target="_blank">https://vincent.bernat.ch/en/blog/2017-ipv4-route-lookup-linux</a><br>
fib path compressed trie - routing search와 관련하여 아주 명확하게 설명이 들어있는 참고 자료이다.<br><br><br>--요약--<br>
보통의 trie 자료구조 같다면 각 자리마다 노드로 나뉘게 될 것이다. 그러나 라우팅 테이블의 경우 각 자리 비트를 기준으로 해야할 것이기 때문에 이는 상당한 메모리 낭비와 많은 노드를 사용하게 한다. 거기에다가 필요없는 비트 비교연산이 많이 있을 것이다.<br>
따라서 path compression을 사용하게 된다. 이는 각 노드마다 하나의 child를 가져야 함이 삭제되었다. (라우팅 엔트리가 있는 경우는 제외하고) 그리고 각 노드들은 이제 얼마나 많은 비트를 스킵하였는지 알려주는 새로운 변수를 가지게 된다. 이러한 trie 자료구조는 Patricia trie 혹은 radix tree로 알려져 있다.<br>
<img alt="Pasted image 20240904095723 1.png" src="lib/media/pasted-image-20240904095723-1.png"><br>
또한, level compression 또한 사용하고 있는데,  trie 구조에서 노드가 많은 부분을 찾아내어 그 child 노드를 0과 1로 나뉘는게 아니라 다루는 비트를 2개 이상으로 함으로써, n개의 비트를 다룰 때, 2^n 개의 자식 노드들이 존재하도록 하는 방법이다.<br>
<img alt="Pasted image 20240904100752 1.png" src="lib/media/pasted-image-20240904100752-1.png"><br>
이러한 trie를 LC-trie 혹은 LPC-trie라고 부르고, radix tree 보다 더 높은 탐색 성능을 제공한다.
휴리스틱한 부분은 해당 노드가 얼마나 많은 비트를 다루어야 할지 결정하는 것이다. 리눅스에서는 비어있지 않은 자식 노드가 전체 자식 노드의 50%보다 많을 경우 해당 노드가 추가적인 비트를 다루게 된다. 반면 그 비율이 25% 이하라면 해당 노드는 본인이 책임지는 비트에 대한 책임을 잃게 된다. 이러한 값은 튜닝할 수 없다.(아마 유저 레벨에서 config하는 것을 말하는 것으로 보인다.)
<img alt="Pasted image 20240904101222 1.png" src="lib/media/pasted-image-20240904101222-1.png"><br>
리눅스에서는 위와 같이 구현되고 있다.<br>
<a rel="noopener nofollow" class="external-link" href="https://vincent.bernat.ch/en/blog/2017-ipv4-route-lookup-linux" target="_blank">https://vincent.bernat.ch/en/blog/2017-ipv4-route-lookup-linux</a><br>
위의 링크는 리눅스 커널 버전 별로 fib_lookup() 함수의 성능 벤치마크를 정리한 사이트이다.
<a rel="noopener nofollow" class="external-link" href="https://docs.kernel.org/networking/fib_trie.html" target="_blank">https://docs.kernel.org/networking/fib_trie.html</a><br>
<a rel="noopener nofollow" class="external-link" href="https://nscpolteksby.ac.id/ebook/files/Ebook/Computer%20Engineering/Linux%20Kernel%20Networking%20-%20Implementation%20(2014)/chapter%205%20The%20IPv4%20Routing%20Subsystem.pdf" target="_blank">https://nscpolteksby.ac.id/ebook/files/Ebook/Computer%20Engineering/Linux%20Kernel%20Networking%20-%20Implementation%20(2014)/chapter%205%20The%20IPv4%20Routing%20Subsystem.pdf</a><br>
추가 참고자료 사이트
<br><br>
이어서 함수를 보자면, get_cindex(key, n)을 호출하고 있다. 이 함수는 #define으로 /net/ipv4/fib_trie.c에 정의되어 있다. 단순하게 ((key)^(n)-&gt;key) &gt;&gt; (n)-&gt;pos를 하게 된다. 이는 현재 라우팅을 해야 할 key값과 본인 노드의 키 값인 n-&gt;key를 우선 XOR 연산을 하게 되고, 이후 pos만큼 left shift 하게 된다. 그렇다면 상위 비트에 남은 값은 스킵한 비트들과 자식 노드들의 array에 대한 index 값을 가지는 비트들이 남게 된다. 이 때 이 둘을 구분하는 마스크의 역할을 하는 변수는 해당 노드의 bits라는 멤버 변수이다.
그 후 만약 index &gt;= (1ul &lt;&lt; n-&gt;bits)라면 break이 걸리게 되는데, 이 때 index는 앞서 get_cindex()함수를 통해서 얻은 값이다. 만약 위의 조건이 만족한다면, 이는 스킵한 비트들 중에서 mismatch가 일어났다는 뜻이므로, 이는 해당 노드에서 탐색을 실패했다는 의미이다. 따라서 break으로 빠지게 되는 것이다. 따라서 바로 아래의 새로운 for무한 루프에 빠지게 된다.
만약 해당 노드가 LEAF라면 이 때는 goto found;를 통해 found:라벨로 향하게 된다.
만약 n-&gt;slen 이 n-&gt;pos보다 크다면, 해당 노드를 parent로써 기억하게 한다. 이렇게 하는 이유는 slen이 pos보다 크다는 것은 더 비교할 비트들이 남아있다는 뜻이기 때문이다. slen에 대하여 찾아보니 일단은 subnet mask의 길이를 저장하고 있는 것으로 보였다. 따라서 그저 key값으로 저장된 라우팅 테이블에서 어디까지가 네트워크 주소를 표현하고 있는지 알 수 있다.
그리고 해당 index를 가지고 자식 노드를 불러오고, 만약 자식 노드가 없다면 이 때는 백트랙킹을 통해 상위 부모 노드로 가게 된다.
또 다른 for 무한루프가 시작되고 있다. 여기는 위의 루프에서 앞서 스킵했던 비트 중에 일치하지 않는 부분이 있어서 시작되는 부분이다. 혹은 중간에 backtrace라는 라벨이 있는데, 이는 자식 노드가 없을 경우에 이 라벨로 오게 된다.<br>
**cptr로 n-&gt;tnode를 가져오고 있다. 이는 현재 노드의 다음 포인터가 어디에 저장되어 있는지 기록하는 포인터이다.
그 다음으로 나오는 if문은 해당 노드가 prefix mismatch가 있거나, 노드의 slen과 pos가 같다면 backtrace 라벨로 가게된다.<br>
이 테스트는 해당 노드의 그 어떠한 비트도 key와 다르지 않다는 것과, 해당key의 prefix가 그 prefix의 비트 영역에 있음을 보증한다. 만약 n-&gt;slen과 n-&gt;pos의 길이가 같다면, 해당 노드에 대해서는 더이상 조사할 자식 노드가 없을 것이다. 왜냐면 더이상 하위 라우팅 테이블이 해당 key에 대해서 의미가 없기 때문이다.
만약 해당 노드가 LEAF라면 for루프를 멈추게 된다.<br>
*cptr을 가져왔을 때 이 것이 NULL 이 아니면 이 아래의 while문을 계속하여 반복하게 되는데,<br>
우선 cindex가 0이 아닐때까지 while을 반복하게 된다. 만약 cindex가 0이라면 해당 노드에서는 더 이상 유효한 노드가 없기 때문이다.<br>
pkey값으로 부모노드의 키 값을 저장하게 되고, 만약 pn이 TRIE 노드가 아니라면 더 이상 할 수 있는 것이 없으므로 에러를 리턴하게 된다.<br>
이후 node_parent_rcu()함수를 통해 해당 노드의 부모노드를 가져오게 된다. 또한 cindex 값을 get_index()함수로 업데이트하게 된다.<br>
이에 의해 cindex값이 0이 아니게 된다면 해당 while문을 탈출하여 다음으로 넘어가게 되는데, cindex &amp;= cindex - 1을 통해 1인 비트중 LSB를 제거하게 된다. (ex. 10010100 =&gt; 10010000)<br>
이 cindex값을 바탕으로 cptr = &amp;pn-&gt;tnode[cindex]를 하여 해당 자식노드를 새로운 자식 포인터로 가져오게 된다.
이 아래로는 found:라벨이다.<br>
우선 인덱스 값을 설정하게 되는데, key ^ n-&gt;key값을 가져오게 된다.<br>
또한 hlist를 도는 for문이 나오게 된다.<br>
이 아래로는 해당 노드의 leaf를 가지고 실행되므로 우선 위의 로직을 이해하기 위해 잠시 멈추었다.
함수 이해가 안되서 Net Filter 부분부터 다시 보기로 하였다. 나중에 다시 올 것이다.
<br>
]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/fib_table_lookup().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/fib_table_lookup().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate><enclosure url="lib/media/pasted-image-20240904095723-1.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;lib/media/pasted-image-20240904095723-1.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[inet_gro_complete()]]></title><description><![CDATA[ 
 <br>int inet_gro_complete(struct sk_buff *skb, int nhoff)
{
	struct iphdr *iph = (struct iphdr *)(skb-&gt;data + nhoff);
	const struct net_offload *ops;
	__be16 totlen = iph-&gt;tot_len;
	int proto = iph-&gt;protocol;
	int err = -ENOSYS;

	if (skb-&gt;encapsulation) {
		skb_set_inner_protocol(skb, cpu_to_be16(ETH_P_IP));
		skb_set_inner_network_header(skb, nhoff);
	}

	iph_set_totlen(iph, skb-&gt;len - nhoff);
	csum_replace2(&amp;iph-&gt;check, totlen, iph-&gt;tot_len);

	ops = rcu_dereference(inet_offloads[proto]);
	if (WARN_ON(!ops || !ops-&gt;callbacks.gro_complete))
		goto out;

	/* Only need to add sizeof(*iph) to get to the next hdr below
	 * because any hdr with option will have been flushed in
	 * inet_gro_receive().
	 */
	err = INDIRECT_CALL_2(ops-&gt;callbacks.gro_complete,
			      tcp4_gro_complete, udp4_gro_complete,
			      skb, nhoff + sizeof(*iph)); // [[Encyclopedia of NetworkSystem/Function/net-ipv4/tcp4_gro_complete().md|tcp4_gro_complete()]]

out:
	return err;
}
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-ipv4/tcp4_gro_complete().md" data-href="Encyclopedia of NetworkSystem/Function/net-ipv4/tcp4_gro_complete().md" href="encyclopedia-of-networksystem/function/net-ipv4/tcp4_gro_complete().html" class="internal-link" target="_self" rel="noopener nofollow">tcp4_gro_complete()</a><br>
다시 tcp와 udp로 콜백을 하는 함수이다. 위의 inet_gro_receive()와 그 결이 비슷하다고 볼 수 있다.
]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/inet_gro_complete().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/inet_gro_complete().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[inet_gro_receive()]]></title><description><![CDATA[ 
 <br>struct sk_buff *inet_gro_receive(struct list_head *head, struct sk_buff *skb)
{
	const struct net_offload *ops;
	struct sk_buff *pp = NULL;
	const struct iphdr *iph;
	struct sk_buff *p;
	unsigned int hlen;
	unsigned int off;
	unsigned int id;
	int flush = 1;
	int proto;

	off = skb_gro_offset(skb);
	hlen = off + sizeof(*iph);
	iph = skb_gro_header(skb, hlen, off);
	if (unlikely(!iph))
		goto out;

	proto = iph-&gt;protocol;

	ops = rcu_dereference(inet_offloads[proto]);
	if (!ops || !ops-&gt;callbacks.gro_receive)
		goto out;

	if (*(u8 *)iph != 0x45)
		goto out;

	if (ip_is_fragment(iph))
		goto out;

	if (unlikely(ip_fast_csum((u8 *)iph, 5)))
		goto out;

	NAPI_GRO_CB(skb)-&gt;proto = proto;
	id = ntohl(*(__be32 *)&amp;iph-&gt;id);
	flush = (u16)((ntohl(*(__be32 *)iph) ^ skb_gro_len(skb)) | (id &amp; ~IP_DF));
	id &gt;&gt;= 16;

	list_for_each_entry(p, head, list) {
		struct iphdr *iph2;
		u16 flush_id;

		if (!NAPI_GRO_CB(p)-&gt;same_flow)
			continue;

		iph2 = (struct iphdr *)(p-&gt;data + off);
		/* The above works because, with the exception of the top
		 * (inner most) layer, we only aggregate pkts with the same
		 * hdr length so all the hdrs we'll need to verify will start
		 * at the same offset.
		 */
		if ((iph-&gt;protocol ^ iph2-&gt;protocol) |
		    ((__force u32)iph-&gt;saddr ^ (__force u32)iph2-&gt;saddr) |
		    ((__force u32)iph-&gt;daddr ^ (__force u32)iph2-&gt;daddr)) {
			NAPI_GRO_CB(p)-&gt;same_flow = 0;
			continue;
		}

		/* All fields must match except length and checksum. */
		NAPI_GRO_CB(p)-&gt;flush |=
			(iph-&gt;ttl ^ iph2-&gt;ttl) |
			(iph-&gt;tos ^ iph2-&gt;tos) |
			((iph-&gt;frag_off ^ iph2-&gt;frag_off) &amp; htons(IP_DF));

		NAPI_GRO_CB(p)-&gt;flush |= flush;

		/* We need to store of the IP ID check to be included later
		 * when we can verify that this packet does in fact belong
		 * to a given flow.
		 */
		flush_id = (u16)(id - ntohs(iph2-&gt;id));

		/* This bit of code makes it much easier for us to identify
		 * the cases where we are doing atomic vs non-atomic IP ID
		 * checks.  Specifically an atomic check can return IP ID
		 * values 0 - 0xFFFF, while a non-atomic check can only
		 * return 0 or 0xFFFF.
		 */
		if (!NAPI_GRO_CB(p)-&gt;is_atomic ||
		    !(iph-&gt;frag_off &amp; htons(IP_DF))) {
			flush_id ^= NAPI_GRO_CB(p)-&gt;count;
			flush_id = flush_id ? 0xFFFF : 0;
		}

		/* If the previous IP ID value was based on an atomic
		 * datagram we can overwrite the value and ignore it.
		 */
		if (NAPI_GRO_CB(skb)-&gt;is_atomic)
			NAPI_GRO_CB(p)-&gt;flush_id = flush_id;
		else
			NAPI_GRO_CB(p)-&gt;flush_id |= flush_id;
	}

	NAPI_GRO_CB(skb)-&gt;is_atomic = !!(iph-&gt;frag_off &amp; htons(IP_DF));
	NAPI_GRO_CB(skb)-&gt;flush |= flush;
	skb_set_network_header(skb, off);
	/* The above will be needed by the transport layer if there is one
	 * immediately following this IP hdr.
	 */
	NAPI_GRO_CB(skb)-&gt;inner_network_offset = off;

	/* Note : No need to call skb_gro_postpull_rcsum() here,
	 * as we already checked checksum over ipv4 header was 0
	 */
	skb_gro_pull(skb, sizeof(*iph));
	skb_set_transport_header(skb, skb_gro_offset(skb));

	pp = indirect_call_gro_receive(tcp4_gro_receive, udp4_gro_receive,
				       ops-&gt;callbacks.gro_receive, head, skb); // [[Encyclopedia of NetworkSystem/Function/net-ipv4/tcp4_gro_receive().md|tcp4_gro_receive()]]

out:
	skb_gro_flush_final(skb, pp, flush);

	return pp;
}
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-ipv4/tcp4_gro_receive().md" data-href="Encyclopedia of NetworkSystem/Function/net-ipv4/tcp4_gro_receive().md" href="encyclopedia-of-networksystem/function/net-ipv4/tcp4_gro_receive().html" class="internal-link" target="_self" rel="noopener nofollow">tcp4_gro_receive()</a><br>
ipv4 에서 gro receive를 처리하는 함수이다. indirect call을 통해서 호출되게 되며, 기존의 gro_list와 병합하려는 패킷이 서로 맞는지 L3에서의 필요한 처리들과 검사들을 진행하게 되며, 여기서는 출발지 주소와 목적지 주소가 같은지, ttl과 tos (time to live, type of service)등이 같은지 여부를 통해 병합해도 되는 패킷인지 검사하게 된다. 이 때 병합하려는 패킷들의 모든 리스트를 검사하게 된다. 그 다음 L3 헤더 길이 만큼 skb_gro_pull을 통해 data_offset에다가 그 길이만큼 더하여 다음 L4 헤더를 볼 수 있게 한다.
]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/inet_gro_receive().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/inet_gro_receive().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ip_defrag()]]></title><description><![CDATA[ 
 <br>/* Process an incoming IP datagram fragment. */
int ip_defrag(struct net *net, struct sk_buff *skb, u32 user)
{
	struct net_device *dev = skb-&gt;dev ? : skb_dst(skb)-&gt;dev;
	int vif = l3mdev_master_ifindex_rcu(dev);
	struct ipq *qp;

	__IP_INC_STATS(net, IPSTATS_MIB_REASMREQDS);

	/* Lookup (or create) queue header */
	qp = ip_find(net, ip_hdr(skb), user, vif);
	if (qp) {
		int ret;

		spin_lock(&amp;qp-&gt;q.lock);

		ret = ip_frag_queue(qp, skb);

		spin_unlock(&amp;qp-&gt;q.lock);
		ipq_put(qp);
		return ret;
	}

	__IP_INC_STATS(net, IPSTATS_MIB_REASMFAILS);
	kfree_skb(skb);
	return -ENOMEM;
}
<br>dev에 skb-&gt;dev을 넣는다. 만약 없으면 skb의 목적지의 dev를 넣는다.<br>
ip queue를 위한 포인터 qp를 선언한다.<br>
queue pointer (qp)에 맞는 incomplete datagram queue를 찾아준다. (ip_find())<br>
만약 없다면 새로운 queue를 생성해준다. <br>qp가 존재하는 경우, ip_frag_queue()를 실행한다.<br>
ip queue에 qp를 넣어준다. <br>ip_find()<br>
Find the correct entry in the "incomplete datagrams" queue for<br>
this IP datagram, and create new one, if nothing is found.<br>
ip_find()함수를 통해 해당 flow에 맞는 queue header를 찾게 된다. 이 큐는 fragment를 해소하기 위한 큐로, frag가 된 패킷들을 하나로 모아 합쳐서 복구하는 역할을 하게 된다.
<br><a data-href="ip_find()" href="encyclopedia-of-networksystem/function/net-ipv4/ip_find().html" class="internal-link" target="_self" rel="noopener nofollow">ip_find()</a><br>
<a data-href="ip_frag_queue() incomplete" href="encyclopedia-of-networksystem/function/net-ipv4/ip_frag_queue()-incomplete.html" class="internal-link" target="_self" rel="noopener nofollow">ip_frag_queue() incomplete</a>]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/ip_defrag().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/ip_defrag().md</guid><pubDate>Tue, 08 Oct 2024 08:24:56 GMT</pubDate></item><item><title><![CDATA[ip_find()]]></title><description><![CDATA[ 
 <br>/* Find the correct entry in the "incomplete datagrams" queue for
 * this IP datagram, and create new one, if nothing is found.
 */
static struct ipq *ip_find(struct net *net, struct iphdr *iph,
			   u32 user, int vif)
{
	struct frag_v4_compare_key key = {
		.saddr = iph-&gt;saddr,
		.daddr = iph-&gt;daddr,
		.user = user,
		.vif = vif,
		.id = iph-&gt;id,
		.protocol = iph-&gt;protocol,
	};
	struct inet_frag_queue *q;

	q = inet_frag_find(net-&gt;ipv4.fqdir, &amp;key);
	if (!q)
		return NULL;

	return container_of(q, struct ipq, q);
}
<br>
inet_frag_find()함수를 통해 해당하는 inet_frag_queue타입의 구조체 변수를 가져오고, 이를 container_of()함수를 통해 이를 가지고 있는 ipq 구조체를 리턴하게 된다. 
이 때 inet_frag_find()함수를 간단하게 설명하자면, rhashtable_lookup()함수를 호출하여 유효한 inet_frag_queue를 가져오게 된다. 이는 hash_table을 뒤지게 되고, 이 때 net-&gt;ipv4의 fqdir이라는 멤버변수를 가져온다. 이는 frag queue directory로, frag queue를 관리하는 구조체이다. 이 구조체 안에는 rhashtable이라는 멤버변수를 가지고 있는데, 이를 바탕으로 lookup이 진행되는 것이다.
]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/ip_find().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/ip_find().md</guid><pubDate>Tue, 08 Oct 2024 08:24:56 GMT</pubDate></item><item><title><![CDATA[ip_frag_queue() incomplete]]></title><description><![CDATA[ 
 <br>/* Add new segment to existing queue. */
static int ip_frag_queue(struct ipq *qp, struct sk_buff *skb)
{
	struct net *net = qp-&gt;q.fqdir-&gt;net;
	int ihl, end, flags, offset;
	struct sk_buff *prev_tail;
	struct net_device *dev;
	unsigned int fragsize;
	int err = -ENOENT;
	SKB_DR(reason);
	u8 ecn;

	/* If reassembly is already done, @skb must be a duplicate frag. */
	if (qp-&gt;q.flags &amp; INET_FRAG_COMPLETE) {
		SKB_DR_SET(reason, DUP_FRAG);
		goto err;
	}

	if (!(IPCB(skb)-&gt;flags &amp; IPSKB_FRAG_COMPLETE) &amp;&amp;
	    unlikely(ip_frag_too_far(qp)) &amp;&amp;
	    unlikely(err = ip_frag_reinit(qp))) {
		ipq_kill(qp);
		goto err;
	}

	ecn = ip4_frag_ecn(ip_hdr(skb)-&gt;tos);
	offset = ntohs(ip_hdr(skb)-&gt;frag_off);
	flags = offset &amp; ~IP_OFFSET;
	offset &amp;= IP_OFFSET;
	offset &lt;&lt;= 3;		/* offset is in 8-byte chunks */
	ihl = ip_hdrlen(skb);

	/* Determine the position of this fragment. */
	end = offset + skb-&gt;len - skb_network_offset(skb) - ihl;
	err = -EINVAL;

	/* Is this the final fragment? */
	if ((flags &amp; IP_MF) == 0) {
		/* If we already have some bits beyond end
		 * or have different end, the segment is corrupted.
		 */
		if (end &lt; qp-&gt;q.len ||
		    ((qp-&gt;q.flags &amp; INET_FRAG_LAST_IN) &amp;&amp; end != qp-&gt;q.len))
			goto discard_qp;
		qp-&gt;q.flags |= INET_FRAG_LAST_IN;
		qp-&gt;q.len = end;
	} else {
		if (end&amp;7) {
			end &amp;= ~7;
			if (skb-&gt;ip_summed != CHECKSUM_UNNECESSARY)
				skb-&gt;ip_summed = CHECKSUM_NONE;
		}
		if (end &gt; qp-&gt;q.len) {
			/* Some bits beyond end -&gt; corruption. */
			if (qp-&gt;q.flags &amp; INET_FRAG_LAST_IN)
				goto discard_qp;
			qp-&gt;q.len = end;
		}
	}
	if (end == offset)
		goto discard_qp;

	err = -ENOMEM;
	if (!pskb_pull(skb, skb_network_offset(skb) + ihl))
		goto discard_qp;

	err = pskb_trim_rcsum(skb, end - offset);
	if (err)
		goto discard_qp;

	/* Note : skb-&gt;rbnode and skb-&gt;dev share the same location. */
	dev = skb-&gt;dev;
	/* Makes sure compiler wont do silly aliasing games */
	barrier();

	prev_tail = qp-&gt;q.fragments_tail;
	err = inet_frag_queue_insert(&amp;qp-&gt;q, skb, offset, end);
	if (err)
		goto insert_error;

	if (dev)
		qp-&gt;iif = dev-&gt;ifindex;

	qp-&gt;q.stamp = skb-&gt;tstamp;
	qp-&gt;q.mono_delivery_time = skb-&gt;mono_delivery_time;
	qp-&gt;q.meat += skb-&gt;len;
	qp-&gt;ecn |= ecn;
	add_frag_mem_limit(qp-&gt;q.fqdir, skb-&gt;truesize);
	if (offset == 0)
		qp-&gt;q.flags |= INET_FRAG_FIRST_IN;

	fragsize = skb-&gt;len + ihl;

	if (fragsize &gt; qp-&gt;q.max_size)
		qp-&gt;q.max_size = fragsize;

	if (ip_hdr(skb)-&gt;frag_off &amp; htons(IP_DF) &amp;&amp;
	    fragsize &gt; qp-&gt;max_df_size)
		qp-&gt;max_df_size = fragsize;

	if (qp-&gt;q.flags == (INET_FRAG_FIRST_IN | INET_FRAG_LAST_IN) &amp;&amp;
	    qp-&gt;q.meat == qp-&gt;q.len) {
		unsigned long orefdst = skb-&gt;_skb_refdst;

		skb-&gt;_skb_refdst = 0UL;
		err = ip_frag_reasm(qp, skb, prev_tail, dev);
		skb-&gt;_skb_refdst = orefdst;
		if (err)
			inet_frag_kill(&amp;qp-&gt;q);
		return err;
	}

	skb_dst_drop(skb);
	skb_orphan(skb);
	return -EINPROGRESS;

insert_error:
	if (err == IPFRAG_DUP) {
		SKB_DR_SET(reason, DUP_FRAG);
		err = -EINVAL;
		goto err;
	}
	err = -EINVAL;
	__IP_INC_STATS(net, IPSTATS_MIB_REASM_OVERLAPS);
discard_qp:
	inet_frag_kill(&amp;qp-&gt;q);
	__IP_INC_STATS(net, IPSTATS_MIB_REASMFAILS);
err:
	kfree_skb_reason(skb, reason);
	return err;
}
]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/ip_frag_queue()-incomplete.html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/ip_frag_queue() incomplete.md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ip_local_deliver_finish()]]></title><description><![CDATA[ 
 <br>2<br>static int ip_local_deliver_finish(struct net *net, struct sock *sk, struct sk_buff *skb)
{
	skb_clear_delivery_time(skb);
	__skb_pull(skb, skb_network_header_len(skb));
	  
	rcu_read_lock();
	ip_protocol_deliver_rcu(net, skb, ip_hdr(skb)-&gt;protocol);
	rcu_read_unlock();
	  
	return 0;
}
<br>
L3 헤더를 지우고 상위 L4 스택으로 넘기고 있다.
<br><a data-href="ip_protocol_deliver_rcu()" href="encyclopedia-of-networksystem/function/net-ipv4/ip_protocol_deliver_rcu().html" class="internal-link" target="_self" rel="noopener nofollow">ip_protocol_deliver_rcu()</a>]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/ip_local_deliver_finish().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/ip_local_deliver_finish().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ip_local_deliver()]]></title><description><![CDATA[ 
 <br>int ip_local_deliver(struct sk_buff *skb)
{
	/*
	* Reassemble IP fragments.
	*/
	struct net *net = dev_net(skb-&gt;dev);
	  
	if (ip_is_fragment(ip_hdr(skb))) {
		if (ip_defrag(net, skb, IP_DEFRAG_LOCAL_DELIVER))
			return 0;
	}
	  
	return NF_HOOK(NFPROTO_IPV4, NF_INET_LOCAL_IN,
				net, NULL, skb, skb-&gt;dev, NULL,
				ip_local_deliver_finish);
}
<br>
L4에 패킷을 전달하는 함수이다. 만약 fragment 되어있는 패킷이라면 이를 재조립하는 ip_defrag()호출하고, 0을 반환한다. 이 때, frage 되어있는지 여부를 확인하는 ip_is_fragment()함수의 경우 간단하게 비트 비교를 하는 방식으로 구현되어 있다.
여기서 두 번째 NF_HOOK을 만날 수 있다. 여기서 보면, NF_INET_LOCAL_IN이라는 enum값과 함께 호출되는 것을 볼 수 있다. 앞에서는 pre routing이였지만, 여기서는 실질적으로 이 기기의 local로 들어가는 부분이다. 이후, ip_local_deliver_finish()함수를 호출하게 된다.
<br><a data-href="ip_defrag()" href="encyclopedia-of-networksystem/function/net-ipv4/ip_defrag().html" class="internal-link" target="_self" rel="noopener nofollow">ip_defrag()</a><br>
<a data-href="ip_local_deliver_finish()" href="encyclopedia-of-networksystem/function/net-ipv4/ip_local_deliver_finish().html" class="internal-link" target="_self" rel="noopener nofollow">ip_local_deliver_finish()</a>]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/ip_local_deliver().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/ip_local_deliver().md</guid><pubDate>Tue, 08 Oct 2024 08:24:56 GMT</pubDate></item><item><title><![CDATA[ip_protocol_deliver_rcu()]]></title><description><![CDATA[ 
 <br>void ip_protocol_deliver_rcu(struct net *net, struct sk_buff *skb, int protocol)
{
	const struct net_protocol *ipprot;
	int raw, ret;
	  
resubmit:
	raw = raw_local_deliver(skb, protocol);
	  
	ipprot = rcu_dereference(inet_protos[protocol]);
	if (ipprot) {
		if (!ipprot-&gt;no_policy) {
			if (!xfrm4_policy_check(NULL, XFRM_POLICY_IN, skb)) {
				kfree_skb_reason(skb,
						SKB_DROP_REASON_XFRM_POLICY);
				return;
			}
			nf_reset_ct(skb);
		}
		ret = INDIRECT_CALL_2(ipprot-&gt;handler, tcp_v4_rcv, udp_rcv,
					skb);
		if (ret &lt; 0) {
			protocol = -ret;
			goto resubmit;
		}
		__IP_INC_STATS(net, IPSTATS_MIB_INDELIVERS);
	} else {
		if (!raw) {
			if (xfrm4_policy_check(NULL, XFRM_POLICY_IN, skb)) {
			__IP_INC_STATS(net, IPSTATS_MIB_INUNKNOWNPROTOS);
			icmp_send(skb, ICMP_DEST_UNREACH,
				  ICMP_PROT_UNREACH, 0);
			}
			kfree_skb_reason(skb, SKB_DROP_REASON_IP_NOPROTO);
		} else {
			__IP_INC_STATS(net, IPSTATS_MIB_INDELIVERS);
			consume_skb(skb);
		}
	}
}
<br>
raw_local_deliver함수를 통해 raw값을 받아온다. 여기서 raw는 해당 패킷이 RAW 소켓을 사용하는지 여부가 되게 된다. 그 다음으로 우선 유효한 프로토콜인지 확인하고, no_policy여부를 확인한다. 그후 INDIRECT_CALL_2함수를 통하여 해당 ipprot의 handler를 검사하여 tcp_v4_rcv혹은 udp_rcv 함수를 호출하게 된다. 만약 ret이 음수면 해당 ret값을 protocol값으로 다시 세팅하여 함수의 처음 부분으로 돌아가게 된다.<br>
만약 유효한 프로토콜이 아니라면 raw값이 NULL인경우 알지못하는 프로토콜로 간주하고 icmp를 보내게 되고, 아니라면 그냥 skb를 없애버린다.
<br><a data-href="raw_local_deliver()" href="김기수/산협-프로젝트-2/백서-제작용/raw_local_deliver().html" class="internal-link" target="_self" rel="noopener nofollow">raw_local_deliver()</a><br>
<a data-href="Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_v4_rcv()" href="encyclopedia-of-networksystem/function/net-ipv4/tcp_v4_rcv().html" class="internal-link" target="_self" rel="noopener nofollow">Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_v4_rcv()</a>]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/ip_protocol_deliver_rcu().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/ip_protocol_deliver_rcu().md</guid><pubDate>Tue, 08 Oct 2024 08:24:56 GMT</pubDate></item><item><title><![CDATA[ip_rcv_core()]]></title><description><![CDATA[ 
 <br>/*
* Main IP Receive routine.
*/
static struct sk_buff *ip_rcv_core(struct sk_buff *skb, struct net *net)
{
	const struct iphdr *iph;
	int drop_reason;
	u32 len;
	  
	/* When the interface is in promisc. mode, drop all the crap
	* that it receives, do not try to analyse it.
	*/
	if (skb-&gt;pkt_type == PACKET_OTHERHOST) {
		dev_core_stats_rx_otherhost_dropped_inc(skb-&gt;dev);
		drop_reason = SKB_DROP_REASON_OTHERHOST;
		goto drop;
	}
	  
	__IP_UPD_PO_STATS(net, IPSTATS_MIB_IN, skb-&gt;len);
	  
	skb = skb_share_check(skb, GFP_ATOMIC);
	if (!skb) {
		__IP_INC_STATS(net, IPSTATS_MIB_INDISCARDS);
		goto out;
	}
	  
	drop_reason = SKB_DROP_REASON_NOT_SPECIFIED;
	if (!pskb_may_pull(skb, sizeof(struct iphdr)))
		goto inhdr_error;
	  
	iph = ip_hdr(skb);
	  
	/*
	* RFC1122: 3.2.1.2 MUST silently discard any IP frame that fails the checksum.
	*
	* Is the datagram acceptable?
	*
	* 1. Length at least the size of an ip header
	* 2. Version of 4
	* 3. Checksums correctly. [Speed optimisation for later, skip loopback checksums]
	* 4. Doesn't have a bogus length
	*/
	  
	if (iph-&gt;ihl &lt; 5 || iph-&gt;version != 4)
		goto inhdr_error;
	  
	BUILD_BUG_ON(IPSTATS_MIB_ECT1PKTS != IPSTATS_MIB_NOECTPKTS + INET_ECN_ECT_1);
	BUILD_BUG_ON(IPSTATS_MIB_ECT0PKTS != IPSTATS_MIB_NOECTPKTS + INET_ECN_ECT_0);
	BUILD_BUG_ON(IPSTATS_MIB_CEPKTS != IPSTATS_MIB_NOECTPKTS + INET_ECN_CE);
	__IP_ADD_STATS(net,
				IPSTATS_MIB_NOECTPKTS + (iph-&gt;tos &amp; INET_ECN_MASK),
				max_t(unsigned short, 1, skb_shinfo(skb)-&gt;gso_segs));
	  
	if (!pskb_may_pull(skb, iph-&gt;ihl*4))
		goto inhdr_error;
	  
	iph = ip_hdr(skb);
	  
	if (unlikely(ip_fast_csum((u8 *)iph, iph-&gt;ihl)))
		goto csum_error;
	  
	len = iph_totlen(skb, iph);
	if (skb-&gt;len &lt; len) {
		drop_reason = SKB_DROP_REASON_PKT_TOO_SMALL;
		__IP_INC_STATS(net, IPSTATS_MIB_INTRUNCATEDPKTS);
		goto drop;
	} else if (len &lt; (iph-&gt;ihl*4))
		goto inhdr_error;
	  
	/* Our transport medium may have padded the buffer out. Now we know it
	* is IP we can trim to the true length of the frame.
	* Note this now means skb-&gt;len holds ntohs(iph-&gt;tot_len).
	*/
	if (pskb_trim_rcsum(skb, len)) {
		__IP_INC_STATS(net, IPSTATS_MIB_INDISCARDS);
		goto drop;
	}
	  
	iph = ip_hdr(skb);
	skb-&gt;transport_header = skb-&gt;network_header + iph-&gt;ihl*4;
	  
	/* Remove any debris in the socket control block */
	memset(IPCB(skb), 0, sizeof(struct inet_skb_parm));
	IPCB(skb)-&gt;iif = skb-&gt;skb_iif;
	  
	/* Must drop socket now because of tproxy. */
	if (!skb_sk_is_prefetched(skb))
		skb_orphan(skb);
	  
	return skb;
	  
csum_error:
	drop_reason = SKB_DROP_REASON_IP_CSUM;
	__IP_INC_STATS(net, IPSTATS_MIB_CSUMERRORS);
inhdr_error:
	if (drop_reason == SKB_DROP_REASON_NOT_SPECIFIED)
	drop_reason = SKB_DROP_REASON_IP_INHDR;
	__IP_INC_STATS(net, IPSTATS_MIB_INHDRERRORS);
drop:
	kfree_skb_reason(skb, drop_reason);
out:
	return NULL;
}
<br>
만약 패킷이 다른 호스트가 목적지라면 패킷은 드랍되게 된다.<br>
여기서는 skb-&gt;pkt_type을 확인하게 되는데, define을 통해 매핑된 숫자가 해당 패킷의 타입으로 설정되어 있다.
<br>/* Packet types */

  

#define PACKET_HOST 0 /* To us */

#define PACKET_BROADCAST 1 /* To all */

#define PACKET_MULTICAST 2 /* To group */

#define PACKET_OTHERHOST 3 /* To someone else */

#define PACKET_OUTGOING 4 /* Outgoing of any type */

#define PACKET_LOOPBACK 5 /* MC/BRD frame looped back */

#define PACKET_USER 6 /* To user space */

#define PACKET_KERNEL 7 /* To kernel space */

/* Unused, PACKET_FASTROUTE and PACKET_LOOPBACK are invisible to user space */

#define PACKET_FASTROUTE 6 /* Fastrouted frame */
<br>
이후 패킷이 공유 중인 상태인지 확인하고, ip 헤더를 가져오게 된다.<br>
가져온 아이피 헤더를 가지고 헤더의 길이, ip 버전, checksum, 전체 길이 등을 비교하여 오류가 있을 경우 그 에러에 해당하는 라벨로 goto가 이루어진다. 다만, 명시된 패킷의 길이가 수신한 것보다 짧다면 그 때는 패킷을 드랍하게 된다. 에러 라벨로 가보면 해당하는 drop_reason을 세팅하고 skb free이후에 NULL값을 리턴하고 있음을 볼 수 있다.
또한, skb-&gt;transport_header 포인터는 여기서 설정하게 되는데, skb-&gt;network_header에서 ip header의 길이만큼 더해줌으로써 계산할 수 있다.
<br>
<br>패킷 타입 확인: PACKET_OTHERHOST 타입의 패킷을 걸러낸다. 이 패킷은 다른 호스트를 위한 것으로, 현재 호스트에서 처리되지 않으므로 드롭된다.
<br>if (skb-&gt;pkt_type == PACKET_OTHERHOST) {
    dev_core_stats_rx_otherhost_dropped_inc(skb-&gt;dev);
    drop_reason = SKB_DROP_REASON_OTHERHOST;
    goto drop;
}
<br>
<br>통계 업데이트: 네트워크 통계를 업데이트하여 수신된 패킷의 길이를 기록한다. 
<br>__IP_UPD_PO_STATS(net, IPSTATS_MIB_IN, skb-&gt;len);
<br>
<br>공유 버퍼 확인: 패킷이 공유된 버퍼인지를 확인하고, 필요 시 새로운 버퍼로 복사한다. 복사에 실패하면 패킷을 드롭한다.
<br>skb = skb_share_check(skb, GFP_ATOMIC);
if (!skb) {
    __IP_INC_STATS(net, IPSTATS_MIB_INDISCARDS);
    goto out;
}
<br>
<br>IP 헤더 길이 확인: IP 헤더의 최소 길이를 확보할 수 있는지 확인한다. 확보하지 못하면 패킷을 드롭한다. 
<br>drop_reason = SKB_DROP_REASON_NOT_SPECIFIED;
if (!pskb_may_pull(skb, sizeof(struct iphdr)))
    goto inhdr_error;
<br>
<br>IP 헤더 참조: IP 헤더의 시작을 가리키는 포인터를 가져온다. 
<br>iph = ip_hdr(skb);
<br>
<br>IP 버전과 헤더 길이 확인: IP 헤더 길이와 버전이 유효한지 확인한다. IP 버전이 4가 아니거나, 헤더 길이가 5보다 작으면 패킷을 드롭한다. 
<br>if (iph-&gt;ihl &lt; 5 || iph-&gt;version != 4)
    goto inhdr_error;
<br>
<br>전체 IP 헤더 가져오기: 전체 IP 헤더를 가져올 수 있는지 확인한다. 실패하면 패킷을 드롭한다. 
<br>if (!pskb_may_pull(skb, iph-&gt;ihl*4))
    goto inhdr_error;
<br>
<br>체크섬 검사: IP 헤더의 체크섬이 유효한지 확인한다. 체크섬이 올바르지 않으면 패킷을 드롭한다. 
<br>iph = ip_hdr(skb);
if (unlikely(ip_fast_csum((u8 *)iph, iph-&gt;ihl)))
    goto csum_error;
<br>
<br>패킷 길이 확인: 패킷의 길이가 IP 헤더의 길이와 일치하는지 확인한다. 일치하지 않으면 패킷을 드롭한다. 
<br>len = iph_totlen(skb, iph);
if (skb-&gt;len &lt; len) {
    drop_reason = SKB_DROP_REASON_PKT_TOO_SMALL;
    __IP_INC_STATS(net, IPSTATS_MIB_INTRUNCATEDPKTS);
    goto drop;
} else if (len &lt; (iph-&gt;ihl*4))
    goto inhdr_error;
<br>
<br>패킷 길이 조정: 패킷이 더 길다면 IP 헤더의 총 길이에 맞춰 잘라낸다. 이 과정에서 문제가 발생하면 패킷을 드롭한다. 
<br>if (pskb_trim_rcsum(skb, len)) {
    __IP_INC_STATS(net, IPSTATS_MIB_INDISCARDS);
    goto drop;
}
<br>
<br>전송 헤더 설정: IP 헤더 이후의 데이터가 전송 헤더이므로 그 시작점을 설정한다. 
<br>iph = ip_hdr(skb);
skb-&gt;transport_header = skb-&gt;network_header + iph-&gt;ihl*4;
<br>
<br>소켓 제어 블록 초기화: 소켓 제어 블록을 초기화하여 잔여 데이터를 제거하고, 입력 인터페이스를 설정한다. 
<br>memset(IPCB(skb), 0, sizeof(struct inet_skb_parm));
IPCB(skb)-&gt;iif = skb-&gt;skb_iif;
<br>
<br>소켓 고아 처리: 패킷이 소켓에 의해 미리 처리되지 않았으면 소켓 참조를 제거한다. 
<br>if (!skb_sk_is_prefetched(skb))
    skb_orphan(skb);
<br>
<br>패킷 반환: 모든 검사를 통과한 패킷을 반환한다. 
<br>return skb;
<br>
<br>오류 처리 및 패킷 드롭: 패킷이 검사에서 실패할 경우, 지전된 이유(drop_reason)로 패킷을 드롭하고, 관련 통계를 업데이트한다. 
<br>csum_error:
drop_reason = SKB_DROP_REASON_IP_CSUM;
__IP_INC_STATS(net, IPSTATS_MIB_CSUMERRORS);
inhdr_error:
if (drop_reason == SKB_DROP_REASON_NOT_SPECIFIED)
drop_reason = SKB_DROP_REASON_IP_INHDR;
__IP_INC_STATS(net, IPSTATS_MIB_INHDRERRORS);
drop:
kfree_skb_reason(skb, drop_reason);
out:
return NULL;
]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/ip_rcv_core().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/ip_rcv_core().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ip_rcv_finish_core()]]></title><description><![CDATA[ 
 <br>static int ip_rcv_finish_core(struct net *net, struct sock *sk,
					struct sk_buff *skb, struct net_device *dev,
					const struct sk_buff *hint)
{
	const struct iphdr *iph = ip_hdr(skb);
	int err, drop_reason;
	struct rtable *rt;
	  
	drop_reason = SKB_DROP_REASON_NOT_SPECIFIED;
	  
	if (ip_can_use_hint(skb, iph, hint)) {
		err = ip_route_use_hint(skb, iph-&gt;daddr, iph-&gt;saddr, iph-&gt;tos,
					dev, hint);
		if (unlikely(err))
			goto drop_error;
	}
	  
	if (READ_ONCE(net-&gt;ipv4.sysctl_ip_early_demux) &amp;&amp;
		!skb_dst(skb) &amp;&amp;
		!skb-&gt;sk &amp;&amp;
		!ip_is_fragment(iph)) {
		switch (iph-&gt;protocol) {
		case IPPROTO_TCP:
			if (READ_ONCE(net-&gt;ipv4.sysctl_tcp_early_demux)) {
				tcp_v4_early_demux(skb);
				  
				/* must reload iph, skb-&gt;head might have changed */
				iph = ip_hdr(skb);
			}
			break;
		case IPPROTO_UDP:
			if (READ_ONCE(net-&gt;ipv4.sysctl_udp_early_demux)) {
				err = udp_v4_early_demux(skb);
				if (unlikely(err))
					goto drop_error;
				  
				/* must reload iph, skb-&gt;head might have changed */
				iph = ip_hdr(skb);
			}
			break;
		}
	}
  
	/*
	* Initialise the virtual path cache for the packet. It describes
	* how the packet travels inside Linux networking.
	*/
	if (!skb_valid_dst(skb)) {
		err = ip_route_input_noref(skb, iph-&gt;daddr, iph-&gt;saddr,
						iph-&gt;tos, dev);
		if (unlikely(err))
			goto drop_error;
	} else {
		struct in_device *in_dev = __in_dev_get_rcu(dev);
		  
		if (in_dev &amp;&amp; IN_DEV_ORCONF(in_dev, NOPOLICY))
			IPCB(skb)-&gt;flags |= IPSKB_NOPOLICY;
	}
	  
#ifdef CONFIG_IP_ROUTE_CLASSID
	if (unlikely(skb_dst(skb)-&gt;tclassid)) {
		struct ip_rt_acct *st = this_cpu_ptr(ip_rt_acct);
		u32 idx = skb_dst(skb)-&gt;tclassid;
		st[idx&amp;0xFF].o_packets++;
		st[idx&amp;0xFF].o_bytes += skb-&gt;len;
		st[(idx&gt;&gt;16)&amp;0xFF].i_packets++;
		st[(idx&gt;&gt;16)&amp;0xFF].i_bytes += skb-&gt;len;
	}
#endif
  
	if (iph-&gt;ihl &gt; 5 &amp;&amp; ip_rcv_options(skb, dev))
		goto drop;
	  
	rt = skb_rtable(skb);
	if (rt-&gt;rt_type == RTN_MULTICAST) {
		__IP_UPD_PO_STATS(net, IPSTATS_MIB_INMCAST, skb-&gt;len);
	} else if (rt-&gt;rt_type == RTN_BROADCAST) {
		__IP_UPD_PO_STATS(net, IPSTATS_MIB_INBCAST, skb-&gt;len);
	} else if (skb-&gt;pkt_type == PACKET_BROADCAST ||
			skb-&gt;pkt_type == PACKET_MULTICAST) {
		struct in_device *in_dev = __in_dev_get_rcu(dev);
	  
		/* RFC 1122 3.3.6:
		*
		* When a host sends a datagram to a link-layer broadcast
		* address, the IP destination address MUST be a legal IP
		* broadcast or IP multicast address.
		*
		* A host SHOULD silently discard a datagram that is received
		* via a link-layer broadcast (see Section 2.4) but does not
		* specify an IP multicast or broadcast destination address.
		*
		* This doesn't explicitly say L2 *broadcast*, but broadcast is
		* in a way a form of multicast and the most common use case for
		* this is 802.11 protecting against cross-station spoofing (the
		* so-called "hole-196" attack) so do it for both.
		*/
		if (in_dev &amp;&amp;
			IN_DEV_ORCONF(in_dev, DROP_UNICAST_IN_L2_MULTICAST)) {
			drop_reason = SKB_DROP_REASON_UNICAST_IN_L2_MULTICAST;
			goto drop;
		}
	}
	  
	return NET_RX_SUCCESS;
	  
drop:
	kfree_skb_reason(skb, drop_reason);
	return NET_RX_DROP;
  
drop_error:
	if (err == -EXDEV) {
		drop_reason = SKB_DROP_REASON_IP_RPFILTER;
		__NET_INC_STATS(net, LINUX_MIB_IPRPFILTER);
	}
	goto drop;
}
<br>
우선 ip 헤더를 가져온다.<br>
현재 경로에서는 hint 가 NULL값이므로 첫 번째 if문은 넘어가야 할 것이다. 
그 다음으로는 early_demux가 설정되어 있을 경우, tcp냐 udp냐에 따라서 switch문으로 분기하여 해당함수를 호출하게 된다. 여기서 early_demux란 말 그대로 더 일찍 demultiplexing을 하는 것이다. tcp 레이어에서 더 일찍 디멀티플렉싱을 하는 것은 포트 번호를 통한 분배 밖에 없을 것이다. 즉, 소켓을 받아와서 모두 만족 할 경우 빠르게 작업을 수행하게 된다.
<br><a data-href="tcp_v4_early_demux()" href="encyclopedia-of-networksystem/function/net-ipv4/tcp_v4_early_demux().html" class="internal-link" target="_self" rel="noopener nofollow">tcp_v4_early_demux()</a><br>
<a data-href="ip_route_input_noref()" href="encyclopedia-of-networksystem/function/net-ipv4/ip_route_input_noref().html" class="internal-link" target="_self" rel="noopener nofollow">ip_route_input_noref()</a>]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/ip_rcv_finish_core().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/ip_rcv_finish_core().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ip_rcv_finish()]]></title><description><![CDATA[ 
 <br>static int ip_rcv_finish(struct net *net, struct sock *sk, struct sk_buff *skb)
{
	struct net_device *dev = skb-&gt;dev;
	int ret;
	  
	/* if ingress device is enslaved to an L3 master device pass the
	* skb to its handler for processing
	*/
	skb = l3mdev_ip_rcv(skb);
	if (!skb)
		return NET_RX_SUCCESS;
	  
	ret = ip_rcv_finish_core(net, sk, skb, dev, NULL);
	if (ret != NET_RX_DROP)
		ret = dst_input(skb);
	return ret;
}
<br>
l3mdev_ip_rcv() 함수로 skb를 가져온다. (layer 3 master device ip receive)<br>
skb가 존재하지 않을 경우 rx success를 리턴한다.
실질적인 작업은 ip_rcv_finish_core()에서 이루어지는 것으로 보인다. 만약 드랍되는 패킷이 아니라면 dst_input()함수 또한 실행하게 되고 이후 결과를 return하게 된다.
<br><a data-href="ip_rcv_finish_core()" href="encyclopedia-of-networksystem/function/net-ipv4/ip_rcv_finish_core().html" class="internal-link" target="_self" rel="noopener nofollow">ip_rcv_finish_core()</a><br>
<a data-href="dst_input()" href="encyclopedia-of-networksystem/function/include-net/dst_input().html" class="internal-link" target="_self" rel="noopener nofollow">dst_input()</a><br>
<a data-href="l3mdev_ip_rcv()" href="encyclopedia-of-networksystem/function/net-ipv4/l3mdev_ip_rcv().html" class="internal-link" target="_self" rel="noopener nofollow">l3mdev_ip_rcv()</a>]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/ip_rcv_finish().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/ip_rcv_finish().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ip_rcv()]]></title><description><![CDATA[ 
 <br>net_device parameter의 경우 두개인데, 중복으로 리스트에 추가 될 수 없어서 뒤에 under_bar를 달아 놓았다.<br>/*
* IP receive entry point
*/
int ip_rcv(struct sk_buff *skb, struct net_device *dev, struct packet_type *pt, struct net_device *orig_dev)
{
	struct net *net = dev_net(dev);
	  
	skb = ip_rcv_core(skb, net);
	if (skb == NULL)
		return NET_RX_DROP;
	  
	return NF_HOOK(NFPROTO_IPV4, NF_INET_PRE_ROUTING,
				net, NULL, skb, dev, NULL,
				ip_rcv_finish);`
}
<br>
L3 protocol handler Layer이다.<br>
ip_rcv_core()함수를 호출하여 핵심 로직이 실행되고, 이에 대한 return으로 패킷의 drop 여부를 결정하게 된다.
<br><a data-href="ip_rcv_core()" href="encyclopedia-of-networksystem/function/net-ipv4/ip_rcv_core().html" class="internal-link" target="_self" rel="noopener nofollow">ip_rcv_core()</a><br>
<a data-href="ip_rcv_finish()" href="encyclopedia-of-networksystem/function/net-ipv4/ip_rcv_finish().html" class="internal-link" target="_self" rel="noopener nofollow">ip_rcv_finish()</a><br>
<a data-tooltip-position="top" aria-label="NF_HOOK()" data-href="NF_HOOK()" href="encyclopedia-of-networksystem/function/include-linux/nf_hook().html" class="internal-link" target="_self" rel="noopener nofollow">NF_HOOK()</a>]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/ip_rcv().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/ip_rcv().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ip_route_input_noref()]]></title><description><![CDATA[ 
 <br>int ip_route_input_noref(struct sk_buff *skb, __be32 daddr, __be32 saddr,
			u8 tos, struct net_device *dev)
{
	struct fib_result res;
	int err;
	  
	tos &amp;= IPTOS_RT_MASK;
	rcu_read_lock();
	err = ip_route_input_rcu(skb, daddr, saddr, tos, dev, &amp;res);
	rcu_read_unlock();
	  
	return err;
}
<br>
rcu 락을 획득하고, ip_route_input_rcu()함수를 호출하게 된다. 단순히 RCU를 획득하기 위한wrapper 함수이다.
<br><a data-href="ip_route_input_rcu()" href="encyclopedia-of-networksystem/function/net-ipv4/ip_route_input_rcu().html" class="internal-link" target="_self" rel="noopener nofollow">ip_route_input_rcu()</a>]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/ip_route_input_noref().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/ip_route_input_noref().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ip_route_input_rcu()]]></title><description><![CDATA[ 
 <br>/* called with rcu_read_lock held */
static int ip_route_input_rcu(struct sk_buff *skb, __be32 daddr, __be32 saddr,
					u8 tos, struct net_device *dev, struct fib_result *res)
{
	/* Multicast recognition logic is moved from route cache to here.
	* The problem was that too many Ethernet cards have broken/missing
	* hardware multicast filters :-( As result the host on multicasting
	* network acquires a lot of useless route cache entries, sort of
	* SDR messages from all the world. Now we try to get rid of them.
	* Really, provided software IP multicast filter is organized
	* reasonably (at least, hashed), it does not result in a slowdown
	* comparing with route cache reject entries.
	* Note, that multicast routers are not affected, because
	* route cache entry is created eventually.
	*/
	if (ipv4_is_multicast(daddr)) {
		struct in_device *in_dev = __in_dev_get_rcu(dev);
		int our = 0;
		int err = -EINVAL;
		  
		if (!in_dev)
			return err;
		our = ip_check_mc_rcu(in_dev, daddr, saddr,
						ip_hdr(skb)-&gt;protocol);
		  
		/* check l3 master if no match yet */
		if (!our &amp;&amp; netif_is_l3_slave(dev)) {
			struct in_device *l3_in_dev;
		  
			l3_in_dev = __in_dev_get_rcu(skb-&gt;dev);
			if (l3_in_dev)
				our = qip_check_mc_rcu(l3_in_dev, daddr, saddr,
							ip_hdr(skb)-&gt;protocol);
	}
  
		if (our
#ifdef CONFIG_IP_MROUTE
			||
			(!ipv4_is_local_multicast(daddr) &amp;&amp;
			IN_DEV_MFORWARD(in_dev))
#endif
			) {
			err = ip_route_input_mc(skb, daddr, saddr,
						tos, dev, our);
		}
		return err;
	}
  
	return ip_route_input_slow(skb, daddr, saddr, tos, dev, res);
}
<br>
멀티캐스트와 관련된 recognition이 여기에 많이 implement 되어있다. 기존에 다른 곳에 있었을 때는 하드웨어 레벨에서 멀티캐스트 필터가 고장나거나 잃어버리면서 제대로 작동하지 않는 경우가 굉장히 많았고, 불필요한 라우트 캐시 등을 생성하였다고 한다. 이러한 코드를 본 함수로 옮김으로써 어느정도 해결하였다고 한다. 
따라서 if문을 통해 ipv4_is_multicast()함수를 조건으로 하여 멀티캐스트와 관련 된 처리들을 할 수 있게 코드가 작성되었고, 실질적인 라우팅은 return에 보면 ip_route_input_slow()함수를 실행하고 있는 것을 볼 수 있다.
<br><a data-href="ip_route_input_slow()" href="encyclopedia-of-networksystem/function/net-ipv4/ip_route_input_slow().html" class="internal-link" target="_self" rel="noopener nofollow">ip_route_input_slow()</a>]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/ip_route_input_rcu().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/ip_route_input_rcu().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ip_route_input_slow()]]></title><description><![CDATA[ 
 <br>/*
 *  NOTE. We drop all the packets that has local source
 *  addresses, because every properly looped back packet
 *  must have correct destination already attached by output routine.
 *  Changes in the enforced policies must be applied also to
 *  ip_route_use_hint().
 *
 *  Such approach solves two big problems:
 *  1. Not simplex devices are handled properly.
 *  2. IP spoofing attempts are filtered with 100% of guarantee.
 *  called with rcu_read_lock()
 */
  
static int ip_route_input_slow(struct sk_buff *skb, __be32 daddr, __be32 saddr,
                   u8 tos, struct net_device *dev,
                   struct fib_result *res)
{
    struct in_device *in_dev = __in_dev_get_rcu(dev);
    struct flow_keys *flkeys = NULL, _flkeys;
    struct net    *net = dev_net(dev);
    struct ip_tunnel_info *tun_info;
    int     err = -EINVAL;
    unsigned int    flags = 0;
    u32     itag = 0;
    struct rtable   *rth;
    struct flowi4   fl4;
    bool do_cache = true;
  
    /* IP on this device is disabled. */
  
    if (!in_dev)
        goto out;
  
    /* Check for the most weird martians, which can be not detected
     * by fib_lookup.
     */
  
    tun_info = skb_tunnel_info(skb);
    if (tun_info &amp;&amp; !(tun_info-&gt;mode &amp; IP_TUNNEL_INFO_TX))
        fl4.flowi4_tun_key.tun_id = tun_info-&gt;key.tun_id;
    else
        fl4.flowi4_tun_key.tun_id = 0;
    skb_dst_drop(skb);
  
    if (ipv4_is_multicast(saddr) || ipv4_is_lbcast(saddr))
        goto martian_source;
  
    res-&gt;fi = NULL;
    res-&gt;table = NULL;
    if (ipv4_is_lbcast(daddr) || (saddr == 0 &amp;&amp; daddr == 0))
        goto brd_input;
  
    /* Accept zero addresses only to limited broadcast;
     * I even do not know to fix it or not. Waiting for complains :-)
     */
    if (ipv4_is_zeronet(saddr))
        goto martian_source;
  
    if (ipv4_is_zeronet(daddr))
        goto martian_destination;
  
    /* Following code try to avoid calling IN_DEV_NET_ROUTE_LOCALNET(),
     * and call it once if daddr or/and saddr are loopback addresses
     */
    if (ipv4_is_loopback(daddr)) {
        if (!IN_DEV_NET_ROUTE_LOCALNET(in_dev, net))
            goto martian_destination;
    } else if (ipv4_is_loopback(saddr)) {
        if (!IN_DEV_NET_ROUTE_LOCALNET(in_dev, net))
            goto martian_source;
    }
  
    /*
     *  Now we are ready to route packet.
     */
    fl4.flowi4_l3mdev = 0;
    fl4.flowi4_oif = 0;
    fl4.flowi4_iif = dev-&gt;ifindex;
    fl4.flowi4_mark = skb-&gt;mark;
    fl4.flowi4_tos = tos;
    fl4.flowi4_scope = RT_SCOPE_UNIVERSE;
    fl4.flowi4_flags = 0;
    fl4.daddr = daddr;
    fl4.saddr = saddr;
    fl4.flowi4_uid = sock_net_uid(net, NULL);
    fl4.flowi4_multipath_hash = 0;
  
    if (fib4_rules_early_flow_dissect(net, skb, &amp;fl4, &amp;_flkeys)) {
        flkeys = &amp;_flkeys;
    } else {
        fl4.flowi4_proto = 0;
        fl4.fl4_sport = 0;
        fl4.fl4_dport = 0;
    }
  
    err = fib_lookup(net, &amp;fl4, res, 0);
    if (err != 0) {
        if (!IN_DEV_FORWARD(in_dev))
            err = -EHOSTUNREACH;
        goto no_route;
    }
  
    if (res-&gt;type == RTN_BROADCAST) {
        if (IN_DEV_BFORWARD(in_dev))
            goto make_route;
        /* not do cache if bc_forwarding is enabled */
        if (IPV4_DEVCONF_ALL_RO(net, BC_FORWARDING))
            do_cache = false;
        goto brd_input;
    }


    if (res-&gt;type == RTN_LOCAL) {
        err = fib_validate_source(skb, saddr, daddr, tos,
                      0, dev, in_dev, &amp;itag);
        if (err &lt; 0)
            goto martian_source;
        goto local_input;
    }
  
    if (!IN_DEV_FORWARD(in_dev)) {
        err = -EHOSTUNREACH;
        goto no_route;
    }
    if (res-&gt;type != RTN_UNICAST)
        goto martian_destination;
  
make_route:
    err = ip_mkroute_input(skb, res, in_dev, daddr, saddr, tos, flkeys);
out:    return err;
  
brd_input:
    if (skb-&gt;protocol != htons(ETH_P_IP))
        goto e_inval;
  
    if (!ipv4_is_zeronet(saddr)) {
        err = fib_validate_source(skb, saddr, 0, tos, 0, dev,
                      in_dev, &amp;itag);
        if (err &lt; 0)
            goto martian_source;
    }
    flags |= RTCF_BROADCAST;
    res-&gt;type = RTN_BROADCAST;
    RT_CACHE_STAT_INC(in_brd);
  
local_input:
    if (IN_DEV_ORCONF(in_dev, NOPOLICY))
        IPCB(skb)-&gt;flags |= IPSKB_NOPOLICY;
  
    do_cache &amp;= res-&gt;fi &amp;&amp; !itag;
    if (do_cache) {
        struct fib_nh_common *nhc = FIB_RES_NHC(*res);
  
        rth = rcu_dereference(nhc-&gt;nhc_rth_input);
        if (rt_cache_valid(rth)) {
            skb_dst_set_noref(skb, &amp;rth-&gt;dst);
            err = 0;
            goto out;
        }
    }
  
    rth = rt_dst_alloc(ip_rt_get_dev(net, res),
               flags | RTCF_LOCAL, res-&gt;type, false);
    if (!rth)
        goto e_nobufs;
  
    rth-&gt;dst.output= ip_rt_bug;
#ifdef CONFIG_IP_ROUTE_CLASSID
    rth-&gt;dst.tclassid = itag;
#endif
    rth-&gt;rt_is_input = 1;
  
    RT_CACHE_STAT_INC(in_slow_tot);
    if (res-&gt;type == RTN_UNREACHABLE) {
        rth-&gt;dst.input= ip_error;
        rth-&gt;dst.error= -err;
        rth-&gt;rt_flags   &amp;= ~RTCF_LOCAL;
    }
  
    if (do_cache) {
        struct fib_nh_common *nhc = FIB_RES_NHC(*res);
  
        rth-&gt;dst.lwtstate = lwtstate_get(nhc-&gt;nhc_lwtstate);
        if (lwtunnel_input_redirect(rth-&gt;dst.lwtstate)) {
            WARN_ON(rth-&gt;dst.input == lwtunnel_input);
            rth-&gt;dst.lwtstate-&gt;orig_input = rth-&gt;dst.input;
            rth-&gt;dst.input = lwtunnel_input;
        }
        
        if (unlikely(!rt_cache_route(nhc, rth)))
            rt_add_uncached_list(rth);
    }
    skb_dst_set(skb, &amp;rth-&gt;dst);
    err = 0;
    goto out;
  
no_route:
    RT_CACHE_STAT_INC(in_no_route);
    res-&gt;type = RTN_UNREACHABLE;
    res-&gt;fi = NULL;
    res-&gt;table = NULL;
    goto local_input;
  
    /*
     *  Do not cache martian addresses: they should be logged (RFC1812)
     */
martian_destination:
    RT_CACHE_STAT_INC(in_martian_dst);
#ifdef CONFIG_IP_ROUTE_VERBOSE
    if (IN_DEV_LOG_MARTIANS(in_dev))
        net_warn_ratelimited("martian destination %pI4 from %pI4, dev %s\n",
                     &amp;daddr, &amp;saddr, dev-&gt;name);
#endif
  
e_inval:
    err = -EINVAL;
    goto out;
  
e_nobufs:
    err = -ENOBUFS;
    goto out;
  
martian_source:
    ip_handle_martian_source(dev, in_dev, skb, daddr, saddr);
    goto out;
}
<br>
중간에 flow_keys라는 구조체를 선언하는 것을 볼 수 있다. 이는 패킷을 전부 처리하지 않고, 특정 메타데이터를 뽑아내는 flow dissector라는 기술에 쓰이는 구조체이다.<br>
다음으로 봐야할 구조체는 flowi4이다.<br>
input / output interface, l3mdev, tos, scope, protocol, flags, tun_key, uid, saddr, daddr, sport, dport, icmp_type ... 등등 많은 필드를 가지고 있다.<br>
또 다른 구조체로는 fib_result가 있다.<br>
fib란, Forwarding Information Base의 약자로, 3계층에서 포워딩에 필요한 정보들을 가지고 있는 베이스이다. fib_result구조체는 다양한 필드를 포함하고 있으며 prefix, prefixlen, nh_sel, type, scope, tclassid, nhc, fib_info타입, fib_table타입, hlist_head타입 등을 포함하고 있다.
본 함수에서 사용되는 "martians"라는 키워드는 '이상한' 이라는 의미를 가진다고 보면 된다.<br>
함수 코드를 차례로 살펴보자면, 우선 tunnel과 관련된 설정을 해주게 된다. tun_info 변수를 확인하여 만약 유효하다면, fl4로 선언되어 있는 flowi4구조체 에다가 해당 필드를 세팅하게 된다.
그 후 skb_dst_drop()함수를 통해 해당 skb의 dst부분을 드랍하게 된다. 이것은 아마도 앞서 호출한 tcp_v4_early_demux를 통해 얻은 dst_entry 구조체로 보인다.
추가적으로 확인하는 것은 멀티캐스트와 limited 브로드캐스트의 여부이다. 또한 source와 dest의 addr이 0.0.0.0인 경우도 예외로 하여 goto를 사용하고 있었다.
그 후에야 fl4 변수를 설정하기 시작하여 패킷을 라우팅할 준비를 하기 시작한다.
그 다음으로 호출하는 함수는 fib_lookup()이다. 이 함수는 parameter로 받은 flp에다가 fib 정보를 저장하게 된다. 그리고 fib_result를 세팅하게 될 것이다.
<br>--9월 20일 미팅 --<br>
end-to-end 경로만 살펴 볼 것이므로, local_input라벨로 가는 것만 살펴 볼 것이다. 우선 fib_lookup()함수를 호출하여 포인터 타입으로 넘겨준 fib_result타입의 res변수에다가 type필드에 해당 패킷이 어디로 가야 할지 enum을 통해 설정하게 되고, 이 값은 RTN_LOCAL이 된다. 따라서 local_input라벨로 가게 되는 것이다. 이후 dst_entry 구조체를 세팅하고 함수가 리턴된다.
<br><a data-href="fib_lookup()" href="encyclopedia-of-networksystem/function/include-net/fib_lookup().html" class="internal-link" target="_self" rel="noopener nofollow">fib_lookup()</a>]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/ip_route_input_slow().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/ip_route_input_slow().md</guid><pubDate>Tue, 08 Oct 2024 08:24:56 GMT</pubDate></item><item><title><![CDATA[l3mdev_ip_rcv()]]></title><description><![CDATA[ 
 <br>static inline
struct sk_buff *l3mdev_ip_rcv(struct sk_buff *skb)
{
	return l3mdev_l3_rcv(skb, AF_INET);
}
<br>l3mdev_l3_rcv() 함수를 호출한다.<br>static inline
struct sk_buff *l3mdev_l3_rcv(struct sk_buff *skb, u16 proto)
{
	struct net_device *master = NULL;

	if (netif_is_l3_slave(skb-&gt;dev))
		master = netdev_master_upper_dev_get_rcu(skb-&gt;dev);
	else if (netif_is_l3_master(skb-&gt;dev) ||
		 netif_has_l3_rx_handler(skb-&gt;dev))
		master = skb-&gt;dev;

	if (master &amp;&amp; master-&gt;l3mdev_ops-&gt;l3mdev_l3_rcv)
		skb = master-&gt;l3mdev_ops-&gt;l3mdev_l3_rcv(master, skb, proto);

	return skb;
}
<br>skb의 device network interface가 l3_slave인 경우, <br>
L3 Slave는 Layer 3 네트워크에서 상위 장치나 네트워크에 종속된 장치를 의미한다. 일반적으로 L3 네트워크에서 패킷 라우팅을 처리하는 상위 장치가 L3 마스터 역할을 하고, 그 아래에서 패킷을 전달받아 처리하는 장치가 L3 Slave 역할을 한다. L3 Slave는 마스터 장치에 종속되어 동작하며, 여러 개의 Slave가 하나의 마스터 아래에서 동작할 수 있다. 이를 통해 네트워크 트래픽을 효율적으로 관리하고 분산 처리할 수 있다. 
<br>static inline
struct sk_buff *l3mdev_ip_rcv(struct sk_buff *skb)
{
	return skb;
}
]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/l3mdev_ip_rcv().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/l3mdev_ip_rcv().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[sk_add_backlog()]]></title><description><![CDATA[ 
 <br>/* The per-socket spinlock must be held here. */
static inline __must_check int sk_add_backlog(struct sock *sk, struct sk_buff *skb,
					      unsigned int limit)
{
	if (sk_rcvqueues_full(sk, limit))
		return -ENOBUFS;

	/*
	 * If the skb was allocated from pfmemalloc reserves, only
	 * allow SOCK_MEMALLOC sockets to use it as this socket is
	 * helping free memory
	 */
	if (skb_pfmemalloc(skb) &amp;&amp; !sock_flag(sk, SOCK_MEMALLOC))
		return -ENOMEM;

	__sk_add_backlog(sk, skb);
	sk-&gt;sk_backlog.len += skb-&gt;truesize;
	return 0;
}
<br>sk_rcv_queue 가 가득차 있는지 확인한다. 공간이 없으면 에러 메세지 출력한다.<br>
pfmemalloc인 경우 에러메세지 리턴. socket이 pfmemalloc을 건드릴수있는지 확인.<br>
sk_add_backlog() 실행한다.<br>/* OOB backlog add */
static inline void __sk_add_backlog(struct sock *sk, struct sk_buff *skb)
{
	/* dont let skb dst not refcounted, we are going to leave rcu lock */
	skb_dst_force(skb);

	if (!sk-&gt;sk_backlog.tail)
		WRITE_ONCE(sk-&gt;sk_backlog.head, skb);
	else
		sk-&gt;sk_backlog.tail-&gt;next = skb;

	WRITE_ONCE(sk-&gt;sk_backlog.tail, skb);
	skb-&gt;next = NULL;
}
]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/sk_add_backlog().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/sk_add_backlog().md</guid><pubDate>Tue, 08 Oct 2024 08:24:56 GMT</pubDate></item><item><title><![CDATA[skb_condense()]]></title><description><![CDATA[ 
 <br>/**
 * skb_condense - try to get rid of fragments/frag_list if possible
 * @skb: buffer
 *
 * Can be used to save memory before skb is added to a busy queue.
 * If packet has bytes in frags and enough tail room in skb-&gt;head,
 * pull all of them, so that we can free the frags right now and adjust
 * truesize.
 * Notes:
 *	We do not reallocate skb-&gt;head thus can not fail.
 *	Caller must re-evaluate skb-&gt;truesize if needed.
 */
void skb_condense(struct sk_buff *skb)
{
	if (skb-&gt;data_len) {
		if (skb-&gt;data_len &gt; skb-&gt;end - skb-&gt;tail ||
		    skb_cloned(skb))
			return;

		/* Nice, we can free page frag(s) right now */
		__pskb_pull_tail(skb, skb-&gt;data_len);
	}
	/* At this point, skb-&gt;truesize might be over estimated,
	 * because skb had a fragment, and fragments do not tell
	 * their truesize.
	 * When we pulled its content into skb-&gt;head, fragment
	 * was freed, but __pskb_pull_tail() could not possibly
	 * adjust skb-&gt;truesize, not knowing the frag truesize.
	 */
	skb-&gt;truesize = SKB_TRUESIZE(skb_end_offset(skb));
}
]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/skb_condense().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/skb_condense().md</guid><pubDate>Tue, 08 Oct 2024 08:24:56 GMT</pubDate></item><item><title><![CDATA[skb_try_coalesce()]]></title><description><![CDATA[ 
 <br>/**
 * skb_try_coalesce - try to merge skb to prior one
 * @to: prior buffer
 * @from: buffer to add
 * @fragstolen: pointer to boolean
 * @delta_truesize: how much more was allocated than was requested
 */
bool skb_try_coalesce(struct sk_buff *to, struct sk_buff *from,
		      bool *fragstolen, int *delta_truesize)
{
	struct skb_shared_info *to_shinfo, *from_shinfo;
	int i, delta, len = from-&gt;len;

	*fragstolen = false;

	if (skb_cloned(to))
		return false;

	/* In general, avoid mixing page_pool and non-page_pool allocated
	 * pages within the same SKB. In theory we could take full
	 * references if @from is cloned and !@to-&gt;pp_recycle but its
	 * tricky (due to potential race with the clone disappearing) and
	 * rare, so not worth dealing with.
	 */
	if (to-&gt;pp_recycle != from-&gt;pp_recycle)
		return false;
	//important
	if (len &lt;= skb_tailroom(to)) {
		if (len)
			BUG_ON(skb_copy_bits(from, 0, skb_put(to, len), len));
		*delta_truesize = 0;
		return true;
	}

	to_shinfo = skb_shinfo(to);
	from_shinfo = skb_shinfo(from);
	if (to_shinfo-&gt;frag_list || from_shinfo-&gt;frag_list)
		return false;
	if (skb_zcopy(to) || skb_zcopy(from))
		return false;

	if (skb_headlen(from) != 0) {
		struct page *page;
		unsigned int offset;

		if (to_shinfo-&gt;nr_frags +
		    from_shinfo-&gt;nr_frags &gt;= MAX_SKB_FRAGS)
			return false;

		if (skb_head_is_locked(from))
			return false;

		delta = from-&gt;truesize - SKB_DATA_ALIGN(sizeof(struct sk_buff));

		page = virt_to_head_page(from-&gt;head);
		offset = from-&gt;data - (unsigned char *)page_address(page);

		//important
		skb_fill_page_desc(to, to_shinfo-&gt;nr_frags,
				   page, offset, skb_headlen(from));
		*fragstolen = true;
	} else {
		if (to_shinfo-&gt;nr_frags +
		    from_shinfo-&gt;nr_frags &gt; MAX_SKB_FRAGS)
			return false;

		delta = from-&gt;truesize - SKB_TRUESIZE(skb_end_offset(from));
	}

	WARN_ON_ONCE(delta &lt; len);

	memcpy(to_shinfo-&gt;frags + to_shinfo-&gt;nr_frags,
	       from_shinfo-&gt;frags,
	       from_shinfo-&gt;nr_frags * sizeof(skb_frag_t));
	to_shinfo-&gt;nr_frags += from_shinfo-&gt;nr_frags;

	if (!skb_cloned(from))
		from_shinfo-&gt;nr_frags = 0;

	/* if the skb is not cloned this does nothing
	 * since we set nr_frags to 0.
	 */
	if (skb_pp_frag_ref(from)) {
		for (i = 0; i &lt; from_shinfo-&gt;nr_frags; i++)
			__skb_frag_ref(&amp;from_shinfo-&gt;frags[i]);
	}

	to-&gt;truesize += delta;
	to-&gt;len += len;
	to-&gt;data_len += len;

	*delta_truesize = delta;
	return true;
}
<br>pkt들을 합치는 함수이다. to skb에다 from skb를 합친다.<br>
to skb는 sk backlog에 가장 마지막 skb이고 from skb는 backlog에 올리고자하는 skb이다.<br>
skb들이 clone된 상태거나, frag_list를 가지고 있거나 zero copy가 된 경우에는 합치지 못한다.<br>
만약 to skb에 공간이 충분하다면<br>
if (len &lt;= skb_tailroom(to)) {<br>
BUG_ON(skb_copy_bits(from, 0, skb_put(to, len), len));<br>
to 에 data를 옮겨주고 리턴한다.<br>공간이 부족한 경우, from skb의 frag들을 to skb로 이동시켜주는 방식으로 합친다.<br>
from skb의 fragment들을 to skb frags[]에다 memcpy해준다. <br>skb_fill_page_desc(to, to_shinfo-&gt;nr_frags, page, offset, skb_headlen(from));<br>
위의 함수로 from skb의 head에 존재하는 data를 to skb의 page로 넣는다.<br>
to skb가 nr_frags 번째 frag가 page 안의 skb_headlen(from)만큼의 데이터를 offset에서 부터 가르키게 만든다. <br>to skb의 true size를 업데이트 해준다. ]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/skb_try_coalesce().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/skb_try_coalesce().md</guid><pubDate>Tue, 08 Oct 2024 08:24:56 GMT</pubDate></item><item><title><![CDATA[sock_def_readable()]]></title><description><![CDATA[ 
 <br>void sock_def_readable(struct sock *sk)
{
	struct socket_wq *wq;

	trace_sk_data_ready(sk);

	rcu_read_lock();
	wq = rcu_dereference(sk-&gt;sk_wq);
	if (skwq_has_sleeper(wq))
		wake_up_interruptible_sync_poll(&amp;wq-&gt;wait, EPOLLIN | EPOLLPRI |
						EPOLLRDNORM | EPOLLRDBAND);
	sk_wake_async_rcu(sk, SOCK_WAKE_WAITD, POLL_IN);
	rcu_read_unlock();
}
<br>#define wake_up_interruptible_sync_poll(x, m)					\
	__wake_up_sync_key((x), TASK_INTERRUPTIBLE, poll_to_key(m))
<br>/**
 * __wake_up_sync_key - wake up threads blocked on a waitqueue.
 * @wq_head: the waitqueue
 * @mode: which threads
 * @key: opaque value to be passed to wakeup targets
 *
 * The sync wakeup differs that the waker knows that it will schedule
 * away soon, so while the target thread will be woken up, it will not
 * be migrated to another CPU - ie. the two threads are 'synchronized'
 * with each other. This can prevent needless bouncing between CPUs.
 *
 * On UP it can prevent extra preemption.
 *
 * If this function wakes up a task, it executes a full memory barrier before
 * accessing the task state.
 */
void __wake_up_sync_key(struct wait_queue_head *wq_head, unsigned int mode,
			void *key)
{
	if (unlikely(!wq_head))
		return;

	__wake_up_common_lock(wq_head, mode, 1, WF_SYNC, key);
}
EXPORT_SYMBOL_GPL(__wake_up_sync_key);
<br>static int __wake_up_common_lock(struct wait_queue_head *wq_head, unsigned int mode,
			int nr_exclusive, int wake_flags, void *key)
{
	unsigned long flags;
	int remaining;

	spin_lock_irqsave(&amp;wq_head-&gt;lock, flags);
	remaining = __wake_up_common(wq_head, mode, nr_exclusive, wake_flags,
			key);
	spin_unlock_irqrestore(&amp;wq_head-&gt;lock, flags);

	return nr_exclusive - remaining;
}
]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/sock_def_readable().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/sock_def_readable().md</guid><pubDate>Thu, 10 Oct 2024 11:46:39 GMT</pubDate></item><item><title><![CDATA[sock_owned_by_user()]]></title><description><![CDATA[ 
 <br>static inline bool sock_owned_by_user(const struct sock *sk)
{
	sock_owned_by_me(sk);
	return sk-&gt;sk_lock.owned;
}
<br>socket에 lock이 되어 있는지 확인하는 간단한 함수이다.<br>sock_owned_by_me()<br>/* Used by processes to "lock" a socket state, so that
 * interrupts and bottom half handlers won't change it
 * from under us. It essentially blocks any incoming
 * packets, so that we won't get any new data or any
 * packets that change the state of the socket.
 *
 * While locked, BH processing will add new packets to
 * the backlog queue.  This queue is processed by the
 * owner of the socket lock right before it is released.
 *
 * Since ~2.3.5 it is also exclusive sleep lock serializing
 * accesses from user process context.
 */

static inline void sock_owned_by_me(const struct sock *sk)
{
#ifdef CONFIG_LOCKDEP
	WARN_ON_ONCE(!lockdep_sock_is_held(sk) &amp;&amp; debug_locks);
#endif
}
]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/sock_owned_by_user().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/sock_owned_by_user().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[tcp_ack()]]></title><description><![CDATA[ 
 <br>/* This routine deals with incoming acks, but not outgoing ones. */
static int tcp_ack(struct sock *sk, const struct sk_buff *skb, int flag)
{
	struct inet_connection_sock *icsk = inet_csk(sk);
	struct tcp_sock *tp = tcp_sk(sk);
	struct tcp_sacktag_state sack_state;
	struct rate_sample rs = { .prior_delivered = 0 };
	u32 prior_snd_una = tp-&gt;snd_una;
	bool is_sack_reneg = tp-&gt;is_sack_reneg;
	u32 ack_seq = TCP_SKB_CB(skb)-&gt;seq;
	u32 ack = TCP_SKB_CB(skb)-&gt;ack_seq;
	int num_dupack = 0;
	int prior_packets = tp-&gt;packets_out;
	u32 delivered = tp-&gt;delivered;
	u32 lost = tp-&gt;lost;
	int rexmit = REXMIT_NONE; /* Flag to (re)transmit to recover losses */
	u32 prior_fack;

	sack_state.first_sackt = 0;
	sack_state.rate = &amp;rs;
	sack_state.sack_delivered = 0;

	/* We very likely will need to access rtx queue. */
	prefetch(sk-&gt;tcp_rtx_queue.rb_node);

	/* If the ack is older than previous acks
	 * then we can probably ignore it.
	 */
	if (before(ack, prior_snd_una)) {
		u32 max_window;

		/* do not accept ACK for bytes we never sent. */
		max_window = min_t(u64, tp-&gt;max_window, tp-&gt;bytes_acked);
		/* RFC 5961 5.2 [Blind Data Injection Attack].[Mitigation] */
		if (before(ack, prior_snd_una - max_window)) {
			if (!(flag &amp; FLAG_NO_CHALLENGE_ACK))
				tcp_send_challenge_ack(sk);
			return -SKB_DROP_REASON_TCP_TOO_OLD_ACK;
		}
		goto old_ack;
	}

	/* If the ack includes data we haven't sent yet, discard
	 * this segment (RFC793 Section 3.9).
	 */
	if (after(ack, tp-&gt;snd_nxt))
		return -SKB_DROP_REASON_TCP_ACK_UNSENT_DATA;

	if (after(ack, prior_snd_una)) {
		flag |= FLAG_SND_UNA_ADVANCED;
		icsk-&gt;icsk_retransmits = 0;

#if IS_ENABLED(CONFIG_TLS_DEVICE)
		if (static_branch_unlikely(&amp;clean_acked_data_enabled.key))
			if (icsk-&gt;icsk_clean_acked)
				icsk-&gt;icsk_clean_acked(sk, ack);
#endif
	}

	prior_fack = tcp_is_sack(tp) ? tcp_highest_sack_seq(tp) : tp-&gt;snd_una;
	rs.prior_in_flight = tcp_packets_in_flight(tp);

	/* ts_recent update must be made after we are sure that the packet
	 * is in window.
	 */
	if (flag &amp; FLAG_UPDATE_TS_RECENT)
		tcp_replace_ts_recent(tp, TCP_SKB_CB(skb)-&gt;seq);

	if ((flag &amp; (FLAG_SLOWPATH | FLAG_SND_UNA_ADVANCED)) ==
	    FLAG_SND_UNA_ADVANCED) {
		/* Window is constant, pure forward advance.
		 * No more checks are required.
		 * Note, we use the fact that SND.UNA&gt;=SND.WL2.
		 */
		tcp_update_wl(tp, ack_seq);
		tcp_snd_una_update(tp, ack);
		flag |= FLAG_WIN_UPDATE;

		tcp_in_ack_event(sk, CA_ACK_WIN_UPDATE);

		NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPHPACKS);
	} else {
		u32 ack_ev_flags = CA_ACK_SLOWPATH;

		if (ack_seq != TCP_SKB_CB(skb)-&gt;end_seq)
			flag |= FLAG_DATA;
		else
			NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPPUREACKS);

		flag |= tcp_ack_update_window(sk, skb, ack, ack_seq);

		if (TCP_SKB_CB(skb)-&gt;sacked)
			flag |= tcp_sacktag_write_queue(sk, skb, prior_snd_una,
							&amp;sack_state);

		if (tcp_ecn_rcv_ecn_echo(tp, tcp_hdr(skb))) {
			flag |= FLAG_ECE;
			ack_ev_flags |= CA_ACK_ECE;
		}

		if (sack_state.sack_delivered)
			tcp_count_delivered(tp, sack_state.sack_delivered,
					    flag &amp; FLAG_ECE);

		if (flag &amp; FLAG_WIN_UPDATE)
			ack_ev_flags |= CA_ACK_WIN_UPDATE;

		tcp_in_ack_event(sk, ack_ev_flags);
	}

	/* This is a deviation from RFC3168 since it states that:
	 * "When the TCP data sender is ready to set the CWR bit after reducing
	 * the congestion window, it SHOULD set the CWR bit only on the first
	 * new data packet that it transmits."
	 * We accept CWR on pure ACKs to be more robust
	 * with widely-deployed TCP implementations that do this.
	 */
	tcp_ecn_accept_cwr(sk, skb);

	/* We passed data and got it acked, remove any soft error
	 * log. Something worked...
	 */
	WRITE_ONCE(sk-&gt;sk_err_soft, 0);
	icsk-&gt;icsk_probes_out = 0;
	tp-&gt;rcv_tstamp = tcp_jiffies32;
	if (!prior_packets)
		goto no_queue;

	/* See if we can take anything off of the retransmit queue. */
	flag |= tcp_clean_rtx_queue(sk, skb, prior_fack, prior_snd_una,
				    &amp;sack_state, flag &amp; FLAG_ECE);

	tcp_rack_update_reo_wnd(sk, &amp;rs);

	if (tp-&gt;tlp_high_seq)
		tcp_process_tlp_ack(sk, ack, flag);

	if (tcp_ack_is_dubious(sk, flag)) {
		if (!(flag &amp; (FLAG_SND_UNA_ADVANCED |
			      FLAG_NOT_DUP | FLAG_DSACKING_ACK))) {
			num_dupack = 1;
			/* Consider if pure acks were aggregated in tcp_add_backlog() */
			if (!(flag &amp; FLAG_DATA))
				num_dupack = max_t(u16, 1, skb_shinfo(skb)-&gt;gso_segs);
		}
		tcp_fastretrans_alert(sk, prior_snd_una, num_dupack, &amp;flag,
				      &amp;rexmit);
	}

	/* If needed, reset TLP/RTO timer when RACK doesn't set. */
	if (flag &amp; FLAG_SET_XMIT_TIMER)
		tcp_set_xmit_timer(sk);

	if ((flag &amp; FLAG_FORWARD_PROGRESS) || !(flag &amp; FLAG_NOT_DUP))
		sk_dst_confirm(sk);

	delivered = tcp_newly_delivered(sk, delivered, flag);
	lost = tp-&gt;lost - lost;			/* freshly marked lost */
	rs.is_ack_delayed = !!(flag &amp; FLAG_ACK_MAYBE_DELAYED);
	tcp_rate_gen(sk, delivered, lost, is_sack_reneg, sack_state.rate);
	tcp_cong_control(sk, ack, delivered, flag, sack_state.rate);
	tcp_xmit_recovery(sk, rexmit);
	return 1;

no_queue:
	/* If data was DSACKed, see if we can undo a cwnd reduction. */
	if (flag &amp; FLAG_DSACKING_ACK) {
		tcp_fastretrans_alert(sk, prior_snd_una, num_dupack, &amp;flag,
				      &amp;rexmit);
		tcp_newly_delivered(sk, delivered, flag);
	}
	/* If this ack opens up a zero window, clear backoff.  It was
	 * being used to time the probes, and is probably far higher than
	 * it needs to be for normal retransmission.
	 */
	tcp_ack_probe(sk);

	if (tp-&gt;tlp_high_seq)
		tcp_process_tlp_ack(sk, ack, flag);
	return 1;

old_ack:
	/* If data was SACKed, tag it and see if we should send more data.
	 * If data was DSACKed, see if we can undo a cwnd reduction.
	 */
	if (TCP_SKB_CB(skb)-&gt;sacked) {
		flag |= tcp_sacktag_write_queue(sk, skb, prior_snd_una,
						&amp;sack_state);
		tcp_fastretrans_alert(sk, prior_snd_una, num_dupack, &amp;flag,
				      &amp;rexmit);
		tcp_newly_delivered(sk, delivered, flag);
		tcp_xmit_recovery(sk, rexmit);
	}

	return 0;
}
<br>tcp_cong_control() 을 실행한다.<br><a data-href="tcp_cong_control()" href="encyclopedia-of-networksystem/function/net-ipv4/tcp_cong_control().html" class="internal-link" target="_self" rel="noopener nofollow">tcp_cong_control()</a>]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/tcp_ack().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_ack().md</guid><pubDate>Tue, 08 Oct 2024 08:24:56 GMT</pubDate></item><item><title><![CDATA[tcp_add_backlog()]]></title><description><![CDATA[ 
 <br>bool tcp_add_backlog(struct sock *sk, struct sk_buff *skb,
		     enum skb_drop_reason *reason)
{
	u32 tail_gso_size, tail_gso_segs;
	struct skb_shared_info *shinfo;
	const struct tcphdr *th;
	struct tcphdr *thtail;
	struct sk_buff *tail;
	unsigned int hdrlen;
	bool fragstolen;
	u32 gso_segs;
	u32 gso_size;
	u64 limit;
	int delta;

	/* In case all data was pulled from skb frags (in __pskb_pull_tail()),
	 * we can fix skb-&gt;truesize to its real value to avoid future drops.
	 * This is valid because skb is not yet charged to the socket.
	 * It has been noticed pure SACK packets were sometimes dropped
	 * (if cooked by drivers without copybreak feature).
	 */
	skb_condense(skb);

	skb_dst_drop(skb);

	if (unlikely(tcp_checksum_complete(skb))) {
		bh_unlock_sock(sk);
		trace_tcp_bad_csum(skb);
		*reason = SKB_DROP_REASON_TCP_CSUM;
		__TCP_INC_STATS(sock_net(sk), TCP_MIB_CSUMERRORS);
		__TCP_INC_STATS(sock_net(sk), TCP_MIB_INERRS);
		return true;
	}

	/* Attempt coalescing to last skb in backlog, even if we are
	 * above the limits.
	 * This is okay because skb capacity is limited to MAX_SKB_FRAGS.
	 */
	th = (const struct tcphdr *)skb-&gt;data;
	hdrlen = th-&gt;doff * 4;

	tail = sk-&gt;sk_backlog.tail;
	if (!tail)
		goto no_coalesce;
	thtail = (struct tcphdr *)tail-&gt;data;

	if (TCP_SKB_CB(tail)-&gt;end_seq != TCP_SKB_CB(skb)-&gt;seq ||
	    TCP_SKB_CB(tail)-&gt;ip_dsfield != TCP_SKB_CB(skb)-&gt;ip_dsfield ||
	    ((TCP_SKB_CB(tail)-&gt;tcp_flags |
	      TCP_SKB_CB(skb)-&gt;tcp_flags) &amp; (TCPHDR_SYN | TCPHDR_RST | TCPHDR_URG)) ||
	    !((TCP_SKB_CB(tail)-&gt;tcp_flags &amp;
	      TCP_SKB_CB(skb)-&gt;tcp_flags) &amp; TCPHDR_ACK) ||
	    ((TCP_SKB_CB(tail)-&gt;tcp_flags ^
	      TCP_SKB_CB(skb)-&gt;tcp_flags) &amp; (TCPHDR_ECE | TCPHDR_CWR)) ||
#ifdef CONFIG_TLS_DEVICE
	    tail-&gt;decrypted != skb-&gt;decrypted ||
#endif
	    !mptcp_skb_can_collapse(tail, skb) ||
	    thtail-&gt;doff != th-&gt;doff ||
	    memcmp(thtail + 1, th + 1, hdrlen - sizeof(*th)))
		goto no_coalesce;

	__skb_pull(skb, hdrlen);

	shinfo = skb_shinfo(skb);
	gso_size = shinfo-&gt;gso_size ?: skb-&gt;len;
	gso_segs = shinfo-&gt;gso_segs ?: 1;

	shinfo = skb_shinfo(tail);
	tail_gso_size = shinfo-&gt;gso_size ?: (tail-&gt;len - hdrlen);
	tail_gso_segs = shinfo-&gt;gso_segs ?: 1;

	if (skb_try_coalesce(tail, skb, &amp;fragstolen, &amp;delta)) {
		TCP_SKB_CB(tail)-&gt;end_seq = TCP_SKB_CB(skb)-&gt;end_seq;

		if (likely(!before(TCP_SKB_CB(skb)-&gt;ack_seq, TCP_SKB_CB(tail)-&gt;ack_seq))) {
			TCP_SKB_CB(tail)-&gt;ack_seq = TCP_SKB_CB(skb)-&gt;ack_seq;
			thtail-&gt;window = th-&gt;window;
		}

		/* We have to update both TCP_SKB_CB(tail)-&gt;tcp_flags and
		 * thtail-&gt;fin, so that the fast path in tcp_rcv_established()
		 * is not entered if we append a packet with a FIN.
		 * SYN, RST, URG are not present.
		 * ACK is set on both packets.
		 * PSH : we do not really care in TCP stack,
		 *       at least for 'GRO' packets.
		 */
		thtail-&gt;fin |= th-&gt;fin;
		TCP_SKB_CB(tail)-&gt;tcp_flags |= TCP_SKB_CB(skb)-&gt;tcp_flags;

		if (TCP_SKB_CB(skb)-&gt;has_rxtstamp) {
			TCP_SKB_CB(tail)-&gt;has_rxtstamp = true;
			tail-&gt;tstamp = skb-&gt;tstamp;
			skb_hwtstamps(tail)-&gt;hwtstamp = skb_hwtstamps(skb)-&gt;hwtstamp;
		}

		/* Not as strict as GRO. We only need to carry mss max value */
		shinfo-&gt;gso_size = max(gso_size, tail_gso_size);
		shinfo-&gt;gso_segs = min_t(u32, gso_segs + tail_gso_segs, 0xFFFF);

		sk-&gt;sk_backlog.len += delta;
		__NET_INC_STATS(sock_net(sk),
				LINUX_MIB_TCPBACKLOGCOALESCE);
		kfree_skb_partial(skb, fragstolen);
		return false;
	}
	__skb_push(skb, hdrlen);

no_coalesce:
	/* sk-&gt;sk_backlog.len is reset only at the end of __release_sock().
	 * Both sk-&gt;sk_backlog.len and sk-&gt;sk_rmem_alloc could reach
	 * sk_rcvbuf in normal conditions.
	 */set
	limit = ((u64)READ_ONCE(sk-&gt;sk_rcvbuf)) &lt;&lt; 1;

	limit += ((u32)READ_ONCE(sk-&gt;sk_sndbuf)) &gt;&gt; 1;

	/* Only socket owner can try to collapse/prune rx queues
	 * to reduce memory overhead, so add a little headroom here.
	 * Few sockets backlog are possibly concurrently non empty.
	 */
	limit += 64 * 1024;

	limit = min_t(u64, limit, UINT_MAX);

	if (unlikely(sk_add_backlog(sk, skb, limit))) {
		bh_unlock_sock(sk);
		*reason = SKB_DROP_REASON_SOCKET_BACKLOG;
		__NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPBACKLOGDROP);
		return true;
	}
	return false;
}
<br>skb_condense() 함수에서 skb내의 fragment/ frag_list를 제거하려고 한다.<br>
frag 가 존재하고 skb의 end 와 tail을 비교해서 room이 충분히 존재할 경우<br>
fragment들을 여유 공간으로 옮겨주고 free한다.  pskb_pull_tail()<br>
이후 skb-&gt;truesize를 재조정해야한다.<br>skb들을 합치는 skb coalescing 과정을 거친다.<br>
sk backlog의 가장 마지막 skb를 가져와서 그것의 tcp header를 찾는다.<br>
마지막 skb를 최근에 들어온 skb와 합칠 수 있는지 확인해본다. seq, ip destination, flag 등을 비교.<br>
조건이 맞을 경우 skb_try_coalesce() 실행한다.<br>
condense, coalesce 를 마치고 나서 sk_add_backlog() 실행한다.<br>gso 관련 parameter가 나온다. (gso size, gso segs) coalescing 할 때 쓰이는데 좀더 찾아봐야할듯<br>
tcp_add_backlog()함수 또한 receive queue에 넣는 것과 마찬가지로 skb_try_coalesce()함수를 통해 마지막 skb에 합치거나 해당 큐에 추가하게 된다.<br>
마지막에 sk_add_backlog()의 경우 실질적으로 sk-&gt;sk_backlog.len을 늘려주고, 해당 백로그의 포인터를 이번에 합쳐진 Skb를 가르키게 옮김으로써, linked list와 관련 된 처리를 해주고 있음을 알 수 있다.
<br><a data-href="skb_condense()" href="encyclopedia-of-networksystem/function/net-ipv4/skb_condense().html" class="internal-link" target="_self" rel="noopener nofollow">skb_condense()</a><br>
<a data-href="skb_try_coalesce()" href="encyclopedia-of-networksystem/function/net-ipv4/skb_try_coalesce().html" class="internal-link" target="_self" rel="noopener nofollow">skb_try_coalesce()</a><br>
<a data-href="sk_add_backlog()" href="encyclopedia-of-networksystem/function/net-ipv4/sk_add_backlog().html" class="internal-link" target="_self" rel="noopener nofollow">sk_add_backlog()</a>]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/tcp_add_backlog().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_add_backlog().md</guid><pubDate>Wed, 06 Nov 2024 13:41:30 GMT</pubDate></item><item><title><![CDATA[tcp_check_req()]]></title><description><![CDATA[ 
 ]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/tcp_check_req().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_check_req().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[tcp_cong_control()]]></title><description><![CDATA[ 
 <br>/* The "ultimate" congestion control function that aims to replace the rigid
 * cwnd increase and decrease control (tcp_cong_avoid,tcp_*cwnd_reduction).
 * It's called toward the end of processing an ACK with precise rate
 * information. All transmission or retransmission are delayed afterwards.
 */
static void tcp_cong_control(struct sock *sk, u32 ack, u32 acked_sacked,
			     int flag, const struct rate_sample *rs)
{
	const struct inet_connection_sock *icsk = inet_csk(sk);

	if (icsk-&gt;icsk_ca_ops-&gt;cong_control) {
		icsk-&gt;icsk_ca_ops-&gt;cong_control(sk, rs);
		return;
	}

	if (tcp_in_cwnd_reduction(sk)) {
		/* Reduce cwnd if state mandates */
		tcp_cwnd_reduction(sk, acked_sacked, rs-&gt;losses, flag);
	} else if (tcp_may_raise_cwnd(sk, flag)) {
		/* Advance cwnd if state allows */
		tcp_cong_avoid(sk, ack, acked_sacked);
	}
	tcp_update_pacing_rate(sk);
}
<br>tcp_cwnd_reduction()<br>
tcp_cong_avoid()]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/tcp_cong_control().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_cong_control().md</guid><pubDate>Tue, 08 Oct 2024 08:24:56 GMT</pubDate></item><item><title><![CDATA[tcp_data_queue_ofo()]]></title><description><![CDATA[ 
 <br>static void tcp_data_queue_ofo(struct sock *sk, struct sk_buff *skb)
{
	struct tcp_sock *tp = tcp_sk(sk);
	struct rb_node **p, *parent;
	struct sk_buff *skb1;
	u32 seq, end_seq;
	bool fragstolen;

	tcp_save_lrcv_flowlabel(sk, skb);
	tcp_ecn_check_ce(sk, skb);

	if (unlikely(tcp_try_rmem_schedule(sk, skb, skb-&gt;truesize))) {
		NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPOFODROP);
		sk-&gt;sk_data_ready(sk);
		tcp_drop_reason(sk, skb, SKB_DROP_REASON_PROTO_MEM);
		return;
	}

	/* Disable header prediction. */
	tp-&gt;pred_flags = 0;
	inet_csk_schedule_ack(sk);

	tp-&gt;rcv_ooopack += max_t(u16, 1, skb_shinfo(skb)-&gt;gso_segs);
	NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPOFOQUEUE);
	seq = TCP_SKB_CB(skb)-&gt;seq;
	end_seq = TCP_SKB_CB(skb)-&gt;end_seq;

	p = &amp;tp-&gt;out_of_order_queue.rb_node;
	if (RB_EMPTY_ROOT(&amp;tp-&gt;out_of_order_queue)) {
		/* Initial out of order segment, build 1 SACK. */
		if (tcp_is_sack(tp)) {
			tp-&gt;rx_opt.num_sacks = 1;
			tp-&gt;selective_acks[0].start_seq = seq;
			tp-&gt;selective_acks[0].end_seq = end_seq;
		}
		rb_link_node(&amp;skb-&gt;rbnode, NULL, p);
		rb_insert_color(&amp;skb-&gt;rbnode, &amp;tp-&gt;out_of_order_queue);
		tp-&gt;ooo_last_skb = skb;
		goto end;
	}

	/* In the typical case, we are adding an skb to the end of the list.
	 * Use of ooo_last_skb avoids the O(Log(N)) rbtree lookup.
	 */
	if (tcp_ooo_try_coalesce(sk, tp-&gt;ooo_last_skb,
				 skb, &amp;fragstolen)) {
coalesce_done:
		/* For non sack flows, do not grow window to force DUPACK
		 * and trigger fast retransmit.
		 */
		if (tcp_is_sack(tp))
			tcp_grow_window(sk, skb, true);
		kfree_skb_partial(skb, fragstolen);
		skb = NULL;
		goto add_sack;
	}
	/* Can avoid an rbtree lookup if we are adding skb after ooo_last_skb */
	if (!before(seq, TCP_SKB_CB(tp-&gt;ooo_last_skb)-&gt;end_seq)) {
		parent = &amp;tp-&gt;ooo_last_skb-&gt;rbnode;
		p = &amp;parent-&gt;rb_right;
		goto insert;
	}

	/* Find place to insert this segment. Handle overlaps on the way. */
	parent = NULL;
	while (*p) {
		parent = *p;
		skb1 = rb_to_skb(parent);
		if (before(seq, TCP_SKB_CB(skb1)-&gt;seq)) {
			p = &amp;parent-&gt;rb_left;
			continue;
		}
		if (before(seq, TCP_SKB_CB(skb1)-&gt;end_seq)) {
			if (!after(end_seq, TCP_SKB_CB(skb1)-&gt;end_seq)) {
				/* All the bits are present. Drop. */
				NET_INC_STATS(sock_net(sk),
					      LINUX_MIB_TCPOFOMERGE);
				tcp_drop_reason(sk, skb,
						SKB_DROP_REASON_TCP_OFOMERGE);
				skb = NULL;
				tcp_dsack_set(sk, seq, end_seq);
				goto add_sack;
			}
			if (after(seq, TCP_SKB_CB(skb1)-&gt;seq)) {
				/* Partial overlap. */
				tcp_dsack_set(sk, seq, TCP_SKB_CB(skb1)-&gt;end_seq);
			} else {
				/* skb's seq == skb1's seq and skb covers skb1.
				 * Replace skb1 with skb.
				 */
				rb_replace_node(&amp;skb1-&gt;rbnode, &amp;skb-&gt;rbnode,
						&amp;tp-&gt;out_of_order_queue);
				tcp_dsack_extend(sk,
						 TCP_SKB_CB(skb1)-&gt;seq,
						 TCP_SKB_CB(skb1)-&gt;end_seq);
				NET_INC_STATS(sock_net(sk),
					      LINUX_MIB_TCPOFOMERGE);
				tcp_drop_reason(sk, skb1,
						SKB_DROP_REASON_TCP_OFOMERGE);
				goto merge_right;
			}
		} else if (tcp_ooo_try_coalesce(sk, skb1,
						skb, &amp;fragstolen)) {
			goto coalesce_done;
		}
		p = &amp;parent-&gt;rb_right;
	}
insert:
	/* Insert segment into RB tree. */
	rb_link_node(&amp;skb-&gt;rbnode, parent, p);
	rb_insert_color(&amp;skb-&gt;rbnode, &amp;tp-&gt;out_of_order_queue);

merge_right:
	/* Remove other segments covered by skb. */
	while ((skb1 = skb_rb_next(skb)) != NULL) {
		if (!after(end_seq, TCP_SKB_CB(skb1)-&gt;seq))
			break;
		if (before(end_seq, TCP_SKB_CB(skb1)-&gt;end_seq)) {
			tcp_dsack_extend(sk, TCP_SKB_CB(skb1)-&gt;seq,
					 end_seq);
			break;
		}
		rb_erase(&amp;skb1-&gt;rbnode, &amp;tp-&gt;out_of_order_queue);
		tcp_dsack_extend(sk, TCP_SKB_CB(skb1)-&gt;seq,
				 TCP_SKB_CB(skb1)-&gt;end_seq);
		NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPOFOMERGE);
		tcp_drop_reason(sk, skb1, SKB_DROP_REASON_TCP_OFOMERGE);
	}
	/* If there is no skb after us, we are the last_skb ! */
	if (!skb1)
		tp-&gt;ooo_last_skb = skb;

add_sack:
	if (tcp_is_sack(tp))
		tcp_sack_new_ofo_skb(sk, seq, end_seq);
end:
	if (skb) {
		/* For non sack flows, do not grow window to force DUPACK
		 * and trigger fast retransmit.
		 */
		if (tcp_is_sack(tp))
			tcp_grow_window(sk, skb, false);
		skb_condense(skb);
		skb_set_owner_r(skb, sk);
	}
}
<br>
정상 receive_queue와 매우 흡사하다. tcp_ooo_try_coalesce()함수를 통해 ooo 패킷을 큐에 집어넣게 된다. 여기서는 ooo 패킷 큐를 관리하기 위해 red-black node 데이터 구조를 사용하고 있었다.
]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/tcp_data_queue_ofo().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_data_queue_ofo().md</guid><pubDate>Wed, 06 Nov 2024 13:41:30 GMT</pubDate></item><item><title><![CDATA[tcp_data_queue()]]></title><description><![CDATA[ 
 <br>static void tcp_data_queue(struct sock *sk, struct sk_buff *skb)
{
	struct tcp_sock *tp = tcp_sk(sk);
	enum skb_drop_reason reason;
	bool fragstolen;
	int eaten;

	/* If a subflow has been reset, the packet should not continue
	 * to be processed, drop the packet.
	 */
	if (sk_is_mptcp(sk) &amp;&amp; !mptcp_incoming_options(sk, skb)) {
		__kfree_skb(skb);
		return;
	}

	if (TCP_SKB_CB(skb)-&gt;seq == TCP_SKB_CB(skb)-&gt;end_seq) {
		__kfree_skb(skb);
		return;
	}
	skb_dst_drop(skb);
	__skb_pull(skb, tcp_hdr(skb)-&gt;doff * 4);

	reason = SKB_DROP_REASON_NOT_SPECIFIED;
	tp-&gt;rx_opt.dsack = 0;

	/*  Queue data for delivery to the user.
	 *  Packets in sequence go to the receive queue.
	 *  Out of sequence packets to the out_of_order_queue.
	 */
	if (TCP_SKB_CB(skb)-&gt;seq == tp-&gt;rcv_nxt) {
		if (tcp_receive_window(tp) == 0) {
			reason = SKB_DROP_REASON_TCP_ZEROWINDOW;
			NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPZEROWINDOWDROP);
			goto out_of_window;
		}

		/* Ok. In sequence. In window. */
queue_and_out:
		if (tcp_try_rmem_schedule(sk, skb, skb-&gt;truesize)) {
			/* TODO: maybe ratelimit these WIN 0 ACK ? */
			inet_csk(sk)-&gt;icsk_ack.pending |=
					(ICSK_ACK_NOMEM | ICSK_ACK_NOW);
			inet_csk_schedule_ack(sk);
			sk-&gt;sk_data_ready(sk);

			if (skb_queue_len(&amp;sk-&gt;sk_receive_queue)) {
				reason = SKB_DROP_REASON_PROTO_MEM;
				NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPRCVQDROP);
				goto drop;
			}
			sk_forced_mem_schedule(sk, skb-&gt;truesize);
		}

		eaten = tcp_queue_rcv(sk, skb, &amp;fragstolen);
		if (skb-&gt;len)
			tcp_event_data_recv(sk, skb);
		if (TCP_SKB_CB(skb)-&gt;tcp_flags &amp; TCPHDR_FIN)
			tcp_fin(sk);

		if (!RB_EMPTY_ROOT(&amp;tp-&gt;out_of_order_queue)) {
			tcp_ofo_queue(sk);

			/* RFC5681. 4.2. SHOULD send immediate ACK, when
			 * gap in queue is filled.
			 */
			if (RB_EMPTY_ROOT(&amp;tp-&gt;out_of_order_queue))
				inet_csk(sk)-&gt;icsk_ack.pending |= ICSK_ACK_NOW;
		}

		if (tp-&gt;rx_opt.num_sacks)
			tcp_sack_remove(tp);

		tcp_fast_path_check(sk);

		if (eaten &gt; 0)
			kfree_skb_partial(skb, fragstolen);
		if (!sock_flag(sk, SOCK_DEAD))
			tcp_data_ready(sk);
		return;
	}

	if (!after(TCP_SKB_CB(skb)-&gt;end_seq, tp-&gt;rcv_nxt)) {
		tcp_rcv_spurious_retrans(sk, skb);
		/* A retransmit, 2nd most common case.  Force an immediate ack. */
		reason = SKB_DROP_REASON_TCP_OLD_DATA;
		NET_INC_STATS(sock_net(sk), LINUX_MIB_DELAYEDACKLOST);
		tcp_dsack_set(sk, TCP_SKB_CB(skb)-&gt;seq, TCP_SKB_CB(skb)-&gt;end_seq);

out_of_window:
		tcp_enter_quickack_mode(sk, TCP_MAX_QUICKACKS);
		inet_csk_schedule_ack(sk);
drop:
		tcp_drop_reason(sk, skb, reason);
		return;
	}

	/* Out of window. F.e. zero window probe. */
	if (!before(TCP_SKB_CB(skb)-&gt;seq,
		    tp-&gt;rcv_nxt + tcp_receive_window(tp))) {
		reason = SKB_DROP_REASON_TCP_OVERWINDOW;
		goto out_of_window;
	}

	if (before(TCP_SKB_CB(skb)-&gt;seq, tp-&gt;rcv_nxt)) {
		/* Partial packet, seq &lt; rcv_next &lt; end_seq */
		tcp_dsack_set(sk, TCP_SKB_CB(skb)-&gt;seq, tp-&gt;rcv_nxt);

		/* If window is closed, drop tail of packet. But after
		 * remembering D-SACK for its head made in previous line.
		 */
		if (!tcp_receive_window(tp)) {
			reason = SKB_DROP_REASON_TCP_ZEROWINDOW;
			NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPZEROWINDOWDROP);
			goto out_of_window;
		}
		goto queue_and_out;
	}

	tcp_data_queue_ofo(sk, skb);
}
<br>if (TCP_SKB_CB(skb)-&gt;seq == tp-&gt;rcv_nxt) { in sequence인 경우<br>
tcp_receive_window의 크기를 체크하고 0이면 pkt을 drop한다.<br>
in sequnece, in window인 경우 tcp_try_rmem_schedule() 로 공간을 할당한다?????<br>
이후 tcp_queue_rcv으로 sk receive queue로 보낸다.<br>@sk_data_ready: callback to indicate there is data to be processed<br>out of sequence인 경우,  tcp data queue ofo() 함수를 호출한다.<br>
먼저 dst_entry를 드랍하고, 패킷을 receive queue에 넣게 된다. 만약 out of order 패킷이 있다면 out_of_order_queue에 집어 넣는다.<br>
우선 현재 수신한 패킷의 seq_num이 해당 소켓이 받아야할 다음 번호와 일치한다면, zero window를 먼저 체크하고, queue_and_out라벨로 넘어가게 된다.
먼저 메모리 재스케줄이 필요한 경우를 확인하는데, 메모리가 부족한 경우 receive queue와 ofo receive queue를 정리하여 재시도 하게 된다. 계속 부족하면 패킷을 드랍하게 된다.
이후는 fast path와 같다. tcp_queue_rcv를 호출하여 receive queue에다가 집어 넣게 된다.<br>
만약 ofo 인 경우, tcp_data_queue_ofo()함수를 호출하게 된다.
<br><a data-href="tcp_data_queue_ofo()" href="encyclopedia-of-networksystem/function/net-ipv4/tcp_data_queue_ofo().html" class="internal-link" target="_self" rel="noopener nofollow">tcp_data_queue_ofo()</a>]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/tcp_data_queue().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_data_queue().md</guid><pubDate>Wed, 06 Nov 2024 13:41:30 GMT</pubDate></item><item><title><![CDATA[tcp_data_ready()]]></title><description><![CDATA[ 
 <br>void tcp_data_ready(struct sock *sk)
{
	if (tcp_epollin_ready(sk, sk-&gt;sk_rcvlowat) || sock_flag(sk, SOCK_DONE))
		sk-&gt;sk_data_ready(sk);
}
<br>sk_data_ready() 함수는 socket내부에 존재하는 함수이다.<br>
tcp_epollin_ready() 함수로 epoll이 가능한지 확인해본다.<br>
socket안에 처리해야할 데이터가 있음을 알려주는 callback 함수다. <br>
sk-&gt;sk_data_ready는 함수포인터이다. 대부분의 소켓들이 어떻게 초기화 되는지 확인해보았을 때, net/core/sock.c에서 sock_init_data_uid()라는 함수에서 sock_def_readable()로 초기화 되고 있는 것을 볼 수 있었다.<br>
이 함수는 똑같은 파일에 있으며, rcu lock을 획득하고, 소켓의 sk_wq를 가져오게 된다.<br>
이를 바탕으로 skwq_has_sleeper()함수를 실행하게 되는데, 이 함수는 만약 기다리고 있는 프로세스가 있는지 확인하는 함수이다. 만약 받은 socket_wq가 waiting processes를 가지고 있다면 true를 반환하게 된다. 이후 wake_up_interruptibal_sync_poll()함수를 실행하게 된다.<br>
그리고 난 다음에 sk_wake_async()함수를 실행하게 되고, rcu lock을 해제하게 된다.
<br>sk_data_ready() -&gt; <a data-href="sock_def_readable()" href="sock_def_readable().html" class="internal-link" target="_self" rel="noopener nofollow">sock_def_readable()</a><br>The&nbsp;sk_data_ready&nbsp;function points to the&nbsp;sock_def_readable&nbsp;function, which calls&nbsp;wake_up_interruptible_sync_poll&nbsp;to wake up the process that is waiting in&nbsp;ep_wait. The&nbsp;ep_wait&nbsp;then delivers epoll events and returns to userspace, which can then call&nbsp;recv&nbsp;system call, for example, to obtain new data that was packed in socket buffer<br>
@skdata_ready: callback to indicate there is data to be processed|<br>
Check if we need to signal EPOLLIN right now */<br><a data-href="sock_def_readable()" href="sock_def_readable().html" class="internal-link" target="_self" rel="noopener nofollow">sock_def_readable()</a>]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/tcp_data_ready().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_data_ready().md</guid><pubDate>Wed, 06 Nov 2024 13:41:30 GMT</pubDate></item><item><title><![CDATA[tcp_epollin_ready()]]></title><description><![CDATA[ 
 ]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/tcp_epollin_ready().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_epollin_ready().md</guid><pubDate>Thu, 10 Oct 2024 11:46:39 GMT</pubDate></item><item><title><![CDATA[tcp_filter()]]></title><description><![CDATA[ 
 <br>int tcp_filter(struct sock *sk, struct sk_buff *skb)
{
    struct tcphdr *th = (struct tcphdr *)skb-&gt;data;
  
    return sk_filter_trim_cap(sk, skb, th-&gt;doff * 4);
}
<br>
tcp 헤더를 가져와서 sk_filter_trim_cap()함수의 return값을 반환하게 된다. 자세한 역할은 tcp_v4_rcv()의 본문에 포함되어 있다.
]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/tcp_filter().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_filter().md</guid><pubDate>Tue, 08 Oct 2024 08:24:56 GMT</pubDate></item><item><title><![CDATA[tcp_gro_complete()]]></title><description><![CDATA[ 
 <br>void tcp_gro_complete(struct sk_buff *skb)
{
	struct tcphdr *th = tcp_hdr(skb);
	struct skb_shared_info *shinfo;

	if (skb-&gt;encapsulation)
		skb-&gt;inner_transport_header = skb-&gt;transport_header;

	skb-&gt;csum_start = (unsigned char *)th - skb-&gt;head;
	skb-&gt;csum_offset = offsetof(struct tcphdr, check);
	skb-&gt;ip_summed = CHECKSUM_PARTIAL;

	shinfo = skb_shinfo(skb);
	shinfo-&gt;gso_segs = NAPI_GRO_CB(skb)-&gt;count;

	if (th-&gt;cwr)
		shinfo-&gt;gso_type |= SKB_GSO_TCP_ECN;
}
<br>
skb와 shinfo에 대하여 gro가 완료되었을 때를 기준으로 정보를 업데이트 해주는 함수이다. 따로 반환값은 존재하지 않았다.
]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/tcp_gro_complete().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_gro_complete().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[tcp_gro_receive()]]></title><description><![CDATA[ 
 <br>struct sk_buff *tcp_gro_receive(struct list_head *head, struct sk_buff *skb)
{
	struct sk_buff *pp = NULL;
	struct sk_buff *p;
	struct tcphdr *th;
	struct tcphdr *th2;
	unsigned int len;
	unsigned int thlen;
	__be32 flags;
	unsigned int mss = 1;
	unsigned int hlen;
	unsigned int off;
	int flush = 1;
	int i;

	off = skb_gro_offset(skb);
	hlen = off + sizeof(*th);
	th = skb_gro_header(skb, hlen, off);
	if (unlikely(!th))
		goto out;

	thlen = th-&gt;doff * 4;
	if (thlen &lt; sizeof(*th))
		goto out;

	hlen = off + thlen;
	if (!skb_gro_may_pull(skb, hlen)) {
		th = skb_gro_header_slow(skb, hlen, off);
		if (unlikely(!th))
			goto out;
	}

	skb_gro_pull(skb, thlen);

	len = skb_gro_len(skb);
	flags = tcp_flag_word(th);

	list_for_each_entry(p, head, list) {
		if (!NAPI_GRO_CB(p)-&gt;same_flow)
			continue;

		th2 = tcp_hdr(p);

		if (*(u32 *)&amp;th-&gt;source ^ *(u32 *)&amp;th2-&gt;source) {
			NAPI_GRO_CB(p)-&gt;same_flow = 0;
			continue;
		}

		goto found;
	}
	p = NULL;
	goto out_check_final;

found:
	/* Include the IP ID check below from the inner most IP hdr */
	flush = NAPI_GRO_CB(p)-&gt;flush;
	flush |= (__force int)(flags &amp; TCP_FLAG_CWR);
	flush |= (__force int)((flags ^ tcp_flag_word(th2)) &amp;
		  ~(TCP_FLAG_CWR | TCP_FLAG_FIN | TCP_FLAG_PSH));
	flush |= (__force int)(th-&gt;ack_seq ^ th2-&gt;ack_seq);
	for (i = sizeof(*th); i &lt; thlen; i += 4)
		flush |= *(u32 *)((u8 *)th + i) ^
			 *(u32 *)((u8 *)th2 + i);

	/* When we receive our second frame we can made a decision on if we
	 * continue this flow as an atomic flow with a fixed ID or if we use
	 * an incrementing ID.
	 */
	if (NAPI_GRO_CB(p)-&gt;flush_id != 1 ||
	    NAPI_GRO_CB(p)-&gt;count != 1 ||
	    !NAPI_GRO_CB(p)-&gt;is_atomic)
		flush |= NAPI_GRO_CB(p)-&gt;flush_id;
	else
		NAPI_GRO_CB(p)-&gt;is_atomic = false;

	mss = skb_shinfo(p)-&gt;gso_size;

	/* If skb is a GRO packet, make sure its gso_size matches prior packet mss.
	 * If it is a single frame, do not aggregate it if its length
	 * is bigger than our mss.
	 */
	if (unlikely(skb_is_gso(skb)))
		flush |= (mss != skb_shinfo(skb)-&gt;gso_size);
	else
		flush |= (len - 1) &gt;= mss;

	flush |= (ntohl(th2-&gt;seq) + skb_gro_len(p)) ^ ntohl(th-&gt;seq);
#ifdef CONFIG_TLS_DEVICE
	flush |= p-&gt;decrypted ^ skb-&gt;decrypted;
#endif

	if (flush || skb_gro_receive(p, skb)) { // [[Encyclopedia of NetworkSystem/Function/net-core/skb_gro_receive().md|skb_gro_receive()]]
		mss = 1;
		goto out_check_final;
	}

	tcp_flag_word(th2) |= flags &amp; (TCP_FLAG_FIN | TCP_FLAG_PSH);

out_check_final:
	/* Force a flush if last segment is smaller than mss. */
	if (unlikely(skb_is_gso(skb)))
		flush = len != NAPI_GRO_CB(skb)-&gt;count * skb_shinfo(skb)-&gt;gso_size;
	else
		flush = len &lt; mss;

	flush |= (__force int)(flags &amp; (TCP_FLAG_URG | TCP_FLAG_PSH |
					TCP_FLAG_RST | TCP_FLAG_SYN |
					TCP_FLAG_FIN));

	if (p &amp;&amp; (!NAPI_GRO_CB(skb)-&gt;same_flow || flush))
		pp = p;

out:
	NAPI_GRO_CB(skb)-&gt;flush |= (flush != 0);

	return pp;
}

<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-core/skb_gro_receive().md" data-href="Encyclopedia of NetworkSystem/Function/net-core/skb_gro_receive().md" href="encyclopedia-of-networksystem/function/net-core/skb_gro_receive().html" class="internal-link" target="_self" rel="noopener nofollow">skb_gro_receive()</a><br>
L4에서 같은 flow인지 확인하는 부분이 들어있다. 함께 들어온 gro_list의 각각의 패킷들에 대하여 같은 포트에서 온건지 확인하여 같은 포트인것이 확인 되면, 바로 found label로 가게 된다. 해당하는 패킷과 찾은 skb를 병합하기 위해 skb_gro_receive()를 실행하게 된다.
]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/tcp_gro_receive().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_gro_receive().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[tcp_queue_rcv()]]></title><description><![CDATA[ 
 <br>static int __must_check tcp_queue_rcv(struct sock *sk, struct sk_buff *skb,
				      bool *fragstolen)
{
	int eaten;
	struct sk_buff *tail = skb_peek_tail(&amp;sk-&gt;sk_receive_queue);

	eaten = (tail &amp;&amp;
		 tcp_try_coalesce(sk, tail,
				  skb, fragstolen)) ? 1 : 0;
	tcp_rcv_nxt_update(tcp_sk(sk), TCP_SKB_CB(skb)-&gt;end_seq);
	if (!eaten) {
		__skb_queue_tail(&amp;sk-&gt;sk_receive_queue, skb);
		skb_set_owner_r(skb, sk);
	}
	return eaten;
}
<br>tcp_try_coalesce()를 시도한다. 성공할 경우 리턴하고<br>
__skb_queue_tail()로 sk_receive_queue에 skb를 추가한다<br>tcp_try_coalesce()도 추가적인 조건을 확인하고 skb_try_coalesce()를 실행한다. <br>
return 할 eaten 변수를 선언하고, sk_buff타입의 tail 변수를 선언하게 되는데, 이는 해당 소켓의 sk_receive_queue에서 마지막 skb를 가져오는 것이다.이때, 이 마지막 skb와 새롭게 들어온 skb를 합치기 위해 tcp_try_coalesce()함수를 호출하게 된다. 만약 합치지 못하였다면, 해당 소켓의 sk_receive_queue에다가 그 skb를 새로 추가하게 되고, skb_set_owner_r()함수를 호출하게 된다.
이후 eaten을 리턴하게 된다. 
<br><a data-href="tcp_try_coalesce()" href="encyclopedia-of-networksystem/function/net-ipv4/tcp_try_coalesce().html" class="internal-link" target="_self" rel="noopener nofollow">tcp_try_coalesce()</a>]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/tcp_queue_rcv().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_queue_rcv().md</guid><pubDate>Wed, 06 Nov 2024 13:41:30 GMT</pubDate></item><item><title><![CDATA[tcp_rcv_established()]]></title><description><![CDATA[ 
 <br>/*
* TCP receive function for the ESTABLISHED state.
*
* It is split into a fast path and a slow path. The fast path is
* disabled when:
* - A zero window was announced from us - zero window probing
* is only handled properly in the slow path.
* - Out of order segments arrived.
* - Urgent data is expected.
* - There is no buffer space left
* - Unexpected TCP flags/window values/header lengths are received
* (detected by checking the TCP header against pred_flags)
* - Data is sent in both directions. Fast path only supports pure senders
* or pure receivers (this means either the sequence number or the ack
* value must stay constant)
* - Unexpected TCP option.
*
* When these conditions are not satisfied it drops into a standard
* receive procedure patterned after RFC793 to handle all cases.
* The first three cases are guaranteed by proper pred_flags setting,
* the rest is checked inline. Fast processing is turned on in
* tcp_data_queue when everything is OK.
*/
void tcp_rcv_established(struct sock *sk, struct sk_buff *skb)
{
	enum skb_drop_reason reason = SKB_DROP_REASON_NOT_SPECIFIED;
	const struct tcphdr *th = (const struct tcphdr *)skb-&gt;data;
	struct tcp_sock *tp = tcp_sk(sk);
	unsigned int len = skb-&gt;len;
	  
	/* TCP congestion window tracking */
	trace_tcp_probe(sk, skb);
	  
	tcp_mstamp_refresh(tp);
	if (unlikely(!rcu_access_pointer(sk-&gt;sk_rx_dst)))
		inet_csk(sk)-&gt;icsk_af_ops-&gt;sk_rx_dst_set(sk, skb);
	/*
	* Header prediction.
	* The code loosely follows the one in the famous
	* "30 instruction TCP receive" Van Jacobson mail.
	*
	* Van's trick is to deposit buffers into socket queue
	* on a device interrupt, to call tcp_recv function
	* on the receive process context and checksum and copy
	* the buffer to user space. smart...
	*
	* Our current scheme is not silly either but we take the
	* extra cost of the net_bh soft interrupt processing...
	* We do checksum and copy also but from device to kernel.
	*/
	  
	tp-&gt;rx_opt.saw_tstamp = 0;
	  
	/* pred_flags is 0xS?10 &lt;&lt; 16 + snd_wnd
	* if header_prediction is to be made
	* 'S' will always be tp-&gt;tcp_header_len &gt;&gt; 2
	* '?' will be 0 for the fast path, otherwise pred_flags is 0 to
	* turn it off (when there are holes in the receive
	* space for instance)
	* PSH flag is ignored.
	*/
	  
	if ((tcp_flag_word(th) &amp; TCP_HP_BITS) == tp-&gt;pred_flags &amp;&amp;
		TCP_SKB_CB(skb)-&gt;seq == tp-&gt;rcv_nxt &amp;&amp;
		!after(TCP_SKB_CB(skb)-&gt;ack_seq, tp-&gt;snd_nxt)) {
		int tcp_header_len = tp-&gt;tcp_header_len;
		  
		/* Timestamp header prediction: tcp_header_len
		* is automatically equal to th-&gt;doff*4 due to pred_flags
		* match.
		*/
		  
		/* Check timestamp */
		if (tcp_header_len == sizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED) {
			/* No? Slow path! */
			if (!tcp_parse_aligned_timestamp(tp, th))
			goto slow_path;
			  
			/* If PAWS failed, check it more carefully in slow path */
			if ((s32)(tp-&gt;rx_opt.rcv_tsval - tp-&gt;rx_opt.ts_recent) &lt; 0)
			goto slow_path;
			  
			/* DO NOT update ts_recent here, if checksum fails
			* and timestamp was corrupted part, it will result
			* in a hung connection since we will drop all
			* future packets due to the PAWS test.
			*/
		}
	  
		if (len &lt;= tcp_header_len) {
		/* Bulk data transfer: sender */
			if (len == tcp_header_len) {
				/* Predicted packet is in window by definition.
				* seq == rcv_nxt and rcv_wup &lt;= rcv_nxt.
				* Hence, check seq&lt;=rcv_wup reduces to:
				*/
				if (tcp_header_len ==
					(sizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED) &amp;&amp;
					tp-&gt;rcv_nxt == tp-&gt;rcv_wup)
					tcp_store_ts_recent(tp);
		  
				/* We know that such packets are checksummed
				* on entry.
				*/
				tcp_ack(sk, skb, 0);
				__kfree_skb(skb);
				tcp_data_snd_check(sk);
				/* When receiving pure ack in fast path, update
				* last ts ecr directly instead of calling
				* tcp_rcv_rtt_measure_ts()
				*/
				tp-&gt;rcv_rtt_last_tsecr = tp-&gt;rx_opt.rcv_tsecr;
				return;
			} else { /* Header too small */
				reason = SKB_DROP_REASON_PKT_TOO_SMALL;
				TCP_INC_STATS(sock_net(sk), TCP_MIB_INERRS);
				goto discard;
			}
		} else {
			int eaten = 0;
			bool fragstolen = false;
			  
			if (tcp_checksum_complete(skb))
				goto csum_error;
			  
			if ((int)skb-&gt;truesize &gt; sk-&gt;sk_forward_alloc
				goto step5;
			  
			/* Predicted packet is in window by definition.
			* seq == rcv_nxt and rcv_wup &lt;= rcv_nxt.
			* Hence, check seq&lt;=rcv_wup reduces to:
			*/
			if (tcp_header_len ==
				(sizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED) &amp;&amp;
				tp-&gt;rcv_nxt == tp-&gt;rcv_wup)
				tcp_store_ts_recent(tp);
			  
			tcp_rcv_rtt_measure_ts(sk, skb);
			  
			NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPHPHITS);
			  
			/* Bulk data transfer: receiver */
			skb_dst_drop(skb);
			__skb_pull(skb, tcp_header_len);
			eaten = tcp_queue_rcv(sk, skb, &amp;fragstolen);
			  
			tcp_event_data_recv(sk, skb);
			  
			if (TCP_SKB_CB(skb)-&gt;ack_seq != tp-&gt;snd_una) {
				/* Well, only one small jumplet in fast path... */
				tcp_ack(sk, skb, FLAG_DATA);
				tcp_data_snd_check(sk);
				if (!inet_csk_ack_scheduled(sk))
					goto no_ack;
			} else {
				tcp_update_wl(tp, TCP_SKB_CB(skb)-&gt;seq);
			}
  
			__tcp_ack_snd_check(sk, 0);
no_ack:
			if (eaten)
				kfree_skb_partial(skb, fragstolen);
			tcp_data_ready(sk);
			return;
		}
	}
  
slow_path:
	if (len &lt; (th-&gt;doff &lt;&lt; 2) || tcp_checksum_complete(skb))
		goto csum_error;
	  
	if (!th-&gt;ack &amp;&amp; !th-&gt;rst &amp;&amp; !th-&gt;syn) {
		reason = SKB_DROP_REASON_TCP_FLAGS;
		goto discard;
	}
	  
	/*
	* Standard slow path.
	*/
	  
	if (!tcp_validate_incoming(sk, skb, th, 1))
		return;
	  
step5:
	reason = tcp_ack(sk, skb, FLAG_SLOWPATH | FLAG_UPDATE_TS_RECENT);
	if ((int)reason &lt; 0) {
		reason = -reason;
		goto discard;
	}
	tcp_rcv_rtt_measure_ts(sk, skb);
	  
	/* Process urgent data. */
	tcp_urg(sk, skb, th);
	  
	/* step 7: process the segment text */
	tcp_data_queue(sk, skb);
	  
	tcp_data_snd_check(sk);
	tcp_ack_snd_check(sk);
	return;
  
csum_error:
	reason = SKB_DROP_REASON_TCP_CSUM;
	trace_tcp_bad_csum(skb);
	TCP_INC_STATS(sock_net(sk), TCP_MIB_CSUMERRORS);
	TCP_INC_STATS(sock_net(sk), TCP_MIB_INERRS);
  
discard:
	tcp_drop_reason(sk, skb, reason);
}
<br>
맨 처음에 skb-&gt;data를 캐스팅하여 tcp header를 얻게 된다. 또한, 주어진 소켓을 통해 tcp_sock타입의 포인터를 획득한다.<br>
또한 len이라는 변수를 설정하게 되는데, skb-&gt;len값으로 가져오게 된다.<br>
flag들과 seqeunce, ack num 이 모두 같은지 확인하고<br>
tcp_sock구조체에서 정의된tcp_header_len이 주어진 tcp 헤더와 길이가 일치하는지 확인하고, tcp_parse_aligned_timestamp()함수를 통해 slow_path인지 확인한다. 이때 주어진 타임스탬프 옵션이 NOP NOP OP_num OP_len 로 포인터로 확인하여 이것이 일치하면 fast_path이다. 또한 PAWS도 확인한다.<br>
<br>
그 다음으로는 len이 tcp_header_len보다 작거나 같은 경우를 확인하게 된다. 만약 len이 tcp_header_len과 같다면 이는 페이로드가 없는 순수한 ACK 패킷이라는 것이다.(pure sender)<br>
이때 만약 둘이 같다면 tcp_ack()함수를 통해 incoming pure ACK 패킷들을 다루게 된다.<br>
그리고 이 후 해당 skb를 할당해제하여 bottom half가 끝나게 된다.<br>
만약 len이 tcp_header_len보다 작다면, 이는 오류 이므로 discard 하게 된다.<br>
<br>
만약 len &gt; tcp_header_len이라면, 이는 pure receiver라는 뜻이다. 여기서 다시 한 번 checksum 과 truesize, tcp_header_len을 확인하고, RTT를 확인하게 된다.<br>
이후 skb의 dst_entry를 드랍하고, tcp 헤더 길이만큼 포인터를 옮긴 다음에 남은 skb-&gt;data가 페이로드를 가르키게 하여 tcp_queue_rcv() 함수를 실행하게 한다. 	이를 eaten 변수에 저장하게 된다. 또한 tcp_event_data_recv()함수를 실행하여 cwnd가 빠르게 증가할 수 있도록 한다.<br>
<br>
그후 ack 확인을 하고 tcp_ack()함수를 실행한다. 또한 tcp_data_snd_check()함수를 실행하고, 소켓의 윈도우 크기를 업데이트 하며 만약 eaten 된 패킷이 있다면 tcp_data_ready()를 실행하여  해당 소켓의 데이터가 준비되었음을 알리게 된다.<br>
여기서부터는 slow path이다.<br>
csum과 flag들을 확인하고, tcp_validate_incoming()함수를 통해 해당 패킷이 유효한 수신패킷인지 확인하게 된다. 그리고 유효하다면, tcp_ack()를 통해 ack 패킷을 보내게 되고, rtt를 측정하고 urgent data를 처리하기 위해 tcp_urg()를 실행하게 된다. 마지막으로 tcp_data_queue라는 함수를 실행하게 되는데, 이는 slow path에서 데이터를 소켓으로 넣어주는 함수이다. 그후 tcp_data_snd_check() 와 tcp_ack_snd_check()함수를 실행하고 함수가 종료된다.
<br><br>trace_tcp_probe() <br>fast path, slow path으로 나누어진다.<br>
tcp flag, sequence number, ack number가 일치한다면 fast path으로 실행한다. <br>if ((s32)(tp-&gt;rx_opt.rcv_tsval - tp-&gt;rx_opt.ts_recent) &lt; 0)<br>
tsval은 sender가 저장한 timestamp, 보낼 때 찍은 시간이다.<br>
recent는 receiver가 저장한 timestamp, 마지막으로 받은 pkt의 tsval를 저장한다.<br>
만약 받은 패킷의 tsval이 recevier가 저장한 ts_recent 보다 작다는 것은 가장 최근에 받은 패킷보다<br>
먼저 생성이 되었던 패킷이라는 뜻이다. 즉 이는 문제가 있는 패킷이라는 뜻이라 다시 slow path로<br>
보낸다.<br>
normal case<br>
저번 pkt  : tsval = 10<br>
지금 pkt  : tsval = 11<br>
strange case<br>
저번 pkt  : tsval = 11<br>
지금 pkt  : tsval = 10<br>tcp_ack()를 실행한다. TCP congestion control 과 관련된 기능이 이 함수에서 호출된다.<br>
tcp_cong_control(), tcp_cwnd_reduction(),....<br>if ((int)skb-&gt;truesize &gt; sk-&gt;sk_forward_alloc)<br>
truesize와 sk_forward_alloc을 비교한다. Receive socket buffer에 새로운 패킷 데이터를 추가할 여유 공간이 있는지 확인한다. 공간이 있으면 header prediction은 hit (prediction 성공)이다. __skb_pull를 호출해서 TCP 헤더를 제거하고 tcp_queue_rcv() 호출한고, __skb_queue_tail을 호출해서 패킷을 receive socket buffer에 추가한다. 마지막으로, __tcp_ack_snd_check를 호출해서 ACK 전송이 필요하면 전송한다.<br>만약 여유 공간이 부족하면 느린 경로를 수행한다. tcp_data_queue 함수는 버퍼 공간을 새로 할당하고 데이터 패킷을 소켓 버퍼에 추가한다. 이때 가능하면 receive socket buffer 크기를 자동으로 증가한다. 빠른 경로와 다르게, tcp_data_snd_check를 호출해서 새로운 데이터 패킷을 전송할 수 있으면 전송하고, 끝으로 tcp_ack_snd_check 호출해서 ACK 전송이 필요하면 ACK 패킷을 생성해서 전송한다.<br><a data-href="tcp_ack()" href="encyclopedia-of-networksystem/function/net-ipv4/tcp_ack().html" class="internal-link" target="_self" rel="noopener nofollow">tcp_ack()</a><br>
<a data-href="tcp_queue_rcv()" href="encyclopedia-of-networksystem/function/net-ipv4/tcp_queue_rcv().html" class="internal-link" target="_self" rel="noopener nofollow">tcp_queue_rcv()</a><br>
<a data-href="tcp_data_queue()" href="encyclopedia-of-networksystem/function/net-ipv4/tcp_data_queue().html" class="internal-link" target="_self" rel="noopener nofollow">tcp_data_queue()</a><br>
<a data-href="tcp_data_ready()" href="encyclopedia-of-networksystem/function/net-ipv4/tcp_data_ready().html" class="internal-link" target="_self" rel="noopener nofollow">tcp_data_ready()</a>]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/tcp_rcv_established().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_rcv_established().md</guid><pubDate>Wed, 06 Nov 2024 13:41:30 GMT</pubDate></item><item><title><![CDATA[tcp_rcv_state_process()]]></title><description><![CDATA[ 
 ]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/tcp_rcv_state_process().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_rcv_state_process().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[tcp_recvmsg_locked()]]></title><description><![CDATA[ 
 <br>static int tcp_recvmsg_locked(struct sock *sk, struct msghdr *msg, size_t len,
			      int flags, struct scm_timestamping_internal *tss,
			      int *cmsg_flags)
{
	struct tcp_sock *tp = tcp_sk(sk);
	int copied = 0;
	u32 peek_seq;
	u32 *seq;
	unsigned long used;
	int err;
	int target;		/* Read at least this many bytes */
	long timeo;
	struct sk_buff *skb, *last;
	u32 urg_hole = 0;

	err = -ENOTCONN;
	if (sk-&gt;sk_state == TCP_LISTEN)
		goto out;

	if (tp-&gt;recvmsg_inq) {
		*cmsg_flags = TCP_CMSG_INQ;
		msg-&gt;msg_get_inq = 1;
	}
	timeo = sock_rcvtimeo(sk, flags &amp; MSG_DONTWAIT);

	/* Urgent data needs to be handled specially. */
	if (flags &amp; MSG_OOB)
		goto recv_urg;

	if (unlikely(tp-&gt;repair)) {
		err = -EPERM;
		if (!(flags &amp; MSG_PEEK))
			goto out;

		if (tp-&gt;repair_queue == TCP_SEND_QUEUE)
			goto recv_sndq;

		err = -EINVAL;
		if (tp-&gt;repair_queue == TCP_NO_QUEUE)
			goto out;

		/* 'common' recv queue MSG_PEEK-ing */
	}

	seq = &amp;tp-&gt;copied_seq;
	if (flags &amp; MSG_PEEK) {
		peek_seq = tp-&gt;copied_seq;
		seq = &amp;peek_seq;
	}

	target = sock_rcvlowat(sk, flags &amp; MSG_WAITALL, len);

	do {
		u32 offset;

		/* Are we at urgent data? Stop if we have read anything or have SIGURG pending. */
		if (unlikely(tp-&gt;urg_data) &amp;&amp; tp-&gt;urg_seq == *seq) {
			if (copied)
				break;
			if (signal_pending(current)) {
				copied = timeo ? sock_intr_errno(timeo) : -EAGAIN;
				break;
			}
		}

		/* Next get a buffer. */

		last = skb_peek_tail(&amp;sk-&gt;sk_receive_queue);
		skb_queue_walk(&amp;sk-&gt;sk_receive_queue, skb) {
			last = skb;
			/* Now that we have two receive queues this
			 * shouldn't happen.
			 */
			if (WARN(before(*seq, TCP_SKB_CB(skb)-&gt;seq),
				 "TCP recvmsg seq # bug: copied %X, seq %X, rcvnxt %X, fl %X\n",
				 *seq, TCP_SKB_CB(skb)-&gt;seq, tp-&gt;rcv_nxt,
				 flags))
				break;

			offset = *seq - TCP_SKB_CB(skb)-&gt;seq;
			if (unlikely(TCP_SKB_CB(skb)-&gt;tcp_flags &amp; TCPHDR_SYN)) {
				pr_err_once("%s: found a SYN, please report !\n", __func__);
				offset--;
			}
			if (offset &lt; skb-&gt;len)
				goto found_ok_skb;
			if (TCP_SKB_CB(skb)-&gt;tcp_flags &amp; TCPHDR_FIN)
				goto found_fin_ok;
			WARN(!(flags &amp; MSG_PEEK),
			     "TCP recvmsg seq # bug 2: copied %X, seq %X, rcvnxt %X, fl %X\n",
			     *seq, TCP_SKB_CB(skb)-&gt;seq, tp-&gt;rcv_nxt, flags);
		}

		/* Well, if we have backlog, try to process it now yet. */

		if (copied &gt;= target &amp;&amp; !READ_ONCE(sk-&gt;sk_backlog.tail))
			break;

		if (copied) {
			if (!timeo ||
			    sk-&gt;sk_err ||
			    sk-&gt;sk_state == TCP_CLOSE ||
			    (sk-&gt;sk_shutdown &amp; RCV_SHUTDOWN) ||
			    signal_pending(current))
				break;
		} else {
			if (sock_flag(sk, SOCK_DONE))
				break;

			if (sk-&gt;sk_err) {
				copied = sock_error(sk);
				break;
			}

			if (sk-&gt;sk_shutdown &amp; RCV_SHUTDOWN)
				break;

			if (sk-&gt;sk_state == TCP_CLOSE) {
				/* This occurs when user tries to read
				 * from never connected socket.
				 */
				copied = -ENOTCONN;
				break;
			}

			if (!timeo) {
				copied = -EAGAIN;
				break;
			}

			if (signal_pending(current)) {
				copied = sock_intr_errno(timeo);
				break;
			}
		}

		if (copied &gt;= target) {
			/* Do not sleep, just process backlog. */
			__sk_flush_backlog(sk);
		} else {
			tcp_cleanup_rbuf(sk, copied);
			err = sk_wait_data(sk, &amp;timeo, last);
			if (err &lt; 0) {
				err = copied ? : err;
				goto out;
			}
		}

		if ((flags &amp; MSG_PEEK) &amp;&amp;
		    (peek_seq - copied - urg_hole != tp-&gt;copied_seq)) {
			net_dbg_ratelimited("TCP(%s:%d): Application bug, race in MSG_PEEK\n",
					    current-&gt;comm,
					    task_pid_nr(current));
			peek_seq = tp-&gt;copied_seq;
		}
		continue;

found_ok_skb:
		/* Ok so how much can we use? */
		used = skb-&gt;len - offset;
		if (len &lt; used)
			used = len;

		/* Do we have urgent data here? */
		if (unlikely(tp-&gt;urg_data)) {
			u32 urg_offset = tp-&gt;urg_seq - *seq;
			if (urg_offset &lt; used) {
				if (!urg_offset) {
					if (!sock_flag(sk, SOCK_URGINLINE)) {
						WRITE_ONCE(*seq, *seq + 1);
						urg_hole++;
						offset++;
						used--;
						if (!used)
							goto skip_copy;
					}
				} else
					used = urg_offset;
			}
		}

		if (!(flags &amp; MSG_TRUNC)) {
			err = skb_copy_datagram_msg(skb, offset, msg, used);
			if (err) {
				/* Exception. Bailout! */
				if (!copied)
					copied = -EFAULT;
				break;
			}
		}

		WRITE_ONCE(*seq, *seq + used);
		copied += used;
		len -= used;

		tcp_rcv_space_adjust(sk);

skip_copy:
		if (unlikely(tp-&gt;urg_data) &amp;&amp; after(tp-&gt;copied_seq, tp-&gt;urg_seq)) {
			WRITE_ONCE(tp-&gt;urg_data, 0);
			tcp_fast_path_check(sk);
		}

		if (TCP_SKB_CB(skb)-&gt;has_rxtstamp) {
			tcp_update_recv_tstamps(skb, tss);
			*cmsg_flags |= TCP_CMSG_TS;
		}

		if (used + offset &lt; skb-&gt;len)
			continue;

		if (TCP_SKB_CB(skb)-&gt;tcp_flags &amp; TCPHDR_FIN)
			goto found_fin_ok;
		if (!(flags &amp; MSG_PEEK))
			tcp_eat_recv_skb(sk, skb);
		continue;

found_fin_ok:
		/* Process the FIN. */
		WRITE_ONCE(*seq, *seq + 1);
		if (!(flags &amp; MSG_PEEK))
			tcp_eat_recv_skb(sk, skb);
		break;
	} while (len &gt; 0);

	/* According to UNIX98, msg_name/msg_namelen are ignored
	 * on connected socket. I was just happy when found this 8) --ANK
	 */

	/* Clean up data we have read: This will do ACK frames. */
	tcp_cleanup_rbuf(sk, copied);
	return copied;

out:
	return err;

recv_urg:
	err = tcp_recv_urg(sk, msg, len, flags);
	goto out;

recv_sndq:
	err = tcp_peek_sndq(sk, msg, len);
	goto out;
}
<br>
timestamp와 urgent data 등을 처리하고 난 뒤, sock_rcvlowat()함수를 통해 수신 받을 길이를 설정하게 된다. 최소한 이 길이 만큼은 읽어야  한 번의 수신 루틴이 끝나게 된다.
그 다음은 do - while 문으로 len보다 많은 길이를 받을 때까지 계속 복사를 하게 되는데, 우선은 skb_peek_tail을 통해 수신 큐의 가장 마지막 패킷을 찾은 다음에, skb_queue_walk()함수를 통해 해당 수신 큐를 처음부터 반복하게 된다. 그 다음 각종 오류들을 탐색하며 만약 오류가 발생했을 시에 이를 경고하고, last에는 해당 skb가 저장될 것이다. 아니라면 중간에 오프셋을 확인하여 정상적인 skb를 가지고 goto found_ok_skb를 통해 해당 skb를 처리하는 루틴으로 넘어가게 된다.
그 후 만약 수신 큐에서 모두 복사가 완료되어 copied &gt;= target이라면 백로그를 처리하지 않고 break가 된다. 그 외에도 sk-&gt;sk_shutdown을 RCV_SHUTDOWN과 비트 &amp; 연산을 하여 종료 조건을 확인한다.
이후 다시 copied &gt;= target이라면 __sk_flush_backlog()함수를 호출하여 백로그를 처리하게 된다. 만약 위의 조건을 만족하지 않는다면 tcp_cleanup_rbuf()함수를 호출하여 수신 큐를 정리하고, sk_wait_data()함수를 시행하게 된다.
found_ok_skb:<br>
사용할 수 있는 길이를 게산하고, urg_data인 경우 먼저 처리를 해주며, 메인 루틴은 skb_copy_datagram_msg()를 통해 이루어진다. 이렇게 user space로 copy가 이루어지고 난 뒤에는 tcp 소켓의 수신 큐가 적절한 길이를 가질 수 있게 tcp_rcv_space_adjust()함수가 호출되게 된다.<br>
그렇게 처음 인자로 받은 len에서 복사한 길이 만큼 계속 빼게 되고, do -while 문은 이 길이가 0보다 작아졌을 때 종료되게 된다.
마지막으로 한번 더 tcp_cleanup_rbuf()를 호출하고 난 뒤 함수가 copied를 반환하며 종료된다.
<br><a data-href="__sk_flush_backlog()" href="encyclopedia-of-networksystem/function/net-core/__sk_flush_backlog().html" class="internal-link" target="_self" rel="noopener nofollow">__sk_flush_backlog()</a><br>
<a data-href="sk_wait_data()" href="encyclopedia-of-networksystem/function/net-core/sk_wait_data().html" class="internal-link" target="_self" rel="noopener nofollow">sk_wait_data()</a><br>
<a data-href="skb_copy_datagram_msg()" href="encyclopedia-of-networksystem/function/include-linux/skb_copy_datagram_msg().html" class="internal-link" target="_self" rel="noopener nofollow">skb_copy_datagram_msg()</a>]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/tcp_recvmsg_locked().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_recvmsg_locked().md</guid><pubDate>Wed, 06 Nov 2024 13:41:30 GMT</pubDate></item><item><title><![CDATA[tcp_recvmsg()]]></title><description><![CDATA[ 
 <br>int tcp_recvmsg(struct sock *sk, struct msghdr *msg, size_t len, int flags,
		int *addr_len)
{
	int cmsg_flags = 0, ret;
	struct scm_timestamping_internal tss;

	if (unlikely(flags &amp; MSG_ERRQUEUE))
		return inet_recv_error(sk, msg, len, addr_len);

	if (sk_can_busy_loop(sk) &amp;&amp;
	    skb_queue_empty_lockless(&amp;sk-&gt;sk_receive_queue) &amp;&amp;
	    sk-&gt;sk_state == TCP_ESTABLISHED)
		sk_busy_loop(sk, flags &amp; MSG_DONTWAIT);

	lock_sock(sk);
	ret = tcp_recvmsg_locked(sk, msg, len, flags, &amp;tss, &amp;cmsg_flags);
	release_sock(sk);

	if ((cmsg_flags || msg-&gt;msg_get_inq) &amp;&amp; ret &gt;= 0) {
		if (cmsg_flags &amp; TCP_CMSG_TS)
			tcp_recv_timestamp(msg, sk, &amp;tss);
		if (msg-&gt;msg_get_inq) {
			msg-&gt;msg_inq = tcp_inq_hint(sk);
			if (cmsg_flags &amp; TCP_CMSG_INQ)
				put_cmsg(msg, SOL_TCP, TCP_CM_INQ,
					 sizeof(msg-&gt;msg_inq), &amp;msg-&gt;msg_inq);
		}
	}
	return ret;
}
<br>
lock_sock()함수를 통해 해당 소켓의 락을 획득하고, <a data-href="tcp_recvmsg_locked()" href="encyclopedia-of-networksystem/function/net-ipv4/tcp_recvmsg_locked().html" class="internal-link" target="_self" rel="noopener nofollow">tcp_recvmsg_locked()</a>를 호출하게 된다. 그리고 소켓 락을 해제한다. 여기서 나온 결과를 그대로 return 하게 된다.
<br><a data-href="tcp_recvmsg_locked()" href="encyclopedia-of-networksystem/function/net-ipv4/tcp_recvmsg_locked().html" class="internal-link" target="_self" rel="noopener nofollow">tcp_recvmsg_locked()</a><br>
<a data-href="release_sock()" href="encyclopedia-of-networksystem/function/net-core/release_sock().html" class="internal-link" target="_self" rel="noopener nofollow">release_sock()</a>]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/tcp_recvmsg().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_recvmsg().md</guid><pubDate>Wed, 06 Nov 2024 13:41:30 GMT</pubDate></item><item><title><![CDATA[tcp_try_coalesce()]]></title><description><![CDATA[ 
 <br>/**
 * tcp_try_coalesce - try to merge skb to prior one
 * @sk: socket
 * @to: prior buffer
 * @from: buffer to add in queue
 * @fragstolen: pointer to boolean
 *
 * Before queueing skb @from after @to, try to merge them
 * to reduce overall memory use and queue lengths, if cost is small.
 * Packets in ofo or receive queues can stay a long time.
 * Better try to coalesce them right now to avoid future collapses.
 * Returns true if caller should free @from instead of queueing it
 */
static bool tcp_try_coalesce(struct sock *sk,
			     struct sk_buff *to,
			     struct sk_buff *from,
			     bool *fragstolen)
{
	int delta;

	*fragstolen = false;

	/* Its possible this segment overlaps with prior segment in queue */
	if (TCP_SKB_CB(from)-&gt;seq != TCP_SKB_CB(to)-&gt;end_seq)
		return false;

	if (!mptcp_skb_can_collapse(to, from))
		return false;

#ifdef CONFIG_TLS_DEVICE
	if (from-&gt;decrypted != to-&gt;decrypted)
		return false;
#endif

	if (!skb_try_coalesce(to, from, fragstolen, &amp;delta))
		return false;

	atomic_add(delta, &amp;sk-&gt;sk_rmem_alloc);
	sk_mem_charge(sk, delta);
	NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPRCVCOALESCE);
	TCP_SKB_CB(to)-&gt;end_seq = TCP_SKB_CB(from)-&gt;end_seq;
	TCP_SKB_CB(to)-&gt;ack_seq = TCP_SKB_CB(from)-&gt;ack_seq;
	TCP_SKB_CB(to)-&gt;tcp_flags |= TCP_SKB_CB(from)-&gt;tcp_flags;

	if (TCP_SKB_CB(from)-&gt;has_rxtstamp) {
		TCP_SKB_CB(to)-&gt;has_rxtstamp = true;
		to-&gt;tstamp = from-&gt;tstamp;
		skb_hwtstamps(to)-&gt;hwtstamp = skb_hwtstamps(from)-&gt;hwtstamp;
	}

	return true;
}
<br>
주어진 두 개의 sk_buff를 합치는 함수이다. skb_try_coalesce()함수를 통해 병합을 시도하며, timestamp, seq num, flags 등을 넘겨 받게 된다.
<br><a data-href="skb_try_coalesce()" href="encyclopedia-of-networksystem/function/net-ipv4/skb_try_coalesce().html" class="internal-link" target="_self" rel="noopener nofollow">skb_try_coalesce()</a>]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/tcp_try_coalesce().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_try_coalesce().md</guid><pubDate>Wed, 06 Nov 2024 13:41:30 GMT</pubDate></item><item><title><![CDATA[tcp_v4_cookie_check()]]></title><description><![CDATA[ 
 <br>static struct sock *tcp_v4_cookie_check(struct sock *sk, struct sk_buff *skb)
{
#ifdef CONFIG_SYN_COOKIES
	const struct tcphdr *th = tcp_hdr(skb);

	if (!th-&gt;syn)
		sk = cookie_v4_check(sk, skb);
#endif
	return sk;
}
<br>/* On input, sk is a listener.
 * Output is listener if incoming packet would not create a child
 *           NULL if memory could not be allocated.
 */
struct sock *cookie_v4_check(struct sock *sk, struct sk_buff *skb)
{
	struct ip_options *opt = &amp;TCP_SKB_CB(skb)-&gt;header.h4.opt;
	const struct tcphdr *th = tcp_hdr(skb);
	struct tcp_sock *tp = tcp_sk(sk);
	struct inet_request_sock *ireq;
	struct net *net = sock_net(sk);
	struct request_sock *req;
	struct sock *ret = sk;
	struct flowi4 fl4;
	struct rtable *rt;
	__u8 rcv_wscale;
	int full_space;
	SKB_DR(reason);

	if (!READ_ONCE(net-&gt;ipv4.sysctl_tcp_syncookies) ||
	    !th-&gt;ack || th-&gt;rst)
		goto out;

	if (cookie_bpf_ok(skb)) {
		req = cookie_bpf_check(sk, skb);
	} else {
		req = cookie_tcp_check(net, sk, skb);
		if (IS_ERR(req))
			goto out;
	}
	if (!req) {
		SKB_DR_SET(reason, NO_SOCKET);
		goto out_drop;
	}

	ireq = inet_rsk(req);

	sk_rcv_saddr_set(req_to_sk(req), ip_hdr(skb)-&gt;daddr);
	sk_daddr_set(req_to_sk(req), ip_hdr(skb)-&gt;saddr);

	/* We throwed the options of the initial SYN away, so we hope
	 * the ACK carries the same options again (see RFC1122 4.2.3.8)
	 */
	RCU_INIT_POINTER(ireq-&gt;ireq_opt, tcp_v4_save_options(net, skb));

	if (security_inet_conn_request(sk, skb, req)) {
		SKB_DR_SET(reason, SECURITY_HOOK);
		goto out_free;
	}

	tcp_ao_syncookie(sk, skb, req, AF_INET);

	/*
	 * We need to lookup the route here to get at the correct
	 * window size. We should better make sure that the window size
	 * hasn't changed since we received the original syn, but I see
	 * no easy way to do this.
	 */
	flowi4_init_output(&amp;fl4, ireq-&gt;ir_iif, ireq-&gt;ir_mark,
			   ip_sock_rt_tos(sk), ip_sock_rt_scope(sk),
			   IPPROTO_TCP, inet_sk_flowi_flags(sk),
			   opt-&gt;srr ? opt-&gt;faddr : ireq-&gt;ir_rmt_addr,
			   ireq-&gt;ir_loc_addr, th-&gt;source, th-&gt;dest, sk-&gt;sk_uid);
	security_req_classify_flow(req, flowi4_to_flowi_common(&amp;fl4));
	rt = ip_route_output_key(net, &amp;fl4);
	if (IS_ERR(rt)) {
		SKB_DR_SET(reason, IP_OUTNOROUTES);
		goto out_free;
	}

	/* Try to redo what tcp_v4_send_synack did. */
	req-&gt;rsk_window_clamp = tp-&gt;window_clamp ? :dst_metric(&amp;rt-&gt;dst, RTAX_WINDOW);
	/* limit the window selection if the user enforce a smaller rx buffer */
	full_space = tcp_full_space(sk);
	if (sk-&gt;sk_userlocks &amp; SOCK_RCVBUF_LOCK &amp;&amp;
	    (req-&gt;rsk_window_clamp &gt; full_space || req-&gt;rsk_window_clamp == 0))
		req-&gt;rsk_window_clamp = full_space;

	tcp_select_initial_window(sk, full_space, req-&gt;mss,
				  &amp;req-&gt;rsk_rcv_wnd, &amp;req-&gt;rsk_window_clamp,
				  ireq-&gt;wscale_ok, &amp;rcv_wscale,
				  dst_metric(&amp;rt-&gt;dst, RTAX_INITRWND));

	/* req-&gt;syncookie is set true only if ACK is validated
	 * by BPF kfunc, then, rcv_wscale is already configured.
	 */
	if (!req-&gt;syncookie)
		ireq-&gt;rcv_wscale = rcv_wscale;
	ireq-&gt;ecn_ok &amp;= cookie_ecn_ok(net, &amp;rt-&gt;dst);

	ret = tcp_get_cookie_sock(sk, skb, req, &amp;rt-&gt;dst);
	/* ip_queue_xmit() depends on our flow being setup
	 * Normal sockets get it right from inet_csk_route_child_sock()
	 */
	if (!ret) {
		SKB_DR_SET(reason, NO_SOCKET);
		goto out_drop;
	}
	inet_sk(ret)-&gt;cork.fl.u.ip4 = fl4;
out:
	return ret;
out_free:
	reqsk_free(req);
out_drop:
	kfree_skb_reason(skb, reason);
	return NULL;
}
]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/tcp_v4_cookie_check().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_v4_cookie_check().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[tcp_v4_do_rcv()]]></title><description><![CDATA[ 
 <br>
/* The socket must have it's spinlock held when we get
* here, unless it is a TCP_LISTEN socket.
*
* We have a potential double-lock case here, so even when
* doing backlog processing we use the BH locking scheme.
* This is because we cannot sleep with the original spinlock
* held.
*/
int tcp_v4_do_rcv(struct sock *sk, struct sk_buff *skb)
{
	enum skb_drop_reason reason;
	struct sock *rsk;

	if (sk-&gt;sk_state == TCP_ESTABLISHED) { /* Fast path */
		struct dst_entry *dst;

		dst = rcu_dereference_protected(sk-&gt;sk_rx_dst,
						lockdep_sock_is_held(sk));

		sock_rps_save_rxhash(sk, skb);
		sk_mark_napi_id(sk, skb);
    
		if (dst) {
			if (sk-&gt;sk_rx_dst_ifindex != skb-&gt;skb_iif ||

			    !INDIRECT_CALL_1(dst-&gt;ops-&gt;check, ipv4_dst_check,
					     dst, 0)) {

				RCU_INIT_POINTER(sk-&gt;sk_rx_dst, NULL);
				dst_release(dst);
			}
		}
		tcp_rcv_established(sk, skb);
		return 0;
	}

	if (tcp_checksum_complete(skb))
		goto csum_err;

	if (sk-&gt;sk_state == TCP_LISTEN) {
		struct sock *nsk = tcp_v4_cookie_check(sk, skb);

		if (!nsk)
			return 0;
		if (nsk != sk) {
			reason = tcp_child_process(sk, nsk, skb);
			if (reason) {
				rsk = nsk;
				goto reset;
			}
			return 0;
		}
	} else
  
		sock_rps_save_rxhash(sk, skb);

	reason = tcp_rcv_state_process(sk, skb);
	if (reason) {
		rsk = sk;
		goto reset;
	}
	return 0;

reset:
	tcp_v4_send_reset(rsk, skb);
discard:
	kfree_skb_reason(skb, reason);
	/* Be careful here. If this function gets more complicated and

	 * gcc suffers from register pressure on the x86, sk (in %ebx)
	 * might be destroyed here. This current version compiles correctly,
	 * but you have been warned.
	 */
	return 0;

csum_err:
	reason = SKB_DROP_REASON_TCP_CSUM;
	trace_tcp_bad_csum(skb);
	TCP_INC_STATS(sock_net(sk), TCP_MIB_CSUMERRORS);
	TCP_INC_STATS(sock_net(sk), TCP_MIB_INERRS);
	goto discard;
}
<br>
우선 소켓의 상태가 TCP_ESTABLISHED인지 확인한다. 이 경로는 Fast path이다.<br>
sk-&gt;sk_rx_dst에서 dst를 불러오게 되는데, INDIRECT_CALL_1 등을 통해 ipv4_dst_check()함수들을 실행하여 조건을 확인하고, 만약 유효하지 않은 dst_entry라면  dst_release를 실행하여 dst_entry구조체를 release한다.<br>
그후 tcp_rcv_established()함수를 실행하여 패킷 처리작업을 이어가고, 0을 반환한다.
소켓의 상태가 TCP_ESTABLISHED가 아닌 경우에는 체크썸을 확인하고, 소켓이 TCP_LISTEN 상태인지 확인하게 된다.<br>
만약 그렇다면, nsk를 선언하여 작업을 이어나가게 된다. 만약 이 nsk가 sk와 다르다면, tcp_child_process() 함수를 호출하고, reason에 결과값을 반환하게 된다. 만약 reason이 있다면, reset 라벨로 가게 되고, 아니라면 그대로 종료된다.
TCP_LISTEN이 아니라면 sock_rps_save_rxhash()함수를 실행한다.
그다음으로 tcp_rcv_state_process()함수의 반환값을 reason으로 받아서 reset 라벨로 가거나 아니라면 종료한다. 여기는 ESTABLISHED상태와 TIME_WAIT 상태가 아닌 모든 소켓들이 처리되는 함수이다.
reset라벨은 tcp_v4_send_reset()함수를 실행시킨다. -&gt; RST flag가 세팅되어 있는 패킷을 전송하게 된다.<br>
discard라벨은 kfree_skb_reason()함수를 실행 시킨다.<br>
이후 0을 반환하게 된다.
<br><br>
<br>socket state가 TCP_ESTABLISHED 경우,&nbsp;tcp_rcv_established()를 호출하여 처리한다.
<br>state가 TCP_LISTEN 경우,&nbsp;cookie_check&nbsp;이 후&nbsp;tcp_child_process()를 호출하여 처리한다.
<br>그 외에는&nbsp;tcp_rcv_state_process()를 호출하여 처리한다.
<br><a data-href="sock_rps_save_rxhash()" href="김기수/산협-프로젝트-2/백서-제작용/sock_rps_save_rxhash().html" class="internal-link" target="_self" rel="noopener nofollow">sock_rps_save_rxhash()</a><br>
<a data-tooltip-position="top" aria-label="김기수/산협 프로젝트 2/백서 제작용/tcp_child_process()" data-href="김기수/산협 프로젝트 2/백서 제작용/tcp_child_process()" href="김기수/산협-프로젝트-2/백서-제작용/tcp_child_process().html" class="internal-link" target="_self" rel="noopener nofollow">tcp_child_process()</a><br>
<a data-href="Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_rcv_established()" href="encyclopedia-of-networksystem/function/net-ipv4/tcp_rcv_established().html" class="internal-link" target="_self" rel="noopener nofollow">Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_rcv_established()</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_rcv_state_process()" data-href="Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_rcv_state_process()" href="encyclopedia-of-networksystem/function/net-ipv4/tcp_rcv_state_process().html" class="internal-link" target="_self" rel="noopener nofollow">tcp_rcv_state_process()</a><br>
<a data-href="tcp_v4_send_reset()" href="encyclopedia-of-networksystem/function/net-ipv4/tcp_v4_send_reset().html" class="internal-link" target="_self" rel="noopener nofollow">tcp_v4_send_reset()</a>]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/tcp_v4_do_rcv().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_v4_do_rcv().md</guid><pubDate>Tue, 08 Oct 2024 08:24:56 GMT</pubDate></item><item><title><![CDATA[tcp_v4_early_demux()]]></title><description><![CDATA[ 
 <br>int tcp_v4_early_demux(struct sk_buff *skb)
{
	struct net *net = dev_net(skb-&gt;dev);
	const struct iphdr *iph;
	const struct tcphdr *th;
	struct sock *sk;
	  
	if (skb-&gt;pkt_type != PACKET_HOST)
		return 0;
	  
	if (!pskb_may_pull(skb, skb_transport_offset(skb) + sizeof(struct tcphdr)))
		return 0;
	
	iph = ip_hdr(skb);
	th = tcp_hdr(skb);
	  
	if (th-&gt;doff &lt; sizeof(struct tcphdr) / 4)
		return 0;
	  
	sk = __inet_lookup_established(net, net-&gt;ipv4.tcp_death_row.hashinfo,
						iph-&gt;saddr, th-&gt;source,
						iph-&gt;daddr, ntohs(th-&gt;dest),
						skb-&gt;skb_iif, inet_sdif(skb));
	if (sk) {
		skb-&gt;sk = sk;
		skb-&gt;destructor = sock_edemux;
		if (sk_fullsock(sk)) {
			struct dst_entry *dst = rcu_dereference(sk-&gt;sk_rx_dst);
			  
			if (dst)
				dst = dst_check(dst, 0);
			if (dst &amp;&amp;
				sk-&gt;sk_rx_dst_ifindex == skb-&gt;skb_iif)
				skb_dst_set_noref(skb, dst);
		}
	}
	return 0;
}
<br>
ip 헤더와 tcp 헤더를 불러와서 __inet_lookup_established()함수를 호출한다. 이는 listen 상태인 소켓을 빠르게 찾아서 디 멀티플렉싱을 한다는 것으로 볼 수 있다. 찾은 소켓이 있다면 skb-&gt;sk, skb-&gt;destructor를 셋팅하고, 만약 full socket이라면 해당 dst를 sk에 매핑하게 되는 것이다.
<br><a data-href="__inet_lookup_established()" href="__inet_lookup_established()" class="internal-link" target="_self" rel="noopener nofollow">__inet_lookup_established()</a>]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/tcp_v4_early_demux().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_v4_early_demux().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[tcp_v4_rcv()]]></title><description><![CDATA[ 
 <br>int tcp_v4_rcv(struct sk_buff *skb)
{
	struct net *net = dev_net(skb-&gt;dev);
	enum skb_drop_reason drop_reason;
	int sdif = inet_sdif(skb);
	int dif = inet_iif(skb);
	const struct iphdr *iph;
	const struct tcphdr *th;
	bool refcounted;
	struct sock *sk;
	int ret;
	  
	drop_reason = SKB_DROP_REASON_NOT_SPECIFIED;
	if (skb-&gt;pkt_type != PACKET_HOST)
		goto discard_it;
		
	/* Count it even if it's bad */
	__TCP_INC_STATS(net, TCP_MIB_INSEGS);

	// skb의 크기가 tcp hdr보다 작으면 discard
	if (!pskb_may_pull(skb, sizeof(struct tcphdr)))
		goto discard_it;
		 
	th = (const struct tcphdr *)skb-&gt;data;

	// header 값들 확인
	if (unlikely(th-&gt;doff &lt; sizeof(struct tcphdr) / 4)) {
		drop_reason = SKB_DROP_REASON_PKT_TOO_SMALL;
		goto bad_packet;
	}
	if (!pskb_may_pull(skb, th-&gt;doff * 4))
		goto discard_it;
		
	/* An explanation is required here, I think.
	* Packet length and doff are validated by header prediction,
	* provided case of th-&gt;doff==0 is eliminated.
	* So, we defer the checks. */
	  
	if (skb_checksum_init(skb, IPPROTO_TCP, inet_compute_pseudo))
		goto csum_error;
	  
	th = (const struct tcphdr *)skb-&gt;data;
	iph = ip_hdr(skb);
lookup:
	sk = __inet_lookup_skb(net-&gt;ipv4.tcp_death_row.hashinfo,
					skb, __tcp_hdrlen(th), th-&gt;source,
					th-&gt;dest, sdif, &amp;refcounted);
	if (!sk)
		goto no_tcp_socket;
  
process:
	if (sk-&gt;sk_state == TCP_TIME_WAIT)
		goto do_time_wait;
	  
	if (sk-&gt;sk_state == TCP_NEW_SYN_RECV) {
		struct request_sock *req = inet_reqsk(sk);
		bool req_stolen = false;
		struct sock *nsk;
	  
		sk = req-&gt;rsk_listener;
		if (!xfrm4_policy_check(sk, XFRM_POLICY_IN, skb))
			drop_reason = SKB_DROP_REASON_XFRM_POLICY;
		else
			drop_reason = tcp_inbound_hash(sk, req, skb,
							&amp;iph-&gt;saddr, &amp;iph-&gt;daddr,
							AF_INET, dif, sdif);
		if (unlikely(drop_reason)) {
			sk_drops_add(sk, skb);
			reqsk_put(req);
			goto discard_it;
		}
		if (tcp_checksum_complete(skb)) {
			reqsk_put(req);
			goto csum_error;
		}
		if (unlikely(sk-&gt;sk_state != TCP_LISTEN)) {
			nsk = reuseport_migrate_sock(sk, req_to_sk(req), skb);
			if (!nsk) {
				inet_csk_reqsk_queue_drop_and_put(sk, req);
				goto lookup;
			}
			sk = nsk;
			/* reuseport_migrate_sock() has already held one sk_refcnt
			* before returning.
			*/
		} else {
			/* We own a reference on the listener, increase it again
			* as we might lose it too soon.
			*/
			sock_hold(sk);
		}
		refcounted = true;
		nsk = NULL;
		if (!tcp_filter(sk, skb)) {
			th = (const struct tcphdr *)skb-&gt;data;
			iph = ip_hdr(skb);
			tcp_v4_fill_cb(skb, iph, th);
			nsk = tcp_check_req(sk, skb, req, false, &amp;req_stolen);
		} else {
			drop_reason = SKB_DROP_REASON_SOCKET_FILTER;
		}
		if (!nsk) {
			reqsk_put(req);
			if (req_stolen) {
				/* Another cpu got exclusive access to req
				* and created a full blown socket.
				* Try to feed this packet to this socket
				* instead of discarding it.
				*/
				tcp_v4_restore_cb(skb);
				sock_put(sk);
				goto lookup;
			}
			goto discard_and_relse;
		}
		nf_reset_ct(skb);
		if (nsk == sk) {
			reqsk_put(req);
			tcp_v4_restore_cb(skb);
		} else {
			drop_reason = tcp_child_process(sk, nsk, skb);
			if (drop_reason) {
				tcp_v4_send_reset(nsk, skb);
				goto discard_and_relse;
			}
			sock_put(sk);
			return 0;
		}
	}
	  
	if (static_branch_unlikely(&amp;ip4_min_ttl)) {
		/* min_ttl can be changed concurrently from do_ip_setsockopt() */
		if (unlikely(iph-&gt;ttl &lt; READ_ONCE(inet_sk(sk)-&gt;min_ttl))) {
			__NET_INC_STATS(net, LINUX_MIB_TCPMINTTLDROP);
			drop_reason = SKB_DROP_REASON_TCP_MINTTL;
			goto discard_and_relse;
		}
	}
	  
	if (!xfrm4_policy_check(sk, XFRM_POLICY_IN, skb)) {
		drop_reason = SKB_DROP_REASON_XFRM_POLICY;
		goto discard_and_relse;
	}
	  
	drop_reason = tcp_inbound_hash(sk, NULL, skb, &amp;iph-&gt;saddr, &amp;iph-&gt;daddr,
						AF_INET, dif, sdif);
	if (drop_reason)
		goto discard_and_relse;
	  
	nf_reset_ct(skb);
	  
	if (tcp_filter(sk, skb)) {
		drop_reason = SKB_DROP_REASON_SOCKET_FILTER;
		goto discard_and_relse;
	}
	th = (const struct tcphdr *)skb-&gt;data;
	iph = ip_hdr(skb);
	tcp_v4_fill_cb(skb, iph, th);
	  
	skb-&gt;dev = NULL;
	  
	if (sk-&gt;sk_state == TCP_LISTEN) {
		ret = tcp_v4_do_rcv(sk, skb);
		goto put_and_return;
	}
	  
	sk_incoming_cpu_update(sk);
	  
	bh_lock_sock_nested(sk);
	tcp_segs_in(tcp_sk(sk), skb);
	ret = 0;
	if (!sock_owned_by_user(sk)) {
		ret = tcp_v4_do_rcv(sk, skb);
	} else {
		if (tcp_add_backlog(sk, skb, &amp;drop_reason))
			goto discard_and_relse;
	}
	bh_unlock_sock(sk);
	  
put_and_return:
	if (refcounted)
		sock_put(sk);
	  
	return ret;
  
no_tcp_socket:
	drop_reason = SKB_DROP_REASON_NO_SOCKET;
	if (!xfrm4_policy_check(NULL, XFRM_POLICY_IN, skb))
		goto discard_it;
	  
	tcp_v4_fill_cb(skb, iph, th);
	  
	if (tcp_checksum_complete(skb)) {
csum_error:
	drop_reason = SKB_DROP_REASON_TCP_CSUM;
	trace_tcp_bad_csum(skb);
	__TCP_INC_STATS(net, TCP_MIB_CSUMERRORS);
bad_packet:
	__TCP_INC_STATS(net, TCP_MIB_INERRS);
	} else {
	tcp_v4_send_reset(NULL, skb);
	}
  
discard_it:
	SKB_DR_OR(drop_reason, NOT_SPECIFIED);
	/* Discard frame. */
	kfree_skb_reason(skb, drop_reason);
	return 0;
  
discard_and_relse:
	sk_drops_add(sk, skb);
	if (refcounted)
		sock_put(sk);
	goto discard_it;
  
do_time_wait:
	if (!xfrm4_policy_check(NULL, XFRM_POLICY_IN, skb)) {
		drop_reason = SKB_DROP_REASON_XFRM_POLICY;
		inet_twsk_put(inet_twsk(sk));
		goto discard_it;
	}
  
	tcp_v4_fill_cb(skb, iph, th);
  
	if (tcp_checksum_complete(skb)) {
		inet_twsk_put(inet_twsk(sk));
		goto csum_error;
	}
	switch (tcp_timewait_state_process(inet_twsk(sk), skb, th)) {
	case TCP_TW_SYN: {
		struct sock *sk2 = inet_lookup_listener(net,
						net-&gt;ipv4.tcp_death_row.hashinfo,
						skb, __tcp_hdrlen(th),
						iph-&gt;saddr, th-&gt;source,
						iph-&gt;daddr, th-&gt;dest,
						inet_iif(skb),
						sdif);
		if (sk2) {
			inet_twsk_deschedule_put(inet_twsk(sk));
			sk = sk2;
			tcp_v4_restore_cb(skb);
			refcounted = false;
			goto process;
		}
	}
		/* to ACK */
		fallthrough;
	case TCP_TW_ACK:
		tcp_v4_timewait_ack(sk, skb);
		break;
	case TCP_TW_RST:
		tcp_v4_send_reset(sk, skb);
		inet_twsk_deschedule_put(inet_twsk(sk));
		goto discard_it;
	case TCP_TW_SUCCESS:;
	}
	goto discard_it;
}
<br>
첫 번째로 skb-&gt;pkt_type을 검사하여 이것이 PACKET_HOST가 아니라면 이를 버리게 된다. 이는 이 패킷의 목적지가 본인인지 확인하는 조건문이다.<br>
우선 skb-&gt;data를 통해 tcp header를 가르키는 포인터를 th에다가 가져오게 된다.<br>
그 후, __inet_lookup_skb()함수를 통해 소켓을 가져오게 되고, 만약 해당하는 TCP 소켓이 없다면 no_tcp_socket라벨로 건너뛰게 된다.
TCP 소켓을 확인하였다면 process 라벨 아래 코드들이 실행되는데, sk-&gt;sk_state값을 확인하여 작업을 이어나간다.
 sk_state == TCP_NEW_SYN_RECV 조건문으로 3-way handshake 작업이 수행되게 되는데, request_sock 타입의 구조체로 sock을 캐스팅하여 req 변수로 다루게 된다.  여기서 request_sock은 sock을 감싸고 있는 구조체이다.<br>
이후,  xfrm4_policy_check() 함수를 통해 패킷의 드랍 여부를 판단하고, 만약 드랍 이유가 발생하였다면 드랍하게 된다.
그후 마저 체크썸을 확인하고 만약 소켓의 상태가 TCP_LISTEN이 아니라면 포트를 그대로 사용하면서 소켓을 migrate하여 새로운 소켓으로 대체한다. 이때 이러한 migration이 일어나는 이유는 바로 한 포트에 여러개의 소켓을 바인딩 할 수 있는 기능 때문이다. 이는 주로 서버에서 사용되며, 하나의 포트로 수많은 요청이 올 것이 예상 될 때 사용하는 기술이다. 이를 통해 단일 연결에서 많은 요청을 적절하게 분산 처리할 수 있다는 장점이 있다. TCP_LISTEN이라면 sock_hold()함수를 사용하여 참조카운트를 증가시키게 된다.<br>
그렇게 nsk에는 새로 demultiplexing 된 listen 상태의 소켓이 설정되고, 이를 sk에 복사함으로써 새로운 소켓이 해당 패킷에 붙게 된다.
이후 nsk는 NULL로 초기화 되고, 그후 tcp_filter()함수를 호출하여 드랍여부를 조사하게 된다. 이 때 tcp_filter()함수는 tcp 헤더를 다시 설정해주고 sk_filter_trim_cap()함수를 호출한다. 이 함수는 해당 소켓의 sk-&gt;sk_filter-&gt;prog을 가지고 eBPF를 실행하게 된다. 이 함수는 skb의 길이를 조절하여 주는 역할을 하고 있다. 이때 실행되는 함수는 sk_attach_filter()라는 함수를 통해 해당 소켓에다가 할당하고자 하는 eBPF 함수를 할당해 줄 수 있다.<br>
위의 내용들을 바탕으로 컨트롤 블럭에 메타데이터를 세팅해주고, tcp_check_req()함수를 호출하여 새로운 flow를 만들게 되는 것이다. 새로운 SYN 패킷이 유효하다면, 이를 바탕으로 해당 소켓의 child 소켓으로 새로운 ESTABLISHED 상태의 소켓을 만들어서 이를 반환하게 된다.<br>
그게 아니라면 패킷을 드랍하게 된다.
만약 위에서 새롭게 만들어진 자식 소켓이 아니거나 본인 소켓이 아닌경우, 즉 tcp_check_req()의 return이 NULL일 경우 해당 request_sock구조체의 참조 카운터를 지우게 된다. 이 때 만약 참조 카운터가 0이 될 경우 그 때는 해당 소켓 구조체를 할당해제하게 된다. 만약 해당 소켓의 req_stolen이 참인 경우, 즉 다른 core가 exclusive access를 얻고 새로운 소켓을 만들었을 경우 control block 정보를 skb의 cb 필드에다가 복사한 후(tcp_v4_restore_cb()), 해당 소켓을 해제하게 된다(sock_put()). 이 후 다시 lookup:라벨로 이동하게 된다. 만약 req_stolen이 false라면 이 패킷을 버리고 release하게 된다.
여기서부터는 nsk가 존재하는 경우이다. 이 때, 정상적으로 새로운 connection이 만들어졌다면, nsk는 ESTABLISHED상태의 child 소켓일거고, 만약 잘못된 ack를 수신하거나 fastopen 옵션이 설정되어 있을 경우(fastopen은 process context에서 tcp_check_req함수가 호출될 때 true이다. 아니라면 BH context에서 호출되고 있다는 뜻이다.)에는 원래 본인의 sk가 부여되어 있을 것이다. 따라서 둘이 같은 경우, req의 참조카운터를 줄이게 되고, 컨트롤 블럭을 복구하게 된다.<br>
그게 아니라면 tcp_child_process()함수를 통해 drop_reason을 받아와서 만약 유효한 경우 tcp_v4_send_reset()을 통해 RST 패킷을 전송하게 되고 discard_and_relse: 라벨로 이동한다.<br>
아니면 sock_put()을 통해 참조 카운트를 줄이고 0을 리턴한다. 여기서 BH가 끝나게 되는 것이다.
<br>
여기서부터는 새로운 flow가 아닌 부분들이다. 만약 SYN패킷이라면 위에서 처리가 끝났다.
이후 정책값 확인, ttl 확인 및 tcp_filter()등을 통해 패킷을 검사하고, tcp_v4_fill_cb()함수를 통해 메타데이터를 채운다음,<br>
만약 소켓의 상태가 TCP_LISTEN이라면 tcp_v4_do_rcv()함수를 호출하고, ret에 결과를 저장하여 put_and_return라벨로 건너뛰게 된다.
<br>
그게 아니라면 sk_incoming_cpu_update()를 실행하여 현재 패킷을 처리하는 코어가 어느 것인지 업데이트 후 user context에서 사용 중인지를 확인하고 사용하지 않는다면 tcp_v4_do_rcv()함수를 실행하게 된다. 만약 사용중이라면 tcp_add_backlog()함수를 실행하게 된다.
<br>
put_and_return라벨은 보통의 정상적인 수신이 되었을 때 나타나는 루틴이며, 여기서도 sock_out()함수를 실행하고 ret을 리턴한다.
<br>
do_time_wait:라벨은 tcp_v4_rcv()함수 맨 위쪽에 만약 sk-&gt;state가 TCP_TIME_WAIT인 경우에 넘어오게 되는 부분이다. tcp_timewait_state_process()이 함수를 통해 잠시 기다렸다가, 결과에 따라서 switch문으로 분기하게 된다.
먼저 TCP_TW_SYN의 경우 해당하는 소켓을 새로 찾아서 바꿔주고 process:라벨로 다시 돌아가게 된다. ACK나 RST같은 경우에도 해당하는 함수를 실행해주고(tcp_v4_timewait_ack(), tcp_v4_send_rest() 등등) 만약 TCP_TW_SUCCESS 라면 아무것도 안하게 된다.
<br><br>packet의 type이 PACKET_HOST인 경우에만 진행한다.<br>
pskb_may_pull() 함수가 tcp header option 이 sk_buff의 kmalloc된 부분에 존재하도록 보장한다.<br>
__inet_lookup_skb()함수로 packet이 가야할 socket을 찾아준다.<br>
위 함수에 찾아진 socket state 따라서 처리 방식이 달라진다.<br>
<br>socket이 TCP_TIME_WAIT인 경우<br>
tcp connection 이 close 된 이후에도 delay packet들을 받기 위해서 잠시 유지하는 상태이다.
<br>TCP_NEW_SYN_RECV인 경우<br>
<img alt="Pasted image 20240901180451.png" src="encyclopedia-of-networksystem/function/net-ipv4/img/pasted-image-20240901180451.png"><br>
xfrm policy check, checksum validation한다. 새로운 socket을 생성할 수도 있다.
<br>TCP_LISTEN인 경우<br>
tcp_v4_do_rcv() 함수 실행한다. 
<br>sock_owned_by_user() 함수는 현재 소켓이 어플리케이션에서 사용하고 있는 상황인지 확인한다<br>
소켓이 사용되지 않는 경우에는 tcp_v4_do_rcv()를 실행하고<br>
소켓이 사용되는 경우에는 tcp_add_backlog()를 실행한다.<br>TCP_listen 상태에서는 syn pkt을 기다리고 있기 때문에 위의 과정이 불필요하다.<br>
syn pkt에는 payload가 없어서 lock이 걸릴 일이 없기 때문이다. <br><a data-href="Encyclopedia of NetworkSystem/Function/net-ipv4/`__inet_lookup_skb()" href="encyclopedia-of-networksystem/function/net-ipv4/`__inet_lookup_skb().html" class="internal-link" target="_self" rel="noopener nofollow">Encyclopedia of NetworkSystem/Function/net-ipv4/`__inet_lookup_skb()</a><br>
<a data-href="tcp_filter()" href="encyclopedia-of-networksystem/function/net-ipv4/tcp_filter().html" class="internal-link" target="_self" rel="noopener nofollow">tcp_filter()</a><br>
<a data-href="tcp_check_req()" href="encyclopedia-of-networksystem/function/net-ipv4/tcp_check_req().html" class="internal-link" target="_self" rel="noopener nofollow">tcp_check_req()</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_v4_do_rcv()" data-href="Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_v4_do_rcv()" href="encyclopedia-of-networksystem/function/net-ipv4/tcp_v4_do_rcv().html" class="internal-link" target="_self" rel="noopener nofollow">tcp_v4_do_rcv()</a><br>
<a data-href="sock_owned_by_user()" href="encyclopedia-of-networksystem/function/net-ipv4/sock_owned_by_user().html" class="internal-link" target="_self" rel="noopener nofollow">sock_owned_by_user()</a><br>
<a data-href="tcp_add_backlog()" href="encyclopedia-of-networksystem/function/net-ipv4/tcp_add_backlog().html" class="internal-link" target="_self" rel="noopener nofollow">tcp_add_backlog()</a><br>As&nbsp;usual,&nbsp;the&nbsp;pskb_may_pull()&nbsp;function&nbsp;is&nbsp;responsible&nbsp;for&nbsp;ensuring&nbsp;that&nbsp;the&nbsp;TCP&nbsp;header,&nbsp;and&nbsp;in&nbsp;the&nbsp;next&nbsp;call,&nbsp;the&nbsp;header&nbsp;options&nbsp;are&nbsp;in&nbsp;the&nbsp;kmalloc'ed&nbsp;portion&nbsp;of&nbsp;the&nbsp;sk_buff.]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/tcp_v4_rcv().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_v4_rcv().md</guid><pubDate>Thu, 10 Oct 2024 11:46:39 GMT</pubDate><enclosure url="encyclopedia-of-networksystem/function/net-ipv4/img/pasted-image-20240901180451.png" length="0" type="image/png"/><content:encoded>&lt;figure&gt;&lt;img src=&quot;encyclopedia-of-networksystem/function/net-ipv4/img/pasted-image-20240901180451.png&quot;&gt;&lt;/figure&gt;</content:encoded></item><item><title><![CDATA[tcp_v4_send_reset()]]></title><description><![CDATA[ 
 ]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/tcp_v4_send_reset().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_v4_send_reset().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[tcp4_gro_complete()]]></title><description><![CDATA[ 
 <br>INDIRECT_CALLABLE_SCOPE int tcp4_gro_complete(struct sk_buff *skb, int thoff)
{
	const struct iphdr *iph = ip_hdr(skb);
	struct tcphdr *th = tcp_hdr(skb);

	th-&gt;check = ~tcp_v4_check(skb-&gt;len - thoff, iph-&gt;saddr,
				  iph-&gt;daddr, 0);

	skb_shinfo(skb)-&gt;gso_type |= SKB_GSO_TCPV4 |
			(NAPI_GRO_CB(skb)-&gt;is_atomic * SKB_GSO_TCP_FIXEDID);

	tcp_gro_complete(skb); // [[Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_gro_complete().md|tcp_gro_complete()]]
	return 0;
}
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_gro_complete().md" data-href="Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_gro_complete().md" href="encyclopedia-of-networksystem/function/net-ipv4/tcp_gro_complete().html" class="internal-link" target="_self" rel="noopener nofollow">tcp_gro_complete()</a><br>
여기도 단순하게 checksum 유효성만 검사하고, tcp_gro_receive()를 호출해주는 과정만 존재한다.
]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/tcp4_gro_complete().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/tcp4_gro_complete().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[tcp4_gro_receive()]]></title><description><![CDATA[ 
 <br>INDIRECT_CALLABLE_SCOPE
struct sk_buff *tcp4_gro_receive(struct list_head *head, struct sk_buff *skb)
{
	/* Don't bother verifying checksum if we're going to flush anyway. */
	if (!NAPI_GRO_CB(skb)-&gt;flush &amp;&amp;
	    skb_gro_checksum_validate(skb, IPPROTO_TCP,
				      inet_gro_compute_pseudo)) {
		NAPI_GRO_CB(skb)-&gt;flush = 1;
		return NULL;
	}

	return tcp_gro_receive(head, skb); // [[Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_gro_receive().md|tcp_gro_receive()]]
}
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_gro_receive().md" data-href="Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_gro_receive().md" href="encyclopedia-of-networksystem/function/net-ipv4/tcp_gro_receive().html" class="internal-link" target="_self" rel="noopener nofollow">tcp_gro_receive()</a><br>
간단하게 flush가 필요한지 여부와 checksum이 유효한지 여부를 따져서 더 processing이 진행되는지 확인하는 If문이 하나가 있고, 아니라면 tcp_gro_receive()함수를 호출하여 계속 진행하게 된다.
]]></description><link>encyclopedia-of-networksystem/function/net-ipv4/tcp4_gro_receive().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4/tcp4_gro_receive().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ipt_do_table()]]></title><description><![CDATA[ 
 <br>훅 함수이다.<br>/* Returns one of the generic firewall policies, like NF_ACCEPT. */

unsigned int
ipt_do_table(void *priv,
         struct sk_buff *skb,
         const struct nf_hook_state *state)
{
    const struct xt_table *table = priv;
    unsigned int hook = state-&gt;hook;
    static const char nulldevname[IFNAMSIZ] __attribute__((aligned(sizeof(long))));
    const struct iphdr *ip;
    /* Initializing verdict to NF_DROP keeps gcc happy. */
    unsigned int verdict = NF_DROP;
    const char *indev, *outdev;
    const void *table_base;
    struct ipt_entry *e, **jumpstack;
    unsigned int stackidx, cpu;
    const struct xt_table_info *private;
    struct xt_action_param acpar;
    unsigned int addend;
  
    /* Initialization */
    stackidx = 0;
    ip = ip_hdr(skb);
    indev = state-&gt;in ? state-&gt;in-&gt;name : nulldevname;
    outdev = state-&gt;out ? state-&gt;out-&gt;name : nulldevname;
    /* We handle fragments by dealing with the first fragment as
     * if it was a normal packet.  All other fragments are treated
     * normally, except that they will NEVER match rules that ask
     * things we don't know, ie. tcp syn flag or ports).  If the
     * rule is also a fragment-specific rule, non-fragments won't
     * match it. */
    acpar.fragoff = ntohs(ip-&gt;frag_off) &amp; IP_OFFSET;
    acpar.thoff   = ip_hdrlen(skb);
    acpar.hotdrop = false;
    acpar.state   = state;
  
    WARN_ON(!(table-&gt;valid_hooks &amp; (1 &lt;&lt; hook)));
    local_bh_disable();
    addend = xt_write_recseq_begin();
    private = READ_ONCE(table-&gt;private); /* Address dependency. */
    cpu        = smp_processor_id();
    table_base = private-&gt;entries;
    jumpstack  = (struct ipt_entry **)private-&gt;jumpstack[cpu];
  
    /* Switch to alternate jumpstack if we're being invoked via TEE.
     * TEE issues XT_CONTINUE verdict on original skb so we must not
     * clobber the jumpstack.
     *
     * For recursion via REJECT or SYNPROXY the stack will be clobbered
     * but it is no problem since absolute verdict is issued by these.
     */
    if (static_key_false(&amp;xt_tee_enabled))
        jumpstack += private-&gt;stacksize * __this_cpu_read(nf_skb_duplicated);
  
    e = get_entry(table_base, private-&gt;hook_entry[hook]);
  
    do {
        const struct xt_entry_target *t;
        const struct xt_entry_match *ematch;
        struct xt_counters *counter;
  
        WARN_ON(!e);

        if (!ip_packet_match(ip, indev, outdev,
            &amp;e-&gt;ip, acpar.fragoff)) {
 no_match:
            e = ipt_next_entry(e);
            continue;
        }
  
        xt_ematch_foreach(ematch, e) {
            acpar.match     = ematch-&gt;u.kernel.match;
            acpar.matchinfo = ematch-&gt;data;
            if (!acpar.match-&gt;match(skb, &amp;acpar))
                goto no_match;
        }
  
        counter = xt_get_this_cpu_counter(&amp;e-&gt;counters);
        ADD_COUNTER(*counter, skb-&gt;len, 1);
  
        t = ipt_get_target_c(e);
        WARN_ON(!t-&gt;u.kernel.target);
  
#if IS_ENABLED(CONFIG_NETFILTER_XT_TARGET_TRACE)
        /* The packet is traced: log it */
        if (unlikely(skb-&gt;nf_trace))
            trace_packet(state-&gt;net, skb, hook, state-&gt;in,
                     state-&gt;out, table-&gt;name, private, e);
#endif
        /* Standard target? */
        if (!t-&gt;u.kernel.targt-&gt;target) {
            int v;
  
            v = ((struct xt_standard_target *)t)-&gt;verdict;
            if (v &lt; 0) {
                /* Pop from stack? */
                if (v != XT_RETURN) {
                    verdict = (unsigned int)(-v) - 1;
                    break;
                }
                if (stackidx == 0) {
                    e = get_entry(table_base,
                        private-&gt;underflow[hook]);
                } else {
                    e = jumpstack[--stackidx];
                    e = ipt_next_entry(e);
                }
                continue;
            }
            if (table_base + v != ipt_next_entry(e) &amp;&amp;
                !(e-&gt;ip.flags &amp; IPT_F_GOTO)) {
                if (unlikely(stackidx &gt;= private-&gt;stacksize)) {
                    verdict = NF_DROP;
                    break;
                }
                jumpstack[stackidx++] = e;
            }
  
            e = get_entry(table_base, v);
            continue;
        }
  
        acpar.target   = t-&gt;u.kernel.target;
        acpar.targinfo = t-&gt;data;
  
        verdict = t-&gt;u.kernel.target-&gt;target(skb, &amp;acpar);
        if (verdict == XT_CONTINUE) {
            /* Target might have changed stuff. */
            ip = ip_hdr(skb);
            e = ipt_next_entry(e);
        } else {
            /* Verdict */
            break;
        }
    } while (!acpar.hotdrop);
  
    xt_write_recseq_end(addend);
    local_bh_enable();
  
    if (acpar.hotdrop)
        return NF_DROP;
    else return verdict;
}
]]></description><link>encyclopedia-of-networksystem/function/net-ipv4-netfilter/ipt_do_table().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv4-netfilter/ipt_do_table().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ipv6_gro_complete()]]></title><description><![CDATA[ 
 <br>INDIRECT_CALLABLE_SCOPE int ipv6_gro_complete(struct sk_buff *skb, int nhoff)
{
	const struct net_offload *ops;
	struct ipv6hdr *iph;
	int err = -ENOSYS;
	u32 payload_len;

	if (skb-&gt;encapsulation) {
		skb_set_inner_protocol(skb, cpu_to_be16(ETH_P_IPV6));
		skb_set_inner_network_header(skb, nhoff);
	}

	payload_len = skb-&gt;len - nhoff - sizeof(*iph);
	if (unlikely(payload_len &gt; IPV6_MAXPLEN)) {
		struct hop_jumbo_hdr *hop_jumbo;
		int hoplen = sizeof(*hop_jumbo);

		/* Move network header left */
		memmove(skb_mac_header(skb) - hoplen, skb_mac_header(skb),
			skb-&gt;transport_header - skb-&gt;mac_header);
		skb-&gt;data -= hoplen;
		skb-&gt;len += hoplen;
		skb-&gt;mac_header -= hoplen;
		skb-&gt;network_header -= hoplen;
		iph = (struct ipv6hdr *)(skb-&gt;data + nhoff);
		hop_jumbo = (struct hop_jumbo_hdr *)(iph + 1);

		/* Build hop-by-hop options */
		hop_jumbo-&gt;nexthdr = iph-&gt;nexthdr;
		hop_jumbo-&gt;hdrlen = 0;
		hop_jumbo-&gt;tlv_type = IPV6_TLV_JUMBO;
		hop_jumbo-&gt;tlv_len = 4;
		hop_jumbo-&gt;jumbo_payload_len = htonl(payload_len + hoplen);

		iph-&gt;nexthdr = NEXTHDR_HOP;
		iph-&gt;payload_len = 0;
	} else {
		iph = (struct ipv6hdr *)(skb-&gt;data + nhoff);
		iph-&gt;payload_len = htons(payload_len);
	}

	nhoff += sizeof(*iph) + ipv6_exthdrs_len(iph, &amp;ops);
	if (WARN_ON(!ops || !ops-&gt;callbacks.gro_complete))
		goto out;

	err = INDIRECT_CALL_L4(ops-&gt;callbacks.gro_complete, tcp6_gro_complete,
			       udp6_gro_complete, skb, nhoff); // [[Encyclopedia of NetworkSystem/Function/net-ipv6/tcp6_gro_complete().md|tcp6_gro_complete()]]

out:
	return err;
}
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-ipv6/tcp6_gro_complete().md" data-href="Encyclopedia of NetworkSystem/Function/net-ipv6/tcp6_gro_complete().md" href="encyclopedia-of-networksystem/function/net-ipv6/tcp6_gro_complete().html" class="internal-link" target="_self" rel="noopener nofollow">tcp6_gro_complete()</a>]]></description><link>encyclopedia-of-networksystem/function/net-ipv6/ipv6_gro_complete().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv6/ipv6_gro_complete().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ipv6_gro_receive()]]></title><description><![CDATA[ 
 <br>INDIRECT_CALLABLE_SCOPE struct sk_buff *ipv6_gro_receive(struct list_head *head,
							 struct sk_buff *skb)
{
	const struct net_offload *ops;
	struct sk_buff *pp = NULL;
	struct sk_buff *p;
	struct ipv6hdr *iph;
	unsigned int nlen;
	unsigned int hlen;
	unsigned int off;
	u16 flush = 1; 
	int proto;

	off = skb_gro_offset(skb);
	hlen = off + sizeof(*iph);
	iph = skb_gro_header(skb, hlen, off);
	if (unlikely(!iph))
		goto out;

	skb_set_network_header(skb, off);
	NAPI_GRO_CB(skb)-&gt;inner_network_offset = off;

	flush += ntohs(iph-&gt;payload_len) != skb-&gt;len - hlen;

	proto = iph-&gt;nexthdr;
	ops = rcu_dereference(inet6_offloads[proto]);
	if (!ops || !ops-&gt;callbacks.gro_receive) {
		proto = ipv6_gro_pull_exthdrs(skb, hlen, proto);

		ops = rcu_dereference(inet6_offloads[proto]);
		if (!ops || !ops-&gt;callbacks.gro_receive)
			goto out;

		iph = skb_gro_network_header(skb);
	} else {
		skb_gro_pull(skb, sizeof(*iph));
	}

	skb_set_transport_header(skb, skb_gro_offset(skb));

	NAPI_GRO_CB(skb)-&gt;proto = proto;

	flush--;
	nlen = skb_network_header_len(skb);

	list_for_each_entry(p, head, list) {
		const struct ipv6hdr *iph2;
		__be32 first_word; /* &lt;Version:4&gt;&lt;Traffic_Class:8&gt;&lt;Flow_Label:20&gt; */

		if (!NAPI_GRO_CB(p)-&gt;same_flow)
			continue;

		iph2 = (struct ipv6hdr *)(p-&gt;data + off);
		first_word = *(__be32 *)iph ^ *(__be32 *)iph2;

		/* All fields must match except length and Traffic Class.
		 * XXX skbs on the gro_list have all been parsed and pulled
		 * already so we don't need to compare nlen
		 * (nlen != (sizeof(*iph2) + ipv6_exthdrs_len(iph2, &amp;ops)))
		 * memcmp() alone below is sufficient, right?
		 */
		 if ((first_word &amp; htonl(0xF00FFFFF)) ||
		     !ipv6_addr_equal(&amp;iph-&gt;saddr, &amp;iph2-&gt;saddr) ||
		     !ipv6_addr_equal(&amp;iph-&gt;daddr, &amp;iph2-&gt;daddr) ||
		     iph-&gt;nexthdr != iph2-&gt;nexthdr) {
not_same_flow:
			NAPI_GRO_CB(p)-&gt;same_flow = 0;
			continue;
		}
		if (unlikely(nlen &gt; sizeof(struct ipv6hdr))) {
			if (memcmp(iph + 1, iph2 + 1,
				   nlen - sizeof(struct ipv6hdr)))
				goto not_same_flow;
		}
		/* flush if Traffic Class fields are different */
		NAPI_GRO_CB(p)-&gt;flush |= !!((first_word &amp; htonl(0x0FF00000)) |
			(__force __be32)(iph-&gt;hop_limit ^ iph2-&gt;hop_limit));
		NAPI_GRO_CB(p)-&gt;flush |= flush;

		/* If the previous IP ID value was based on an atomic
		 * datagram we can overwrite the value and ignore it.
		 */
		if (NAPI_GRO_CB(skb)-&gt;is_atomic)
			NAPI_GRO_CB(p)-&gt;flush_id = 0;
	}

	NAPI_GRO_CB(skb)-&gt;is_atomic = true;
	NAPI_GRO_CB(skb)-&gt;flush |= flush;

	skb_gro_postpull_rcsum(skb, iph, nlen);

	pp = indirect_call_gro_receive_l4(tcp6_gro_receive, udp6_gro_receive,
					 ops-&gt;callbacks.gro_receive, head, skb); // [[Encyclopedia of NetworkSystem/Function/net-ipv6/tcp6_gro_receive().md|tcp6_gro_receive()]]

out:
	skb_gro_flush_final(skb, pp, flush);

	return pp;
}
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-ipv6/tcp6_gro_receive().md" data-href="Encyclopedia of NetworkSystem/Function/net-ipv6/tcp6_gro_receive().md" href="encyclopedia-of-networksystem/function/net-ipv6/tcp6_gro_receive().html" class="internal-link" target="_self" rel="noopener nofollow">tcp6_gro_receive()</a>]]></description><link>encyclopedia-of-networksystem/function/net-ipv6/ipv6_gro_receive().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv6/ipv6_gro_receive().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[tcp6_gro_complete()]]></title><description><![CDATA[ 
 <br>INDIRECT_CALLABLE_SCOPE int tcp6_gro_complete(struct sk_buff *skb, int thoff)
{
	const struct ipv6hdr *iph = ipv6_hdr(skb);
	struct tcphdr *th = tcp_hdr(skb);

	th-&gt;check = ~tcp_v6_check(skb-&gt;len - thoff, &amp;iph-&gt;saddr,
				  &amp;iph-&gt;daddr, 0);
	skb_shinfo(skb)-&gt;gso_type |= SKB_GSO_TCPV6;

	tcp_gro_complete(skb); // [[Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_gro_complete().md|tcp_gro_complete()]]
	return 0;
}
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_gro_complete().md" data-href="Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_gro_complete().md" href="encyclopedia-of-networksystem/function/net-ipv4/tcp_gro_complete().html" class="internal-link" target="_self" rel="noopener nofollow">tcp_gro_complete()</a>]]></description><link>encyclopedia-of-networksystem/function/net-ipv6/tcp6_gro_complete().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv6/tcp6_gro_complete().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[tcp6_gro_receive()]]></title><description><![CDATA[ 
 <br>INDIRECT_CALLABLE_SCOPE
struct sk_buff *tcp6_gro_receive(struct list_head *head, struct sk_buff *skb)
{
	/* Don't bother verifying checksum if we're going to flush anyway. */
	if (!NAPI_GRO_CB(skb)-&gt;flush &amp;&amp;
	    skb_gro_checksum_validate(skb, IPPROTO_TCP,
				      ip6_gro_compute_pseudo)) {
		NAPI_GRO_CB(skb)-&gt;flush = 1;
		return NULL;
	}

	return tcp_gro_receive(head, skb); // [[Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_gro_receive().md|tcp_gro_receive()]]
}
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_gro_receive().md" data-href="Encyclopedia of NetworkSystem/Function/net-ipv4/tcp_gro_receive().md" href="encyclopedia-of-networksystem/function/net-ipv4/tcp_gro_receive().html" class="internal-link" target="_self" rel="noopener nofollow">tcp_gro_receive()</a>]]></description><link>encyclopedia-of-networksystem/function/net-ipv6/tcp6_gro_receive().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-ipv6/tcp6_gro_receive().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[nf_hook_entry_hookfn()]]></title><description><![CDATA[ 
 <br>static inline int
nf_hook_entry_hookfn(const struct nf_hook_entry *entry, struct sk_buff *skb,
		     struct nf_hook_state *state)
{
	return entry-&gt;hook(entry-&gt;priv, skb, state);
}
<br>
hook entry 에 있는 함수를 실행한다.
<br><a rel="noopener nofollow" class="external-link" href="https://wiki.nftables.org/wiki-nftables/index.php/Netfilter_hooks" target="_blank">https://wiki.nftables.org/wiki-nftables/index.php/Netfilter_hooks</a><br>
<a data-tooltip-position="top" aria-label="https://hayz.tistory.com/entry/%EB%B2%88%EC%97%AD-Iptables-%EB%B0%8F-Netfilter-%EA%B5%AC%EC%A1%B0%EC%97%90-%EB%8C%80%ED%95%9C-%EC%8B%AC%EC%B8%B5%EB%B6%84%EC%84%9D" rel="noopener nofollow" class="external-link" href="https://hayz.tistory.com/entry/%EB%B2%88%EC%97%AD-Iptables-%EB%B0%8F-Netfilter-%EA%B5%AC%EC%A1%B0%EC%97%90-%EB%8C%80%ED%95%9C-%EC%8B%AC%EC%B8%B5%EB%B6%84%EC%84%9D" target="_blank">https://hayz.tistory.com/entry/번역-Iptables-및-Netfilter-구조에-대한-심층분석</a><br>
참고자료]]></description><link>encyclopedia-of-networksystem/function/net-netfilter/nf_hook_entry_hookfn().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-netfilter/nf_hook_entry_hookfn().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[nf_hook_slow()]]></title><description><![CDATA[ 
 <br>/* Returns 1 if okfn() needs to be executed by the caller,
 * -EPERM for NF_DROP, 0 otherwise.  Caller must hold rcu_read_lock. */
int nf_hook_slow(struct sk_buff *skb, struct nf_hook_state *state,
		 const struct nf_hook_entries *e, unsigned int s)
{
	unsigned int verdict;
	int ret;

	for (; s &lt; e-&gt;num_hook_entries; s++) {
		verdict = nf_hook_entry_hookfn(&amp;e-&gt;hooks[s], skb, state);
		switch (verdict &amp; NF_VERDICT_MASK) {
		case NF_ACCEPT:
			break;
		case NF_DROP:
			kfree_skb_reason(skb,
					 SKB_DROP_REASON_NETFILTER_DROP);
			ret = NF_DROP_GETERR(verdict);
			if (ret == 0)
				ret = -EPERM;
			return ret;
		case NF_QUEUE:
			ret = nf_queue(skb, state, s, verdict);
			if (ret == 1)
				continue;
			return ret;
		case NF_STOLEN:
			return NF_DROP_GETERR(verdict);
		default:
			WARN_ON_ONCE(1);
			return 0;
		}
	}

	return 1;
}
<br>
이전 함수에 추가했었던 hook_entries에서 하나씩 꺼내서 nf_hook_entry_hookfn()에 실행한다. 그리고 그 결과를 가지고 switch문이 돌아가는데, 만약 NF_QUEUE인 경우 nf_queue()함수를 실행하고 아니면 각자 적당한 처리를 하고 넘어가게 된다.
<br>여기서 verdict 판결이 나오는데, 이 값에 따라서 실행되는 코드가 달라진다.<br>
(accept, drop, queue, stolen)<br>
accept 될 경우, pkt이 위의 stack에 전달이 되었고 1을 리턴해서 성공했음을 알린다.<br>
<br>NF_DROP(0) : 패킷을 폐기되고 더 이상 처리되지 않는다.
<br>NF_ACCEPT(1) : 패킷이 커널 네트워크 스택에서 계속 이동한다.
<br>NF_STOLEN : 패킷은 훅 함수로 처리되어 이동하지 않는다.
<br>NF_QUEUE : userspace에서 처리되기 위해 queue된다. iptable, nftable과 같은 daemon에서 실행
<br>NF_REPEAT : pkt이 동일한 hook point 로 re-inject 재실행? 되어서 다시 처리가 된다. 
<br><a data-href="nf_hook_entry_hookfn()" href="encyclopedia-of-networksystem/function/net-netfilter/nf_hook_entry_hookfn().html" class="internal-link" target="_self" rel="noopener nofollow">nf_hook_entry_hookfn()</a>]]></description><link>encyclopedia-of-networksystem/function/net-netfilter/nf_hook_slow().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-netfilter/nf_hook_slow().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[netif_carrier_on()]]></title><description><![CDATA[ 
 <br>/**
 *	netif_carrier_on - set carrier
 *	@dev: network device
 *
 * Device has detected acquisition of carrier.
 */
void netif_carrier_on(struct net_device *dev)
{
	if (test_and_clear_bit(__LINK_STATE_NOCARRIER, &amp;dev-&gt;state)) {
		if (dev-&gt;reg_state == NETREG_UNINITIALIZED)
			return;
		atomic_inc(&amp;dev-&gt;carrier_up_count);
		linkwatch_fire_event(dev);
		if (netif_running(dev))
			__netdev_watchdog_up(dev);
	}
}
EXPORT_SYMBOL(netif_carrier_on);
]]></description><link>encyclopedia-of-networksystem/function/net-sched/netif_carrier_on().html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/net-sched/netif_carrier_on().md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[Function]]></title><description><![CDATA[ 
 ]]></description><link>encyclopedia-of-networksystem/function/function.html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Function/Function.md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_32b_rx_flex_desc]]></title><description><![CDATA[ 
 <br>/* Rx Flex Descriptor
* This descriptor is used instead of the legacy version descriptor when
* ice_rlan_ctx.adv_desc is set
*/
union ice_32b_rx_flex_desc {
	struct {
		__le64 pkt_addr; /* Packet buffer address */
		__le64 hdr_addr; /* Header buffer address */
				/* bit 0 of hdr_addr is DD bit */
		__le64 rsvd1;
		__le64 rsvd2;
	} read;
	struct {
		/* Qword 0 */
		u8 rxdid; /* descriptor builder profile ID */
		u8 mir_id_umb_cast; /* mirror=[5:0], umb=[7:6] */
		__le16 ptype_flex_flags0; /* ptype=[9:0], ff0=[15:10] */
		__le16 pkt_len; /* [15:14] are reserved */
		__le16 hdr_len_sph_flex_flags1; /* header=[10:0] */
						/* sph=[11:11] */
						/* ff1/ext=[15:12] */
		  
		/* Qword 1 */
		__le16 status_error0;
		__le16 l2tag1;
		__le16 flex_meta0;
		__le16 flex_meta1;
		  
		/* Qword 2 */
		__le16 status_error1;
		u8 flex_flags2;
		u8 time_stamp_low;
		__le16 l2tag2_1st;
		__le16 l2tag2_2nd;
		  
		/* Qword 3 */
		__le16 flex_meta2;
		__le16 flex_meta3;
		union {
			struct {
				__le16 flex_meta4;
				__le16 flex_meta5;
			} flex;
			__le32 ts_high;25
		} flex_ts;
	} wb; /* writeback */
};
]]></description><link>encyclopedia-of-networksystem/struct/drivers-net-ethernet-intel-ice/ice_32b_rx_flex_desc.html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Struct/drivers-net-ethernet-intel-ice/ice_32b_rx_flex_desc.md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_32byte_rx_desc]]></title><description><![CDATA[ 
 <br>union ice_32byte_rx_desc {
	struct {
		__le64 pkt_addr; /* Packet buffer address */
		__le64 hdr_addr; /* Header buffer address */
			/* bit 0 of hdr_addr is DD bit */
		__le64 rsvd1;
		__le64 rsvd2;
	} read;
	struct {
		struct {
			struct {
				__le16 mirroring_status;
				__le16 l2tag1;
			} lo_dword;
			union {
				__le32 rss; /* RSS Hash */
				__le32 fd_id; /* Flow Director filter ID */
			} hi_dword;
		} qword0;
		struct {
			/* status/error/PTYPE/length */
			__le64 status_error_len;
		} qword1;
		struct {
			__le16 ext_status; /* extended status */
			__le16 rsvd;
			__le16 l2tag2_1;
			__le16 l2tag2_2;
		} qword2;
		struct {
			__le32 reserved;
			__le32 fd_id;
		} qword3;
	} wb; /* writeback */
};
<br>
사용되지 않는 union이다. 잘못 작성하였다. 그러나 legacy 버전이므로, 참고가 될까하여 남겨두었다.
]]></description><link>encyclopedia-of-networksystem/struct/drivers-net-ethernet-intel-ice/ice_32byte_rx_desc.html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Struct/drivers-net-ethernet-intel-ice/ice_32byte_rx_desc.md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[<strong>주요 필드 설명</strong>]]></title><description><![CDATA[ 
 <br>/* Port hardware description */
struct ice_hw {
	u8 __iomem *hw_addr;
	void *back;
	struct ice_aqc_layer_props *layer_info;
	struct ice_port_info *port_info;
	/* PSM clock frequency for calculating RL profile params */
	u32 psm_clk_freq;
	u64 debug_mask;		/* bitmap for debug mask */
	enum ice_mac_type mac_type;

	u16 fd_ctr_base;	/* FD counter base index */

	/* pci info */
	u16 device_id;
	u16 vendor_id;
	u16 subsystem_device_id;
	u16 subsystem_vendor_id;
	u8 revision_id;

	u8 pf_id;		/* device profile info */
	enum ice_phy_model phy_model;

	u16 max_burst_size;	/* driver sets this value */

	/* Tx Scheduler values */
	u8 num_tx_sched_layers;
	u8 num_tx_sched_phys_layers;
	u8 flattened_layers;
	u8 max_cgds;
	u8 sw_entry_point_layer;
	u16 max_children[ICE_AQC_TOPO_MAX_LEVEL_NUM];
	struct list_head agg_list;	/* lists all aggregator */

	struct ice_vsi_ctx *vsi_ctx[ICE_MAX_VSI];
	u8 evb_veb;		/* true for VEB, false for VEPA */
	u8 reset_ongoing;	/* true if HW is in reset, false otherwise */
	struct ice_bus_info bus;
	struct ice_flash_info flash;
	struct ice_hw_dev_caps dev_caps;	/* device capabilities */
	struct ice_hw_func_caps func_caps;	/* function capabilities */

	struct ice_switch_info *switch_info;	/* switch filter lists */

	/* Control Queue info */
	struct ice_ctl_q_info adminq;
	struct ice_ctl_q_info sbq;
	struct ice_ctl_q_info mailboxq;

	u8 api_branch;		/* API branch version */
	u8 api_maj_ver;		/* API major version */
	u8 api_min_ver;		/* API minor version */
	u8 api_patch;		/* API patch version */
	u8 fw_branch;		/* firmware branch version */
	u8 fw_maj_ver;		/* firmware major version */
	u8 fw_min_ver;		/* firmware minor version */
	u8 fw_patch;		/* firmware patch version */
	u32 fw_build;		/* firmware build number */

	struct ice_fwlog_cfg fwlog_cfg;
	bool fwlog_supported; /* does hardware support FW logging? */
	struct ice_fwlog_ring fwlog_ring;

/* Device max aggregate bandwidths corresponding to the GL_PWR_MODE_CTL
 * register. Used for determining the ITR/INTRL granularity during
 * initialization.
 */
#define ICE_MAX_AGG_BW_200G	0x0
#define ICE_MAX_AGG_BW_100G	0X1
#define ICE_MAX_AGG_BW_50G	0x2
#define ICE_MAX_AGG_BW_25G	0x3
	/* ITR granularity for different speeds */
#define ICE_ITR_GRAN_ABOVE_25	2
#define ICE_ITR_GRAN_MAX_25	4
	/* ITR granularity in 1 us */
	u8 itr_gran;
	/* INTRL granularity for different speeds */
#define ICE_INTRL_GRAN_ABOVE_25	4
#define ICE_INTRL_GRAN_MAX_25	8
	/* INTRL granularity in 1 us */
	u8 intrl_gran;

#define ICE_MAX_QUAD			2
#define ICE_QUADS_PER_PHY_E82X		2
#define ICE_PORTS_PER_PHY_E82X		8
#define ICE_PORTS_PER_QUAD		4
#define ICE_PORTS_PER_PHY_E810		4
#define ICE_NUM_EXTERNAL_PORTS		(ICE_MAX_QUAD * ICE_PORTS_PER_QUAD)

	/* Active package version (currently active) */
	struct ice_pkg_ver active_pkg_ver;
	u32 pkg_seg_id;
	u32 pkg_sign_type;
	u32 active_track_id;
	u8 pkg_has_signing_seg:1;
	u8 active_pkg_name[ICE_PKG_NAME_SIZE];
	u8 active_pkg_in_nvm;

	/* Driver's package ver - (from the Ice Metadata section) */
	struct ice_pkg_ver pkg_ver;
	u8 pkg_name[ICE_PKG_NAME_SIZE];

	/* Driver's Ice segment format version and ID (from the Ice seg) */
	struct ice_pkg_ver ice_seg_fmt_ver;
	u8 ice_seg_id[ICE_SEG_ID_SIZE];

	/* Pointer to the ice segment */
	struct ice_seg *seg;

	/* Pointer to allocated copy of pkg memory */
	u8 *pkg_copy;
	u32 pkg_size;

	/* tunneling info */
	struct mutex tnl_lock;
	struct ice_tunnel_table tnl;

	struct udp_tunnel_nic_shared udp_tunnel_shared;
	struct udp_tunnel_nic_info udp_tunnel_nic;

	/* dvm boost update information */
	struct ice_dvm_table dvm_upd;

	/* HW block tables */
	struct ice_blk_info blk[ICE_BLK_COUNT];
	struct mutex fl_profs_locks[ICE_BLK_COUNT];	/* lock fltr profiles */
	struct list_head fl_profs[ICE_BLK_COUNT];

	/* Flow Director filter info */
	int fdir_active_fltr;

	struct mutex fdir_fltr_lock;	/* protect Flow Director */
	struct list_head fdir_list_head;

	/* Book-keeping of side-band filter count per flow-type.
	 * This is used to detect and handle input set changes for
	 * respective flow-type.
	 */
	u16 fdir_fltr_cnt[ICE_FLTR_PTYPE_MAX];

	struct ice_fd_hw_prof **fdir_prof;
	DECLARE_BITMAP(fdir_perfect_fltr, ICE_FLTR_PTYPE_MAX);
	struct mutex rss_locks;	/* protect RSS configuration */
	struct list_head rss_list_head;
	struct ice_mbx_snapshot mbx_snapshot;
	DECLARE_BITMAP(hw_ptype, ICE_FLOW_PTYPE_MAX);
	u8 dvm_ena;
	u16 io_expander_handle;
	u8 cgu_part_number;
};
<br>struct ice_hw 구조체는 Intel Ethernet Controller (ICE) 드라이버에서 사용되는 데이터 구조체로, 하드웨어 상태와 설정을 관리하는 역할을 한다. 이 구조체는 다양한 하드웨어 리소스, 설정, 상태 정보를 포함하며, 드라이버가 장치를 초기화하고 제어하는 데 필요한 정보를 제공한다.<br><br><br>
<br>u8 __iomem *hw_addr: 메모리 매핑된 하드웨어 주소.
<br>void *back: 드라이버의 백엔드 데이터에 대한 포인터.
<br>struct ice_aqc_layer_props *layer_info: AQC(Aquanta Clock) 레이어 속성 정보.
<br>struct ice_port_info *port_info: 포트 정보.
<br>u32 psm_clk_freq: RL 프로필 매개변수 계산을 위한 PSM 클록 주파수.
<br>u64 debug_mask: 디버그 마스크 비트맵.
<br>enum ice_mac_type mac_type: MAC 유형.
<br><br>
<br>u16 device_id: 디바이스 ID.
<br>u16 vendor_id: 벤더 ID.
<br>u16 subsystem_device_id: 서브시스템 디바이스 ID.
<br>u16 subsystem_vendor_id: 서브시스템 벤더 ID.
<br>u8 revision_id: PCI 리비전 ID.
<br>u8 pf_id: 디바이스 프로필 정보.
<br>enum ice_phy_model phy_model: PHY 모델.
<br><br>
<br>u8 num_tx_sched_layers: 전송 스케줄링 레이어 수.
<br>u8 num_tx_sched_phys_layers: 전송 스케줄링 물리 레이어 수.
<br>u8 flattened_layers: 평탄화된 레이어 수.
<br>u8 max_cgds: 최대 CGD 수.
<br>u8 sw_entry_point_layer: 소프트웨어 진입점 레이어.
<br>u16 max_children[ICE_AQC_TOPO_MAX_LEVEL_NUM]: 각 레벨의 최대 자식 노드 수.
<br>struct list_head agg_list: 모든 애그리게이터 리스트.
<br><br>
<br>struct ice_vsi_ctx *vsi_ctx[ICE_MAX_VSI]: VSI(Context) 배열.
<br>u8 evb_veb: VEB 또는 VEPA 플래그.
<br>u8 reset_ongoing: 하드웨어 리셋 진행 여부.
<br>struct ice_switch_info *switch_info: 스위치 필터 리스트 정보.
<br><br>
<br>struct ice_ctl_q_info adminq: Admin 큐 정보.
<br>struct ice_ctl_q_info sbq: SBQ 큐 정보.
<br>struct ice_ctl_q_info mailboxq: 메일박스 큐 정보.
<br><br>
<br>u8 api_branch: API 브랜치 버전.
<br>u8 api_maj_ver: API 메이저 버전.
<br>u8 api_min_ver: API 마이너 버전.
<br>u8 api_patch: API 패치 버전.
<br>u8 fw_branch: 펌웨어 브랜치 버전.
<br>u8 fw_maj_ver: 펌웨어 메이저 버전.
<br>u8 fw_min_ver: 펌웨어 마이너 버전.
<br>u8 fw_patch: 펌웨어 패치 버전.
<br>u32 fw_build: 펌웨어 빌드 번호.
<br><br>
<br>struct ice_pkg_ver active_pkg_ver: 활성 패키지 버전.
<br>u32 pkg_seg_id: 패키지 세그먼트 ID.
<br>u32 pkg_sign_type: 패키지 서명 타입.
<br>u8 active_pkg_name[ICE_PKG_NAME_SIZE]: 활성 패키지 이름.
<br>u8 active_pkg_in_nvm: NVM에 있는 활성 패키지 여부.
<br>struct ice_pkg_ver pkg_ver: 드라이버 패키지 버전.
<br>u8 pkg_name[ICE_PKG_NAME_SIZE]: 드라이버 패키지 이름.
<br>struct ice_pkg_ver ice_seg_fmt_ver: 드라이버 Ice 세그먼트 포맷 버전.
<br>u8 ice_seg_id[ICE_SEG_ID_SIZE]: Ice 세그먼트 ID.
<br>struct ice_seg *seg: Ice 세그먼트에 대한 포인터.
<br>u8 *pkg_copy: 할당된 패키지 메모리의 포인터.
<br>u32 pkg_size: 패키지 크기.
<br><br>
<br>struct mutex tnl_lock: 터널링 정보 보호 뮤텍스.
<br>struct ice_tunnel_table tnl: 터널 테이블.
<br>struct udp_tunnel_nic_shared udp_tunnel_shared: 공유된 UDP 터널 정보.
<br>struct udp_tunnel_nic_info udp_tunnel_nic: UDP 터널 NIC 정보.
<br>struct ice_dvm_table dvm_upd: DVM 부스트 업데이트 정보.
<br>struct ice_blk_info blk[ICE_BLK_COUNT]: 하드웨어 블록 테이블.
<br>struct mutex fl_profs_locks[ICE_BLK_COUNT]: 필터 프로필 보호 뮤텍스.
<br>struct list_head fl_profs[ICE_BLK_COUNT]: 필터 프로필 리스트.
<br><br>
<br>int fdir_active_fltr: 활성 Flow Director 필터.
<br>struct mutex fdir_fltr_lock: Flow Director 보호 뮤텍스.
<br>struct list_head fdir_list_head: Flow Director 리스트 헤드.
<br>u16 fdir_fltr_cnt[ICE_FLTR_PTYPE_MAX]: 플로우 타입별 필터 수.
<br>struct ice_fd_hw_prof **fdir_prof: Flow Director 하드웨어 프로필.
<br>DECLARE_BITMAP(fdir_perfect_fltr, ICE_FLTR_PTYPE_MAX): 완벽 필터 비트맵.
<br>struct mutex rss_locks: RSS(Reduce Sequence Spoofing) 설정 보호 뮤텍스.
<br>struct list_head rss_list_head: RSS 리스트 헤드.
<br>struct ice_mbx_snapshot mbx_snapshot: 메일박스 스냅샷.
<br>DECLARE_BITMAP(hw_ptype, ICE_FLOW_PTYPE_MAX): 하드웨어 플로우 타입 비트맵.
<br>u8 dvm_ena: DVM 활성화 플래그.
<br>u16 io_expander_handle: IO 익스팬더 핸들.
<br>u8 cgu_part_number: CGU 부품 번호.
<br><br>struct ice_hw는 Intel Ethernet Controller(ICE) 드라이버에서 하드웨어 상태와 설정을 관리하는 중요한 구조체다. 이 구조체는 하드웨어 리소스, 설정, 상태 정보를 포함하며, 드라이버가 장치를 초기화하고 제어하는 데 필요한 다양한 정보를 제공한다. 이 구조체를 통해 드라이버는 하드웨어를 효율적으로 관리하고, 다양한 기능과 설정을 제어할 수 있다.]]></description><link>encyclopedia-of-networksystem/struct/drivers-net-ethernet-intel-ice/ice_hw.html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Struct/drivers-net-ethernet-intel-ice/ice_hw.md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[<strong>주요 필드 설명</strong>]]></title><description><![CDATA[ 
 <br>struct ice_pf {
	struct pci_dev *pdev;

	struct devlink_region *nvm_region;
	struct devlink_region *sram_region;
	struct devlink_region *devcaps_region;

	/* devlink port data */
	struct devlink_port devlink_port;

	/* OS reserved IRQ details */
	struct msix_entry *msix_entries;
	struct ice_irq_tracker irq_tracker;
	/* First MSIX vector used by SR-IOV VFs. Calculated by subtracting the
	 * number of MSIX vectors needed for all SR-IOV VFs from the number of
	 * MSIX vectors allowed on this PF.
	 */
	u16 sriov_base_vector;
	unsigned long *sriov_irq_bm;	/* bitmap to track irq usage */
	u16 sriov_irq_size;		/* size of the irq_bm bitmap */

	u16 ctrl_vsi_idx;		/* control VSI index in pf-&gt;vsi array */

	struct ice_vsi **vsi;		/* VSIs created by the driver */
	struct ice_vsi_stats **vsi_stats;
	struct ice_sw *first_sw;	/* first switch created by firmware */
	u16 eswitch_mode;		/* current mode of eswitch */
	struct dentry *ice_debugfs_pf;
	struct dentry *ice_debugfs_pf_fwlog;
	/* keep track of all the dentrys for FW log modules */
	struct dentry **ice_debugfs_pf_fwlog_modules;
	struct ice_vfs vfs;
	DECLARE_BITMAP(features, ICE_F_MAX);
	DECLARE_BITMAP(state, ICE_STATE_NBITS);
	DECLARE_BITMAP(flags, ICE_PF_FLAGS_NBITS);
	DECLARE_BITMAP(misc_thread, ICE_MISC_THREAD_NBITS);
	unsigned long *avail_txqs;	/* bitmap to track PF Tx queue usage */
	unsigned long *avail_rxqs;	/* bitmap to track PF Rx queue usage */
	unsigned long serv_tmr_period;
	unsigned long serv_tmr_prev;
	struct timer_list serv_tmr;
	struct work_struct serv_task;
	struct mutex avail_q_mutex;	/* protects access to avail_[rx|tx]qs */
	struct mutex sw_mutex;		/* lock for protecting VSI alloc flow */
	struct mutex tc_mutex;		/* lock to protect TC changes */
	struct mutex adev_mutex;	/* lock to protect aux device access */
	struct mutex lag_mutex;		/* protect ice_lag struct in PF */
	u32 msg_enable;
	struct ice_ptp ptp;
	struct gnss_serial *gnss_serial;
	struct gnss_device *gnss_dev;
	u16 num_rdma_msix;		/* Total MSIX vectors for RDMA driver */
	u16 rdma_base_vector;

	/* spinlock to protect the AdminQ wait list */
	spinlock_t aq_wait_lock;
	struct hlist_head aq_wait_list;
	wait_queue_head_t aq_wait_queue;
	bool fw_emp_reset_disabled;

	wait_queue_head_t reset_wait_queue;

	u32 hw_csum_rx_error;
	u32 oicr_err_reg;
	struct msi_map oicr_irq;	/* Other interrupt cause MSIX vector */
	struct msi_map ll_ts_irq;	/* LL_TS interrupt MSIX vector */
	u16 max_pf_txqs;	/* Total Tx queues PF wide */
	u16 max_pf_rxqs;	/* Total Rx queues PF wide */
	u16 num_lan_msix;	/* Total MSIX vectors for base driver */
	u16 num_lan_tx;		/* num LAN Tx queues setup */
	u16 num_lan_rx;		/* num LAN Rx queues setup */
	u16 next_vsi;		/* Next free slot in pf-&gt;vsi[] - 0-based! */
	u16 num_alloc_vsi;
	u16 corer_count;	/* Core reset count */
	u16 globr_count;	/* Global reset count */
	u16 empr_count;		/* EMP reset count */
	u16 pfr_count;		/* PF reset count */

	u8 wol_ena : 1;		/* software state of WoL */
	u32 wakeup_reason;	/* last wakeup reason */
	struct ice_hw_port_stats stats;
	struct ice_hw_port_stats stats_prev;
	struct ice_hw hw;
	u8 stat_prev_loaded:1; /* has previous stats been loaded */
	u8 rdma_mode;
	u16 dcbx_cap;
	u32 tx_timeout_count;
	unsigned long tx_timeout_last_recovery;
	u32 tx_timeout_recovery_level;
	char int_name[ICE_INT_NAME_STR_LEN];
	char int_name_ll_ts[ICE_INT_NAME_STR_LEN];
	struct auxiliary_device *adev;
	int aux_idx;
	u32 sw_int_count;
	/* count of tc_flower filters specific to channel (aka where filter
	 * action is "hw_tc &lt;tc_num&gt;")
	 */
	u16 num_dmac_chnl_fltrs;
	struct hlist_head tc_flower_fltr_list;

	u64 supported_rxdids;

	__le64 nvm_phy_type_lo; /* NVM PHY type low */
	__le64 nvm_phy_type_hi; /* NVM PHY type high */
	struct ice_link_default_override_tlv link_dflt_override;
	struct ice_lag *lag; /* Link Aggregation information */

	struct ice_eswitch eswitch;
	struct ice_esw_br_port *br_port;

#define ICE_INVALID_AGG_NODE_ID		0
#define ICE_PF_AGG_NODE_ID_START	1
#define ICE_MAX_PF_AGG_NODES		32
	struct ice_agg_node pf_agg_node[ICE_MAX_PF_AGG_NODES];
#define ICE_VF_AGG_NODE_ID_START	65
#define ICE_MAX_VF_AGG_NODES		32
	struct ice_agg_node vf_agg_node[ICE_MAX_VF_AGG_NODES];
	struct ice_dplls dplls;
	struct device *hwmon_dev;
};
<br>Intel의 네트워크 인터페이스 카드(NIC) 드라이버에서 사용되는 데이터 구조체로, 물리적 기능(PF: Physical Function)과 관련된 다양한 정보를 저장하고 관리한다. 이 구조체는 PF와 관련된 리소스, 상태, 설정 등을 관리하는 역할을 한다.<br><br><br>
<br>struct pci_dev *pdev: PCI 장치 구조체 포인터로, 장치의 PCI 정보를 포함한다.
<br>struct devlink_region *nvm_region, *sram_region, *devcaps_region: 다양한 메모리 영역(NVM, SRAM, DevCaps) 관련 정보를 저장하는 devlink 지역.
<br><br>
<br>struct devlink_port devlink_port: devlink 포트와 관련된 데이터.
<br><br>
<br>struct msix_entry *msix_entries: MSIX 엔트리 배열.
<br>struct ice_irq_tracker irq_tracker: IRQ 추적기.
<br>u16 sriov_base_vector: SR-IOV VFs에 사용되는 첫 번째 MSIX 벡터.
<br>unsigned long *sriov_irq_bm: IRQ 사용을 추적하는 비트맵.
<br>u16 sriov_irq_size: IRQ 비트맵의 크기.
<br><br>
<br>u16 ctrl_vsi_idx: 제어 VSI 인덱스.
<br>struct ice_vsi **vsi: 드라이버에 의해 생성된 VSI 배열.
<br>struct ice_vsi_stats **vsi_stats: VSI 통계 배열.
<br>struct ice_sw *first_sw: 첫 번째 스위치.
<br><br>
<br>struct dentry *ice_debugfs_pf: PF 디버그 파일 시스템 엔트리.
<br>struct dentry *ice_debugfs_pf_fwlog: FW 로그 디버그 파일 시스템 엔트리.
<br>struct dentry **ice_debugfs_pf_fwlog_modules: FW 로그 모듈 디버그 엔트리 배열.
<br>DECLARE_BITMAP(features, ICE_F_MAX): 기능 비트맵.
<br>DECLARE_BITMAP(state, ICE_STATE_NBITS): 상태 비트맵.
<br>DECLARE_BITMAP(flags, ICE_PF_FLAGS_NBITS): 플래그 비트맵.
<br>DECLARE_BITMAP(misc_thread, ICE_MISC_THREAD_NBITS): 기타 스레드 비트맵.
<br><br>
<br>unsigned long *avail_txqs: 사용 가능한 Tx 큐 비트맵.
<br>unsigned long *avail_rxqs: 사용 가능한 Rx 큐 비트맵.
<br>struct mutex avail_q_mutex: Tx/Rx 큐 접근 보호 뮤텍스.
<br>struct mutex sw_mutex: VSI 할당 보호 뮤텍스.
<br>struct mutex tc_mutex: TC 변경 보호 뮤텍스.
<br>struct mutex adev_mutex: 보조 장치 접근 보호 뮤텍스.
<br>struct mutex lag_mutex: 링크 어그리게이션 보호 뮤텍스.
<br><br>
<br>struct ice_ptp ptp: PTP(Precision Time Protocol) 관련 데이터.
<br>struct gnss_serial *gnss_serial: GNSS 시리얼 인터페이스.
<br>struct gnss_device *gnss_dev: GNSS 장치.
<br><br>
<br>u16 num_rdma_msix: RDMA 드라이버용 MSIX 벡터 수.
<br>u16 rdma_base_vector: RDMA용 첫 번째 MSIX 벡터.
<br>spinlock_t aq_wait_lock: AdminQ 대기 목록 보호 스핀락.
<br>struct hlist_head aq_wait_list: AdminQ 대기 목록.
<br>wait_queue_head_t aq_wait_queue: AdminQ 대기 큐.
<br>u16 num_lan_msix: LAN 드라이버용 MSIX 벡터 수.
<br>u16 num_lan_tx: 설정된 LAN Tx 큐 수.
<br>u16 num_lan_rx: 설정된 LAN Rx 큐 수.
<br><br>
<br>u16 corer_count: 코어 리셋 카운트.
<br>u16 globr_count: 글로벌 리셋 카운트.
<br>u16 empr_count: EMP 리셋 카운트.
<br>u16 pfr_count: PF 리셋 카운트.
<br>u32 hw_csum_rx_error: RX 체크섬 오류.
<br>u32 oicr_err_reg: OICR 오류 레지스터.
<br>struct msi_map oicr_irq: 기타 인터럽트 원인 MSIX 벡터.
<br>struct msi_map ll_ts_irq: LL_TS 인터럽트 MSIX 벡터.
<br><br>
<br>struct ice_hw_port_stats stats: 포트 통계.
<br>struct ice_hw_port_stats stats_prev: 이전 포트 통계.
<br>struct ice_hw hw: 하드웨어 정보.
<br>u8 stat_prev_loaded: 이전 통계 로드 여부.
<br>u32 tx_timeout_count: Tx 타임아웃 카운트.
<br>unsigned long tx_timeout_last_recovery: 마지막 Tx 타임아웃 복구 시간.
<br>u32 tx_timeout_recovery_level: Tx 타임아웃 복구 레벨.
<br>u8 wol_ena: Wake-on-LAN 활성화 상태.
<br>u32 wakeup_reason: 마지막 웨이크업 이유.
<br><br>
<br>struct ice_lag *lag: 링크 어그리게이션 정보.
<br>struct ice_eswitch eswitch: eSwitch 정보.
<br>struct ice_esw_br_port *br_port: eSwitch 브리지 포트.
<br><br>
<br>char int_name[ICE_INT_NAME_STR_LEN]: 인터럽트 이름.
<br>char int_name_ll_ts[ICE_INT_NAME_STR_LEN]: LL_TS 인터럽트 이름.
<br>struct auxiliary_device *adev: 보조 장치.
<br>int aux_idx: 보조 장치 인덱스.
<br>u32 sw_int_count: 소프트웨어 인터럽트 카운트.
<br>u16 num_dmac_chnl_fltrs: DMAC 채널 필터 수.
<br>struct hlist_head tc_flower_fltr_list: TC Flower 필터 목록.
<br>u64 supported_rxdids: 지원되는 RX DID.
<br>__le64 nvm_phy_type_lo: NVM PHY 타입 하위 64비트.
<br>__le64 nvm_phy_type_hi: NVM PHY 타입 상위 64비트.
<br>struct ice_link_default_override_tlv link_dflt_override: 링크 기본 오버라이드.
<br>struct ice_agg_node pf_agg_node[ICE_MAX_PF_AGG_NODES]: PF 어그리게이션 노드.
<br>struct ice_agg_node vf_agg_node[ICE_MAX_VF_AGG_NODES]: VF 어그리게이션 노드.
<br>struct ice_dplls dplls: DPLL 정보.
<br>struct device *hwmon_dev: 하드웨어 모니터링 장치.
<br><br>ice_pf 구조체는 Intel의 NIC 드라이버에서 PF와 관련된 다양한 정보를 저장하고 관리하는 역할을 한다. 이 구조체는 장치 초기화, 리소스 관리, 상태 모니터링, 가상 기능 관리, 인터럽트 처리 등 다양한 기능을 포함하며, 드라이버가 장치를 효율적으로 제어하고 운영할 수 있도록 돕는다.]]></description><link>encyclopedia-of-networksystem/struct/drivers-net-ethernet-intel-ice/ice_pf.html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Struct/drivers-net-ethernet-intel-ice/ice_pf.md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_q_vector]]></title><description><![CDATA[ 
 <br>/* struct that defines an interrupt vector */
struct ice_q_vector {
	struct ice_vsi *vsi;

	u16 v_idx;			/* index in the vsi-&gt;q_vector array. */
	u16 reg_idx;
	u8 num_ring_rx;			/* total number of Rx rings in vector */
	u8 num_ring_tx;			/* total number of Tx rings in vector */
	u8 wb_on_itr:1;			/* if true, WB on ITR is enabled */
	/* in usecs, need to use ice_intrl_to_usecs_reg() before writing this
	 * value to the device
	 */
	u8 intrl;

	struct napi_struct napi;

	struct ice_ring_container rx;
	struct ice_ring_container tx; //rx, tx 각각 있음.

	cpumask_t affinity_mask;
	struct irq_affinity_notify affinity_notify;

	struct ice_channel *ch;

	char name[ICE_INT_NAME_STR_LEN];

	u16 total_events;	/* net_dim(): number of interrupts processed */
	struct msi_map irq; // [[Encyclopedia of NetworkSystem/Struct/include-linux/msi_map.md|msi_map]]
} ____cacheline_internodealigned_in_smp;
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Struct/include-linux/msi_map.md" data-href="Encyclopedia of NetworkSystem/Struct/include-linux/msi_map.md" href="encyclopedia-of-networksystem/struct/include-linux/msi_map.html" class="internal-link" target="_self" rel="noopener nofollow">msi_map</a><br>각각의 q_vector는 rx, tx ring buffer의 그룹이다. 이러한 큐들의 그룹이 하나의 인터럽트에 할당 되게 되고, 이 때 묶음 큐는 linked list로 ice_ring_container에 포인터로 참조되어 있다.]]></description><link>encyclopedia-of-networksystem/struct/drivers-net-ethernet-intel-ice/ice_q_vector.html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Struct/drivers-net-ethernet-intel-ice/ice_q_vector.md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_ring_container]]></title><description><![CDATA[ 
 <br>struct ice_ring_container {
	/* head of linked-list of rings */
	union {
		struct ice_rx_ring *rx_ring; // [[Encyclopedia of NetworkSystem/Struct/drivers-net-ethernet-intel-ice/ice_rx_ring.md|ice_rx_ring]], [[Encyclopedia of NetworkSystem/Struct/drivers-net-ethernet-intel-ice/ice_tx_ring.md|ice_tx_ring]]
		struct ice_tx_ring *tx_ring; // 링을 가리키는 포인터
	};
	struct dim dim;		/* data for net_dim algorithm */
	u16 itr_idx;		/* index in the interrupt vector */
	/* this matches the maximum number of ITR bits, but in usec
	 * values, so it is shifted left one bit (bit zero is ignored)
	 */
	union {
		struct {
			u16 itr_setting:13;
			u16 itr_reserved:2;
			u16 itr_mode:1;
		};
		u16 itr_settings;
	};
	enum ice_container_type type;
};
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Struct/drivers-net-ethernet-intel-ice/ice_rx_ring.md" data-href="Encyclopedia of NetworkSystem/Struct/drivers-net-ethernet-intel-ice/ice_rx_ring.md" href="encyclopedia-of-networksystem/struct/drivers-net-ethernet-intel-ice/ice_rx_ring.html" class="internal-link" target="_self" rel="noopener nofollow">ice_rx_ring</a><br>
<a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Struct/drivers-net-ethernet-intel-ice/ice_tx_ring.md" data-href="Encyclopedia of NetworkSystem/Struct/drivers-net-ethernet-intel-ice/ice_tx_ring.md" href="encyclopedia-of-networksystem/struct/drivers-net-ethernet-intel-ice/ice_tx_ring.html" class="internal-link" target="_self" rel="noopener nofollow">ice_tx_ring</a>]]></description><link>encyclopedia-of-networksystem/struct/drivers-net-ethernet-intel-ice/ice_ring_container.html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Struct/drivers-net-ethernet-intel-ice/ice_ring_container.md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_rx_ring]]></title><description><![CDATA[ 
 <br>/* descriptor ring, associated with a VSI */
struct ice_rx_ring {
	/* CL1 - 1st cacheline starts here */
	void *desc;			/* Descriptor ring memory */
	struct device *dev;		/* Used for DMA mapping */
	struct net_device *netdev;	/* netdev ring maps to */
	struct ice_vsi *vsi;		/* Backreference to associated VSI */
	struct ice_q_vector *q_vector;	/* Backreference to associated vector */
	u8 __iomem *tail;
	u16 q_index;			/* Queue number of ring */

	u16 count;			/* Number of descriptors */
	u16 reg_idx;			/* HW register index of the ring */
	u16 next_to_alloc;

	union {
		struct ice_rx_buf *rx_buf;
		struct xdp_buff **xdp_buf;
	};
	/* CL2 - 2nd cacheline starts here */
	union {
		struct ice_xdp_buff xdp_ext;
		struct xdp_buff xdp;
	};
	/* CL3 - 3rd cacheline starts here */
	union {
		struct ice_pkt_ctx pkt_ctx;
		struct {
			u64 cached_phctime;
			__be16 vlan_proto;
		};
	};
	struct bpf_prog *xdp_prog;
	u16 rx_offset;

	/* used in interrupt processing */
	u16 next_to_use;
	u16 next_to_clean;
	u16 first_desc;

	/* stats structs */
	struct ice_ring_stats *ring_stats;

	struct rcu_head rcu;		/* to avoid race on free */
	/* CL4 - 4th cacheline starts here */
	struct ice_channel *ch;
	struct ice_tx_ring *xdp_ring;
	struct ice_rx_ring *next;	/* pointer to next ring in q_vector */
	struct xsk_buff_pool *xsk_pool;
	u32 nr_frags;
	dma_addr_t dma;			/* physical address of ring */
	u16 rx_buf_len;
	u8 dcb_tc;			/* Traffic class of ring */
	u8 ptp_rx;
#define ICE_RX_FLAGS_RING_BUILD_SKB	BIT(1)
#define ICE_RX_FLAGS_CRC_STRIP_DIS	BIT(2)
	u8 flags;
	/* CL5 - 5th cacheline starts here */
	struct xdp_rxq_info xdp_rxq;
} ____cacheline_internodealigned_in_smp;
<br>→ next 포인터는 다음 ice_rx_ring을 가르키는 포인터임. 고로 linked list 형태]]></description><link>encyclopedia-of-networksystem/struct/drivers-net-ethernet-intel-ice/ice_rx_ring.html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Struct/drivers-net-ethernet-intel-ice/ice_rx_ring.md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_tx_ring]]></title><description><![CDATA[ 
 <br>struct ice_tx_ring {
	/* CL1 - 1st cacheline starts here */
	struct ice_tx_ring *next;	/* pointer to next ring in q_vector */
	void *desc;			/* Descriptor ring memory */
	struct device *dev;		/* Used for DMA mapping */
	u8 __iomem *tail;
	struct ice_tx_buf *tx_buf;
	struct ice_q_vector *q_vector;	/* Backreference to associated vector */
	struct net_device *netdev;	/* netdev ring maps to */
	struct ice_vsi *vsi;		/* Backreference to associated VSI */
	/* CL2 - 2nd cacheline starts here */
	dma_addr_t dma;			/* physical address of ring */
	struct xsk_buff_pool *xsk_pool;
	u16 next_to_use;
	u16 next_to_clean;
	u16 q_handle;			/* Queue handle per TC */
	u16 reg_idx;			/* HW register index of the ring */
	u16 count;			/* Number of descriptors */
	u16 q_index;			/* Queue number of ring */
	u16 xdp_tx_active;
	/* stats structs */
	struct ice_ring_stats *ring_stats;
	/* CL3 - 3rd cacheline starts here */
	struct rcu_head rcu;		/* to avoid race on free */
	DECLARE_BITMAP(xps_state, ICE_TX_NBITS);	/* XPS Config State */
	struct ice_channel *ch;
	struct ice_ptp_tx *tx_tstamps;
	spinlock_t tx_lock;
	u32 txq_teid;			/* Added Tx queue TEID */
	/* CL4 - 4th cacheline starts here */
#define ICE_TX_FLAGS_RING_XDP		BIT(0)
#define ICE_TX_FLAGS_RING_VLAN_L2TAG1	BIT(1)
#define ICE_TX_FLAGS_RING_VLAN_L2TAG2	BIT(2)
	u8 flags;
	u8 dcb_tc;			/* Traffic class of ring */
} ____cacheline_internodealigned_in_smp;
<br>→ next 포인터는 다음 ice_tx_ring을 가르키는 포인터임. 고로 linked list 형태]]></description><link>encyclopedia-of-networksystem/struct/drivers-net-ethernet-intel-ice/ice_tx_ring.html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Struct/drivers-net-ethernet-intel-ice/ice_tx_ring.md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[ice_vsi]]></title><description><![CDATA[ 
 <br>/* struct that defines a VSI, associated with a dev */
struct ice_vsi {
	struct net_device *netdev;
	struct ice_sw *vsw;		 /* switch this VSI is on */
	struct ice_pf *back;		 /* back pointer to PF */
	struct ice_port_info *port_info; /* back pointer to port_info */
	struct ice_rx_ring **rx_rings;	 /* Rx ring array */
	struct ice_tx_ring **tx_rings;	 /* Tx ring array */
	struct ice_q_vector **q_vectors; /* q_vector array */
	// [[Encyclopedia of NetworkSystem/Struct/drivers-net-ethernet-intel-ice/ice_q_vector.md|ice_q_vector]]

	irqreturn_t (*irq_handler)(int irq, void *data);

	u64 tx_linearize;
	DECLARE_BITMAP(state, ICE_VSI_STATE_NBITS);
	unsigned int current_netdev_flags;
	u32 tx_restart;
	u32 tx_busy;
	u32 rx_buf_failed;
	u32 rx_page_failed;
	u16 num_q_vectors;
	/* tell if only dynamic irq allocation is allowed */
	bool irq_dyn_alloc;

	enum ice_vsi_type type;
	u16 vsi_num;			/* HW (absolute) index of this VSI */
	u16 idx;			/* software index in pf-&gt;vsi[] */

	struct ice_vf *vf;		/* VF associated with this VSI */

	u16 num_gfltr;
	u16 num_bfltr;

	/* RSS config */
	u16 rss_table_size;	/* HW RSS table size */
	u16 rss_size;		/* Allocated RSS queues */
	u8 rss_hfunc;		/* User configured hash type */
	u8 *rss_hkey_user;	/* User configured hash keys */
	u8 *rss_lut_user;	/* User configured lookup table entries */
	u8 rss_lut_type;	/* used to configure Get/Set RSS LUT AQ call */

	/* aRFS members only allocated for the PF VSI */
#define ICE_MAX_ARFS_LIST	1024
#define ICE_ARFS_LST_MASK	(ICE_MAX_ARFS_LIST - 1)
	struct hlist_head *arfs_fltr_list;
	struct ice_arfs_active_fltr_cntrs *arfs_fltr_cntrs;
	spinlock_t arfs_lock;	/* protects aRFS hash table and filter state */
	atomic_t *arfs_last_fltr_id;

	u16 max_frame;
	u16 rx_buf_len;

	struct ice_aqc_vsi_props info;	 /* VSI properties */
	struct ice_vsi_vlan_info vlan_info;	/* vlan config to be restored */

	/* VSI stats */
	struct rtnl_link_stats64 net_stats;
	struct rtnl_link_stats64 net_stats_prev;
	struct ice_eth_stats eth_stats;
	struct ice_eth_stats eth_stats_prev;

	struct list_head tmp_sync_list;		/* MAC filters to be synced */
	struct list_head tmp_unsync_list;	/* MAC filters to be unsynced */

	u8 irqs_ready:1;
	u8 current_isup:1;		 /* Sync 'link up' logging */
	u8 stat_offsets_loaded:1;
	struct ice_vsi_vlan_ops inner_vlan_ops;
	struct ice_vsi_vlan_ops outer_vlan_ops;
	u16 num_vlan;

	/* queue information */
	u8 tx_mapping_mode;		 /* ICE_MAP_MODE_[CONTIG|SCATTER] */
	u8 rx_mapping_mode;		 /* ICE_MAP_MODE_[CONTIG|SCATTER] */
	u16 *txq_map;			 /* index in pf-&gt;avail_txqs */
	u16 *rxq_map;			 /* index in pf-&gt;avail_rxqs */
	u16 alloc_txq;			 /* Allocated Tx queues */
	u16 num_txq;			 /* Used Tx queues */
	u16 alloc_rxq;			 /* Allocated Rx queues */
	u16 num_rxq;			 /* Used Rx queues */
	u16 req_txq;			 /* User requested Tx queues */
	u16 req_rxq;			 /* User requested Rx queues */
	u16 num_rx_desc;
	u16 num_tx_desc;
	u16 qset_handle[ICE_MAX_TRAFFIC_CLASS];
	struct ice_tc_cfg tc_cfg;
	struct bpf_prog *xdp_prog;
	struct ice_tx_ring **xdp_rings;	 /* XDP ring array */
	unsigned long *af_xdp_zc_qps;	 /* tracks AF_XDP ZC enabled qps */
	u16 num_xdp_txq;		 /* Used XDP queues */
	u8 xdp_mapping_mode;		 /* ICE_MAP_MODE_[CONTIG|SCATTER] */

	struct net_device **target_netdevs;

	struct tc_mqprio_qopt_offload mqprio_qopt; /* queue parameters */

	/* Channel Specific Fields */
	struct ice_vsi *tc_map_vsi[ICE_CHNL_MAX_TC];
	u16 cnt_q_avail;
	u16 next_base_q;	/* next queue to be used for channel setup */
	struct list_head ch_list;
	u16 num_chnl_rxq;
	u16 num_chnl_txq;
	u16 ch_rss_size;
	u16 num_chnl_fltr;
	/* store away rss size info before configuring ADQ channels so that,
	 * it can be used after tc-qdisc delete, to get back RSS setting as
	 * they were before
	 */
	u16 orig_rss_size;
	/* this keeps tracks of all enabled TC with and without DCB
	 * and inclusive of ADQ, vsi-&gt;mqprio_opt keeps track of queue
	 * information
	 */
	u8 all_numtc;
	u16 all_enatc;

	/* store away TC info, to be used for rebuild logic */
	u8 old_numtc;
	u16 old_ena_tc;

	struct ice_channel *ch;

	/* setup back reference, to which aggregator node this VSI
	 * corresponds to
	 */
	struct ice_agg_node *agg_node;
} ____cacheline_internodealigned_in_smp;
<br><a data-tooltip-position="top" aria-label="Encyclopedia of NetworkSystem/Struct/drivers-net-ethernet-intel-ice/ice_q_vector.md" data-href="Encyclopedia of NetworkSystem/Struct/drivers-net-ethernet-intel-ice/ice_q_vector.md" href="encyclopedia-of-networksystem/struct/drivers-net-ethernet-intel-ice/ice_q_vector.html" class="internal-link" target="_self" rel="noopener nofollow">ice_q_vector</a>]]></description><link>encyclopedia-of-networksystem/struct/drivers-net-ethernet-intel-ice/ice_vsi.html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Struct/drivers-net-ethernet-intel-ice/ice_vsi.md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[<strong>주요 필드 설명</strong>]]></title><description><![CDATA[ 
 <br>struct device {
	struct kobject kobj;
	struct device		*parent;

	struct device_private	*p;

	const char		*init_name; /* initial name of the device */
	const struct device_type *type;

	const struct bus_type	*bus;	/* type of bus device is on */
	struct device_driver *driver;	/* which driver has allocated this
					   device */
	void		*platform_data;	/* Platform specific data, device
					   core doesn't touch it */
	void		*driver_data;	/* Driver data, set and get with
					   dev_set_drvdata/dev_get_drvdata */
	struct mutex		mutex;	/* mutex to synchronize calls to
					 * its driver.
					 */

	struct dev_links_info	links;
	struct dev_pm_info	power;
	struct dev_pm_domain	*pm_domain;

#ifdef CONFIG_ENERGY_MODEL
	struct em_perf_domain	*em_pd;
#endif

#ifdef CONFIG_PINCTRL
	struct dev_pin_info	*pins;
#endif
	struct dev_msi_info	msi;
#ifdef CONFIG_DMA_OPS
	const struct dma_map_ops *dma_ops;
#endif
	u64		*dma_mask;	/* dma mask (if dma'able device) */
	u64		coherent_dma_mask;/* Like dma_mask, but for
					     alloc_coherent mappings as
					     not all hardware supports
					     64 bit addresses for consistent
					     allocations such descriptors. */
	u64		bus_dma_limit;	/* upstream dma constraint */
	const struct bus_dma_region *dma_range_map;

	struct device_dma_parameters *dma_parms;

	struct list_head	dma_pools;	/* dma pools (if dma'ble) */

#ifdef CONFIG_DMA_DECLARE_COHERENT
	struct dma_coherent_mem	*dma_mem; /* internal for coherent mem
					     override */
#endif
#ifdef CONFIG_DMA_CMA
	struct cma *cma_area;		/* contiguous memory area for dma
					   allocations */
#endif
#ifdef CONFIG_SWIOTLB
	struct io_tlb_mem *dma_io_tlb_mem;
#endif
#ifdef CONFIG_SWIOTLB_DYNAMIC
	struct list_head dma_io_tlb_pools;
	spinlock_t dma_io_tlb_lock;
	bool dma_uses_io_tlb;
#endif
	/* arch specific additions */
	struct dev_archdata	archdata;

	struct device_node	*of_node; /* associated device tree node */
	struct fwnode_handle	*fwnode; /* firmware device node */

#ifdef CONFIG_NUMA
	int		numa_node;	/* NUMA node this device is close to */
#endif
	dev_t			devt;	/* dev_t, creates the sysfs "dev" */
	u32			id;	/* device instance */

	spinlock_t		devres_lock;
	struct list_head	devres_head;

	const struct class	*class;
	const struct attribute_group **groups;	/* optional groups */

	void	(*release)(struct device *dev);
	struct iommu_group	*iommu_group;
	struct dev_iommu	*iommu;

	struct device_physical_location *physical_location;

	enum device_removable	removable;

	bool			offline_disabled:1;
	bool			offline:1;
	bool			of_node_reused:1;
	bool			state_synced:1;
	bool			can_match:1;
#if defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_DEVICE) || \
    defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU) || \
    defined(CONFIG_ARCH_HAS_SYNC_DMA_FOR_CPU_ALL)
	bool			dma_coherent:1;
#endif
#ifdef CONFIG_DMA_OPS_BYPASS
	bool			dma_ops_bypass : 1;
#endif
};
<br>struct device는 리눅스 커널에서 사용되는 중요한 구조체로, 다양한 장치(device)의 정보를 관리하고 제어하는 데 사용된다. 이 구조체는 장치의 속성, 상태, 드라이버와의 연계, DMA 설정, 전원 관리 등 다양한 측면을 포괄한다. 각 필드는 특정한 기능이나 정보를 나타내며, 이를 통해 커널이 장치를 효율적으로 관리할 수 있게 한다.<br><br><br>
<br>struct kobject kobj: 커널 객체(kobject)로, 장치를 sysfs에 노출하고 관리하는 데 사용된다.
<br>struct device *parent: 부모 장치에 대한 포인터로, 장치 계층 구조를 형성하는 데 사용된다.
<br>const char *init_name: 장치의 초기 이름.
<br>const struct device_type *type: 장치 유형을 나타내는 구조체 포인터.
<br><br>
<br>const struct bus_type *bus: 장치가 연결된 버스 유형.
<br>struct device_driver *driver: 장치를 관리하는 드라이버에 대한 포인터.
<br>void *platform_data: 플랫폼 특화 데이터를 저장하는 포인터.
<br>void *driver_data: 드라이버 특화 데이터를 저장하는 포인터로, dev_set_drvdata와 dev_get_drvdata 함수를 통해 접근할 수 있다.
<br><br>
<br>struct mutex mutex: 장치 드라이버 호출을 동기화하는 뮤텍스.
<br>struct dev_pm_info power: 전원 관리 정보.
<br>struct dev_pm_domain *pm_domain: 전원 관리 도메인에 대한 포인터.
<br><br>
<br>const struct dma_map_ops *dma_ops: DMA 맵핑 연산에 대한 포인터.
<br>u64 *dma_mask: DMA 마스크로, 장치가 사용할 수 있는 DMA 주소 범위를 나타낸다.
<br>u64 coherent_dma_mask: 일관된 DMA 매핑을 위한 마스크.
<br>u64 bus_dma_limit: 업스트림 DMA 제한.
<br>const struct bus_dma_region *dma_range_map: DMA 범위 맵.
<br>struct device_dma_parameters *dma_parms: DMA 매개변수.
<br>struct list_head dma_pools: DMA 풀 리스트.
<br>struct dma_coherent_mem *dma_mem: 일관된 DMA 메모리 오버라이드.
<br>struct cma *cma_area: DMA 할당을 위한 연속 메모리 영역.
<br>struct io_tlb_mem *dma_io_tlb_mem: IO TLB 메모리.
<br>struct list_head dma_io_tlb_pools: IO TLB 풀 리스트.
<br>spinlock_t dma_io_tlb_lock: IO TLB 락.
<br>bool dma_uses_io_tlb: IO TLB 사용 여부.
<br><br>
<br>struct device_node *of_node: 장치 트리 노드.
<br>struct fwnode_handle *fwnode: 펌웨어 장치 노드.
<br>int numa_node: 장치가 가까운 NUMA 노드.
<br><br>
<br>dev_t devt: 장치 번호.
<br>u32 id: 장치 인스턴스 ID.
<br>const struct class *class: 장치 클래스.
<br>const struct attribute_group **groups: 선택적 속성 그룹.
<br><br>
<br>spinlock_t devres_lock: 장치 리소스 잠금.
<br>struct list_head devres_head: 장치 리소스 리스트.
<br><br>
<br>struct iommu_group *iommu_group: IOMMU 그룹.
<br>struct dev_iommu *iommu: IOMMU 정보.
<br>struct device_physical_location *physical_location: 물리적 위치.
<br><br>
<br>void (*release)(struct device *dev): 장치가 해제될 때 호출되는 함수 포인터.
<br>enum device_removable removable: 장치 제거 가능 여부.
<br>bool offline_disabled: 오프라인 비활성화 여부.
<br>bool offline: 오프라인 여부.
<br>bool of_node_reused: 노드 재사용 여부.
<br>bool state_synced: 상태 동기화 여부.
<br>bool can_match: 일치 가능 여부.
<br>bool dma_coherent: DMA 일관성 여부.
<br>bool dma_ops_bypass: DMA 연산 우회 여부.
<br><br>struct device는 리눅스 커널에서 다양한 장치의 상태와 속성을 관리하는 핵심 데이터 구조체이다. 이 구조체는 장치 계층 구조를 형성하고, 버스 및 드라이버와의 연계를 제공하며, 전원 관리, DMA 설정, NUMA 노드, 장치 트리 등의 다양한 기능을 포함한다. 이 구조체를 통해 커널은 장치의 효율적인 관리와 제어를 수행할 수 있다.]]></description><link>encyclopedia-of-networksystem/struct/include-linux/device.html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Struct/include-linux/device.md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[gro_result]]></title><description><![CDATA[ 
 <br>enum gro_result {
	GRO_MERGED,
	GRO_MERGED_FREE,
	GRO_HELD,
	GRO_NORMAL,
	GRO_CONSUMED,
};
typedef enum gro_result gro_result_t;
]]></description><link>encyclopedia-of-networksystem/struct/include-linux/gro_result.html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Struct/include-linux/gro_result.md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[msi_map]]></title><description><![CDATA[ 
 <br>/**
 * msi_map - Mapping between MSI index and Linux interrupt number
 * @index:	The MSI index, e.g. slot in the MSI-X table or
 *		a software managed index if &gt;= 0. If negative
 *		the allocation function failed and it contains
 *		the error code.
 * @virq:	The associated Linux interrupt number
 */
struct msi_map {
	int	index;
	int	virq;
};
<br>virq는 관련된 interrupt number, index는 msi-x의 테이블이나 소프트웨어로 관리되는 인덱스임.]]></description><link>encyclopedia-of-networksystem/struct/include-linux/msi_map.html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Struct/include-linux/msi_map.md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[napi_struct]]></title><description><![CDATA[ 
 <br>/*
 * Structure for NAPI scheduling similar to tasklet but with weighting
 */
struct napi_struct {
	/* The poll_list must only be managed by the entity which
	 * changes the state of the NAPI_STATE_SCHED bit.  This means
	 * whoever atomically sets that bit can add this napi_struct
	 * to the per-CPU poll_list, and whoever clears that bit
	 * can remove from the list right before clearing the bit.
	 */
	struct list_head	poll_list;

	unsigned long		state;
	int			weight;
	int			defer_hard_irqs_count;
	unsigned long		gro_bitmask;
	int			(*poll)(struct napi_struct *, int);
#ifdef CONFIG_NETPOLL
	/* CPU actively polling if netpoll is configured */
	int			poll_owner;
#endif
	/* CPU on which NAPI has been scheduled for processing */
	int			list_owner;
	struct net_device	*dev;
	struct gro_list		gro_hash[GRO_HASH_BUCKETS];
	struct sk_buff		*skb;
	struct list_head	rx_list; /* Pending GRO_NORMAL skbs */
	int			rx_count; /* length of rx_list */
	unsigned int		napi_id;
	struct hrtimer		timer;
	struct task_struct	*thread;
	/* control-path-only fields follow */
	struct list_head	dev_list;
	struct hlist_node	napi_hash_node;
	int			irq;
};
]]></description><link>encyclopedia-of-networksystem/struct/include-linux/napi_struct.html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Struct/include-linux/napi_struct.md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[net_device]]></title><description><![CDATA[ 
 <br>struct net_device {
	/* Cacheline organization can be found documented in
	 * Documentation/networking/net_cachelines/net_device.rst.
	 * Please update the document when adding new fields.
	 */

	/* TX read-mostly hotpath */
	__cacheline_group_begin(net_device_read_tx);
	unsigned long long	priv_flags;
	const struct net_device_ops *netdev_ops;
	const struct header_ops *header_ops;
	struct netdev_queue	*_tx;
	netdev_features_t	gso_partial_features;
	unsigned int		real_num_tx_queues;
	unsigned int		gso_max_size;
	unsigned int		gso_ipv4_max_size;
	u16			gso_max_segs;
	s16			num_tc;
	/* Note : dev-&gt;mtu is often read without holding a lock.
	 * Writers usually hold RTNL.
	 * It is recommended to use READ_ONCE() to annotate the reads,
	 * and to use WRITE_ONCE() to annotate the writes.
	 */
	unsigned int		mtu;
	unsigned short		needed_headroom;
	struct netdev_tc_txq	tc_to_txq[TC_MAX_QUEUE];
#ifdef CONFIG_XPS
	struct xps_dev_maps __rcu *xps_maps[XPS_MAPS_MAX];
#endif
#ifdef CONFIG_NETFILTER_EGRESS
	struct nf_hook_entries __rcu *nf_hooks_egress;
#endif
#ifdef CONFIG_NET_XGRESS
	struct bpf_mprog_entry __rcu *tcx_egress;
#endif
	__cacheline_group_end(net_device_read_tx);

	/* TXRX read-mostly hotpath */
	__cacheline_group_begin(net_device_read_txrx);
	union {
		struct pcpu_lstats __percpu		*lstats;
		struct pcpu_sw_netstats __percpu	*tstats;
		struct pcpu_dstats __percpu		*dstats;
	};
	unsigned long		state;
	unsigned int		flags;
	unsigned short		hard_header_len;
	netdev_features_t	features;
	struct inet6_dev __rcu	*ip6_ptr;
	__cacheline_group_end(net_device_read_txrx);

	/* RX read-mostly hotpath */
	__cacheline_group_begin(net_device_read_rx);
	struct bpf_prog __rcu	*xdp_prog;
	struct list_head	ptype_specific;
	int			ifindex;
	unsigned int		real_num_rx_queues;
	struct netdev_rx_queue	*_rx;
	unsigned long		gro_flush_timeout;
	int			napi_defer_hard_irqs;
	unsigned int		gro_max_size;
	unsigned int		gro_ipv4_max_size;
	rx_handler_func_t __rcu	*rx_handler;
	void __rcu		*rx_handler_data;
	possible_net_t			nd_net;
#ifdef CONFIG_NETPOLL
	struct netpoll_info __rcu	*npinfo;
#endif
#ifdef CONFIG_NET_XGRESS
	struct bpf_mprog_entry __rcu *tcx_ingress;
#endif
	__cacheline_group_end(net_device_read_rx);

	char			name[IFNAMSIZ];
	struct netdev_name_node	*name_node;
	struct dev_ifalias	__rcu *ifalias;
	/*
	 *	I/O specific fields
	 *	FIXME: Merge these and struct ifmap into one
	 */
	unsigned long		mem_end;
	unsigned long		mem_start;
	unsigned long		base_addr;

	/*
	 *	Some hardware also needs these fields (state,dev_list,
	 *	napi_list,unreg_list,close_list) but they are not
	 *	part of the usual set specified in Space.c.
	 */


	struct list_head	dev_list;
	struct list_head	napi_list;
	struct list_head	unreg_list;
	struct list_head	close_list;
	struct list_head	ptype_all;

	struct {
		struct list_head upper;
		struct list_head lower;
	} adj_list;

	/* Read-mostly cache-line for fast-path access */
	xdp_features_t		xdp_features;
	const struct xdp_metadata_ops *xdp_metadata_ops;
	const struct xsk_tx_metadata_ops *xsk_tx_metadata_ops;
	unsigned short		gflags;

	unsigned short		needed_tailroom;

	netdev_features_t	hw_features;
	netdev_features_t	wanted_features;
	netdev_features_t	vlan_features;
	netdev_features_t	hw_enc_features;
	netdev_features_t	mpls_features;

	unsigned int		min_mtu;
	unsigned int		max_mtu;
	unsigned short		type;
	unsigned char		min_header_len;
	unsigned char		name_assign_type;

	int			group;

	struct net_device_stats	stats; /* not used by modern drivers */

	struct net_device_core_stats __percpu *core_stats;

	/* Stats to monitor link on/off, flapping */
	atomic_t		carrier_up_count;
	atomic_t		carrier_down_count;

#ifdef CONFIG_WIRELESS_EXT
	const struct iw_handler_def *wireless_handlers;
	struct iw_public_data	*wireless_data;
#endif
	const struct ethtool_ops *ethtool_ops;
#ifdef CONFIG_NET_L3_MASTER_DEV
	const struct l3mdev_ops	*l3mdev_ops;
#endif
#if IS_ENABLED(CONFIG_IPV6)
	const struct ndisc_ops *ndisc_ops;
#endif

#ifdef CONFIG_XFRM_OFFLOAD
	const struct xfrmdev_ops *xfrmdev_ops;
#endif

#if IS_ENABLED(CONFIG_TLS_DEVICE)
	const struct tlsdev_ops *tlsdev_ops;
#endif

	unsigned int		operstate;
	unsigned char		link_mode;

	unsigned char		if_port;
	unsigned char		dma;

	/* Interface address info. */
	unsigned char		perm_addr[MAX_ADDR_LEN];
	unsigned char		addr_assign_type;
	unsigned char		addr_len;
	unsigned char		upper_level;
	unsigned char		lower_level;

	unsigned short		neigh_priv_len;
	unsigned short          dev_id;
	unsigned short          dev_port;
	unsigned short		padded;

	spinlock_t		addr_list_lock;
	int			irq;

	struct netdev_hw_addr_list	uc;
	struct netdev_hw_addr_list	mc;
	struct netdev_hw_addr_list	dev_addrs;

#ifdef CONFIG_SYSFS
	struct kset		*queues_kset;
#endif
#ifdef CONFIG_LOCKDEP
	struct list_head	unlink_list;
#endif
	unsigned int		promiscuity;
	unsigned int		allmulti;
	bool			uc_promisc;
#ifdef CONFIG_LOCKDEP
	unsigned char		nested_level;
#endif


	/* Protocol-specific pointers */
	struct in_device __rcu	*ip_ptr;
#if IS_ENABLED(CONFIG_VLAN_8021Q)
	struct vlan_info __rcu	*vlan_info;
#endif
#if IS_ENABLED(CONFIG_NET_DSA)
	struct dsa_port		*dsa_ptr;
#endif
#if IS_ENABLED(CONFIG_TIPC)
	struct tipc_bearer __rcu *tipc_ptr;
#endif
#if IS_ENABLED(CONFIG_ATALK)
	void 			*atalk_ptr;
#endif
#if IS_ENABLED(CONFIG_AX25)
	void			*ax25_ptr;
#endif
#if IS_ENABLED(CONFIG_CFG80211)
	struct wireless_dev	*ieee80211_ptr;
#endif
#if IS_ENABLED(CONFIG_IEEE802154) || IS_ENABLED(CONFIG_6LOWPAN)
	struct wpan_dev		*ieee802154_ptr;
#endif
#if IS_ENABLED(CONFIG_MPLS_ROUTING)
	struct mpls_dev __rcu	*mpls_ptr;
#endif
#if IS_ENABLED(CONFIG_MCTP)
	struct mctp_dev __rcu	*mctp_ptr;
#endif

/*
 * Cache lines mostly used on receive path (including eth_type_trans())
 */
	/* Interface address info used in eth_type_trans() */
	const unsigned char	*dev_addr;

	unsigned int		num_rx_queues;
#define GRO_LEGACY_MAX_SIZE	65536u
/* TCP minimal MSS is 8 (TCP_MIN_GSO_SIZE),
 * and shinfo-&gt;gso_segs is a 16bit field.
 */
#define GRO_MAX_SIZE		(8 * 65535u)
	unsigned int		xdp_zc_max_segs;
	struct netdev_queue __rcu *ingress_queue;
#ifdef CONFIG_NETFILTER_INGRESS
	struct nf_hook_entries __rcu *nf_hooks_ingress;
#endif

	unsigned char		broadcast[MAX_ADDR_LEN];
#ifdef CONFIG_RFS_ACCEL
	struct cpu_rmap		*rx_cpu_rmap;
#endif
	struct hlist_node	index_hlist;

/*
 * Cache lines mostly used on transmit path
 */
	unsigned int		num_tx_queues;
	struct Qdisc __rcu	*qdisc;
	unsigned int		tx_queue_len;
	spinlock_t		tx_global_lock;

	struct xdp_dev_bulk_queue __percpu *xdp_bulkq;

#ifdef CONFIG_NET_SCHED
	DECLARE_HASHTABLE	(qdisc_hash, 4);
#endif
	/* These may be needed for future network-power-down code. */
	struct timer_list	watchdog_timer;
	int			watchdog_timeo;

	u32                     proto_down_reason;

	struct list_head	todo_list;

#ifdef CONFIG_PCPU_DEV_REFCNT
	int __percpu		*pcpu_refcnt;
#else
	refcount_t		dev_refcnt;
#endif
	struct ref_tracker_dir	refcnt_tracker;

	struct list_head	link_watch_list;

	u8 reg_state;

	bool dismantle;

	enum {
		RTNL_LINK_INITIALIZED,
		RTNL_LINK_INITIALIZING,
	} rtnl_link_state:16;

	bool needs_free_netdev;
	void (*priv_destructor)(struct net_device *dev);

	/* mid-layer private */
	void				*ml_priv;
	enum netdev_ml_priv_type	ml_priv_type;

	enum netdev_stat_type		pcpu_stat_type:8;

#if IS_ENABLED(CONFIG_GARP)
	struct garp_port __rcu	*garp_port;
#endif
#if IS_ENABLED(CONFIG_MRP)
	struct mrp_port __rcu	*mrp_port;
#endif
#if IS_ENABLED(CONFIG_NET_DROP_MONITOR)
	struct dm_hw_stat_delta __rcu *dm_private;
#endif
	struct device		dev;
	const struct attribute_group *sysfs_groups[4];
	const struct attribute_group *sysfs_rx_queue_group;

	const struct rtnl_link_ops *rtnl_link_ops;

	const struct netdev_stat_ops *stat_ops;

	/* for setting kernel sock attribute on TCP connection setup */
#define GSO_MAX_SEGS		65535u
#define GSO_LEGACY_MAX_SIZE	65536u
/* TCP minimal MSS is 8 (TCP_MIN_GSO_SIZE),
 * and shinfo-&gt;gso_segs is a 16bit field.
 */
#define GSO_MAX_SIZE		(8 * GSO_MAX_SEGS)

#define TSO_LEGACY_MAX_SIZE	65536
#define TSO_MAX_SIZE		UINT_MAX
	unsigned int		tso_max_size;
#define TSO_MAX_SEGS		U16_MAX
	u16			tso_max_segs;

#ifdef CONFIG_DCB
	const struct dcbnl_rtnl_ops *dcbnl_ops;
#endif
	u8			prio_tc_map[TC_BITMASK + 1];

#if IS_ENABLED(CONFIG_FCOE)
	unsigned int		fcoe_ddp_xid;
#endif
#if IS_ENABLED(CONFIG_CGROUP_NET_PRIO)
	struct netprio_map __rcu *priomap;
#endif
	struct phy_device	*phydev;
	struct sfp_bus		*sfp_bus;
	struct lock_class_key	*qdisc_tx_busylock;
	bool			proto_down;
	unsigned		wol_enabled:1;
	unsigned		threaded:1;

	struct list_head	net_notifier_list;

#if IS_ENABLED(CONFIG_MACSEC)
	/* MACsec management functions */
	const struct macsec_ops *macsec_ops;
#endif
	const struct udp_tunnel_nic_info	*udp_tunnel_nic_info;
	struct udp_tunnel_nic	*udp_tunnel_nic;

	/* protected by rtnl_lock */
	struct bpf_xdp_entity	xdp_state[__MAX_XDP_MODE];

	u8 dev_addr_shadow[MAX_ADDR_LEN];
	netdevice_tracker	linkwatch_dev_tracker;
	netdevice_tracker	watchdog_dev_tracker;
	netdevice_tracker	dev_registered_tracker;
	struct rtnl_hw_stats64	*offload_xstats_l3;

	struct devlink_port	*devlink_port;

#if IS_ENABLED(CONFIG_DPLL)
	struct dpll_pin	__rcu	*dpll_pin;
#endif
#if IS_ENABLED(CONFIG_PAGE_POOL)
	/** @page_pools: page pools created for this netdevice */
	struct hlist_head	page_pools;
#endif
};
]]></description><link>encyclopedia-of-networksystem/struct/include-linux/net_device.html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Struct/include-linux/net_device.md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[packet_offload]]></title><description><![CDATA[ 
 <br>struct packet_offload {
	__be16			 type;	/* This is really htons(ether_type). */
	u16			 priority;
	struct offload_callbacks callbacks;
	struct list_head	 list;
};
]]></description><link>encyclopedia-of-networksystem/struct/include-linux/packet_offload.html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Struct/include-linux/packet_offload.md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[<strong>주요 필드 설명</strong>]]></title><description><![CDATA[ 
 <br>/* The pci_dev structure describes PCI devices */
struct pci_dev {
	struct list_head bus_list;	/* Node in per-bus list */
	struct pci_bus	*bus;		/* Bus this device is on */
	struct pci_bus	*subordinate;	/* Bus this device bridges to */

	void		*sysdata;	/* Hook for sys-specific extension */
	struct proc_dir_entry *procent;	/* Device entry in /proc/bus/pci */
	struct pci_slot	*slot;		/* Physical slot this device is in */

	unsigned int	devfn;		/* Encoded device &amp; function index */
	unsigned short	vendor;
	unsigned short	device;
	unsigned short	subsystem_vendor;
	unsigned short	subsystem_device;
	unsigned int	class;		/* 3 bytes: (base,sub,prog-if) */
	u8		revision;	/* PCI revision, low byte of class word */
	u8		hdr_type;	/* PCI header type (`multi' flag masked out) */
#ifdef CONFIG_PCIEAER
	u16		aer_cap;	/* AER capability offset */
	struct aer_stats *aer_stats;	/* AER stats for this device */
#endif
#ifdef CONFIG_PCIEPORTBUS
	struct rcec_ea	*rcec_ea;	/* RCEC cached endpoint association */
	struct pci_dev  *rcec;          /* Associated RCEC device */
#endif
	u32		devcap;		/* PCIe Device Capabilities */
	u8		pcie_cap;	/* PCIe capability offset */
	u8		msi_cap;	/* MSI capability offset */
	u8		msix_cap;	/* MSI-X capability offset */
	u8		pcie_mpss:3;	/* PCIe Max Payload Size Supported */
	u8		rom_base_reg;	/* Config register controlling ROM */
	u8		pin;		/* Interrupt pin this device uses */
	u16		pcie_flags_reg;	/* Cached PCIe Capabilities Register */
	unsigned long	*dma_alias_mask;/* Mask of enabled devfn aliases */

	struct pci_driver *driver;	/* Driver bound to this device */
	u64		dma_mask;	/* Mask of the bits of bus address this
					   device implements.  Normally this is
					   0xffffffff.  You only need to change
					   this if your device has broken DMA
					   or supports 64-bit transfers.  */

	struct device_dma_parameters dma_parms;

	pci_power_t	current_state;	/* Current operating state. In ACPI,
					   this is D0-D3, D0 being fully
					   functional, and D3 being off. */
	u8		pm_cap;		/* PM capability offset */
	unsigned int	imm_ready:1;	/* Supports Immediate Readiness */
	unsigned int	pme_support:5;	/* Bitmask of states from which PME#
					   can be generated */
	unsigned int	pme_poll:1;	/* Poll device's PME status bit */
	unsigned int	d1_support:1;	/* Low power state D1 is supported */
	unsigned int	d2_support:1;	/* Low power state D2 is supported */
	unsigned int	no_d1d2:1;	/* D1 and D2 are forbidden */
	unsigned int	no_d3cold:1;	/* D3cold is forbidden */
	unsigned int	bridge_d3:1;	/* Allow D3 for bridge */
	unsigned int	d3cold_allowed:1;	/* D3cold is allowed by user */
	unsigned int	mmio_always_on:1;	/* Disallow turning off io/mem
						   decoding during BAR sizing */
	unsigned int	wakeup_prepared:1;
	unsigned int	skip_bus_pm:1;	/* Internal: Skip bus-level PM */
	unsigned int	ignore_hotplug:1;	/* Ignore hotplug events */
	unsigned int	hotplug_user_indicators:1; /* SlotCtl indicators
						      controlled exclusively by
						      user sysfs */
	unsigned int	clear_retrain_link:1;	/* Need to clear Retrain Link
						   bit manually */
	unsigned int	d3hot_delay;	/* D3hot-&gt;D0 transition time in ms */
	unsigned int	d3cold_delay;	/* D3cold-&gt;D0 transition time in ms */

#ifdef CONFIG_PCIEASPM
	struct pcie_link_state	*link_state;	/* ASPM link state */
	u16		l1ss;		/* L1SS Capability pointer */
	unsigned int	ltr_path:1;	/* Latency Tolerance Reporting
					   supported from root to here */
#endif
	unsigned int	pasid_no_tlp:1;		/* PASID works without TLP Prefix */
	unsigned int	eetlp_prefix_path:1;	/* End-to-End TLP Prefix */

	pci_channel_state_t error_state;	/* Current connectivity state */
	struct device	dev;			/* Generic device interface */

	int		cfg_size;		/* Size of config space */

	/*
	 * Instead of touching interrupt line and base address registers
	 * directly, use the values stored here. They might be different!
	 */
	unsigned int	irq;
	struct resource resource[DEVICE_COUNT_RESOURCE]; /* I/O and memory regions + expansion ROMs */
	struct resource driver_exclusive_resource;	 /* driver exclusive resource ranges */

	bool		match_driver;		/* Skip attaching driver */

	unsigned int	transparent:1;		/* Subtractive decode bridge */
	unsigned int	io_window:1;		/* Bridge has I/O window */
	unsigned int	pref_window:1;		/* Bridge has pref mem window */
	unsigned int	pref_64_window:1;	/* Pref mem window is 64-bit */
	unsigned int	multifunction:1;	/* Multi-function device */

	unsigned int	is_busmaster:1;		/* Is busmaster */
	unsigned int	no_msi:1;		/* May not use MSI */
	unsigned int	no_64bit_msi:1;		/* May only use 32-bit MSIs */
	unsigned int	block_cfg_access:1;	/* Config space access blocked */
	unsigned int	broken_parity_status:1;	/* Generates false positive parity */
	unsigned int	irq_reroute_variant:2;	/* Needs IRQ rerouting variant */
	unsigned int	msi_enabled:1;
	unsigned int	msix_enabled:1;
	unsigned int	ari_enabled:1;		/* ARI forwarding */
	unsigned int	ats_enabled:1;		/* Address Translation Svc */
	unsigned int	pasid_enabled:1;	/* Process Address Space ID */
	unsigned int	pri_enabled:1;		/* Page Request Interface */
	unsigned int	is_managed:1;		/* Managed via devres */
	unsigned int	is_msi_managed:1;	/* MSI release via devres installed */
	unsigned int	needs_freset:1;		/* Requires fundamental reset */
	unsigned int	state_saved:1;
	unsigned int	is_physfn:1;
	unsigned int	is_virtfn:1;
	unsigned int	is_hotplug_bridge:1;
	unsigned int	shpc_managed:1;		/* SHPC owned by shpchp */
	unsigned int	is_thunderbolt:1;	/* Thunderbolt controller */
	/*
	 * Devices marked being untrusted are the ones that can potentially
	 * execute DMA attacks and similar. They are typically connected
	 * through external ports such as Thunderbolt but not limited to
	 * that. When an IOMMU is enabled they should be getting full
	 * mappings to make sure they cannot access arbitrary memory.
	 */
	unsigned int	untrusted:1;
	/*
	 * Info from the platform, e.g., ACPI or device tree, may mark a
	 * device as "external-facing".  An external-facing device is
	 * itself internal but devices downstream from it are external.
	 */
	unsigned int	external_facing:1;
	unsigned int	broken_intx_masking:1;	/* INTx masking can't be used */
	unsigned int	io_window_1k:1;		/* Intel bridge 1K I/O windows */
	unsigned int	irq_managed:1;
	unsigned int	non_compliant_bars:1;	/* Broken BARs; ignore them */
	unsigned int	is_probed:1;		/* Device probing in progress */
	unsigned int	link_active_reporting:1;/* Device capable of reporting link active */
	unsigned int	no_vf_scan:1;		/* Don't scan for VFs after IOV enablement */
	unsigned int	no_command_memory:1;	/* No PCI_COMMAND_MEMORY */
	unsigned int	rom_bar_overlap:1;	/* ROM BAR disable broken */
	unsigned int	rom_attr_enabled:1;	/* Display of ROM attribute enabled? */
	pci_dev_flags_t dev_flags;
	atomic_t	enable_cnt;	/* pci_enable_device has been called */

	spinlock_t	pcie_cap_lock;		/* Protects RMW ops in capability accessors */
	u32		saved_config_space[16]; /* Config space saved at suspend time */
	struct hlist_head saved_cap_space;
	struct bin_attribute *res_attr[DEVICE_COUNT_RESOURCE]; /* sysfs file for resources */
	struct bin_attribute *res_attr_wc[DEVICE_COUNT_RESOURCE]; /* sysfs file for WC mapping of resources */

#ifdef CONFIG_HOTPLUG_PCI_PCIE
	unsigned int	broken_cmd_compl:1;	/* No compl for some cmds */
#endif
#ifdef CONFIG_PCIE_PTM
	u16		ptm_cap;		/* PTM Capability */
	unsigned int	ptm_root:1;
	unsigned int	ptm_enabled:1;
	u8		ptm_granularity;
#endif
#ifdef CONFIG_PCI_MSI
	void __iomem	*msix_base;
	raw_spinlock_t	msi_lock;
#endif
	struct pci_vpd	vpd;
#ifdef CONFIG_PCIE_DPC
	u16		dpc_cap;
	unsigned int	dpc_rp_extensions:1;
	u8		dpc_rp_log_size;
#endif
#ifdef CONFIG_PCI_ATS
	union {
		struct pci_sriov	*sriov;		/* PF: SR-IOV info */
		struct pci_dev		*physfn;	/* VF: related PF */
	};
	u16		ats_cap;	/* ATS Capability offset */
	u8		ats_stu;	/* ATS Smallest Translation Unit */
#endif
#ifdef CONFIG_PCI_PRI
	u16		pri_cap;	/* PRI Capability offset */
	u32		pri_reqs_alloc; /* Number of PRI requests allocated */
	unsigned int	pasid_required:1; /* PRG Response PASID Required */
#endif
#ifdef CONFIG_PCI_PASID
	u16		pasid_cap;	/* PASID Capability offset */
	u16		pasid_features;
#endif
#ifdef CONFIG_PCI_P2PDMA
	struct pci_p2pdma __rcu *p2pdma;
#endif
#ifdef CONFIG_PCI_DOE
	struct xarray	doe_mbs;	/* Data Object Exchange mailboxes */
#endif
	u16		acs_cap;	/* ACS Capability offset */
	phys_addr_t	rom;		/* Physical address if not from BAR */
	size_t		romlen;		/* Length if not from BAR */
	/*
	 * Driver name to force a match.  Do not set directly, because core
	 * frees it.  Use driver_set_override() to set or clear it.
	 */
	const char	*driver_override;

	unsigned long	priv_flags;	/* Private flags for the PCI driver */

	/* These methods index pci_reset_fn_methods[] */
	u8 reset_methods[PCI_NUM_RESET_METHODS]; /* In priority order */
};
<br>struct pci_dev 구조체는 리눅스 커널에서 PCI(Peripheral Component Interconnect) 장치를 나타내는 데이터 구조체이다. 이 구조체는 PCI 장치의 다양한 속성, 상태, 버스 및 드라이버와의 연계, DMA 설정 등을 관리한다. 각 필드는 특정한 기능이나 정보를 나타내며, 이를 통해 커널이 PCI 장치를 효율적으로 관리할 수 있다.<br><br><br>
<br>struct list_head bus_list: 버스 내 장치 리스트에 대한 노드.
<br>struct pci_bus *bus: 장치가 연결된 버스.
<br>struct pci_bus *subordinate: 브리지 장치가 연결된 하위 버스.
<br>struct pci_slot *slot: 장치가 물리적으로 장착된 슬롯.
<br><br>
<br>unsigned int devfn: 장치 및 함수 인덱스(장치 번호 및 함수 번호).
<br>unsigned short vendor: 벤더 ID.
<br>unsigned short device: 디바이스 ID.
<br>unsigned short subsystem_vendor: 서브시스템 벤더 ID.
<br>unsigned short subsystem_device: 서브시스템 디바이스 ID.
<br>unsigned int class: 장치 클래스(베이스 클래스, 서브 클래스, 프로그래밍 인터페이스).
<br>u8 revision: PCI 리비전 번호.
<br>u8 hdr_type: PCI 헤더 타입.
<br><br>
<br>u8 pcie_cap: PCIe capability 오프셋.
<br>u8 msi_cap: MSI capability 오프셋.
<br>u8 msix_cap: MSI-X capability 오프셋.
<br>u8 pcie_mpss: PCIe 최대 페이로드 크기 지원.
<br><br>
<br>struct pci_driver *driver: 장치에 바인딩된 드라이버.
<br>u64 dma_mask: DMA 주소 마스크.
<br>struct device_dma_parameters dma_parms: DMA 매개변수.
<br><br>
<br>pci_power_t current_state: 현재 운영 상태(D0-D3).
<br>u8 pm_cap: 전원 관리 capability 오프셋.
<br>unsigned int imm_ready: 즉시 준비 상태 지원 여부.
<br>unsigned int pme_support: PME#을 생성할 수 있는 상태 비트마스크.
<br>unsigned int pme_poll: PME 상태 비트를 폴링할지 여부.
<br>unsigned int d1_support: D1 저전력 상태 지원 여부.
<br>unsigned int d2_support: D2 저전력 상태 지원 여부.
<br>unsigned int no_d1d2: D1 및 D2 상태 금지 여부.
<br>unsigned int no_d3cold: D3cold 상태 금지 여부.
<br><br>
<br>struct resource resource[DEVICE_COUNT_RESOURCE]: I/O 및 메모리 영역 + 확장 ROM.
<br>unsigned int irq: 인터럽트 라인.
<br>struct device dev: 일반 장치 인터페이스.
<br>int cfg_size: 구성 공간 크기.
<br>atomic_t enable_cnt: pci_enable_device가 호출된 횟수.
<br><br>
<br>unsigned int transparent: 감산 디코드 브리지 여부.
<br>unsigned int io_window: 브리지가 I/O 윈도우를 가지고 있는지 여부.
<br>unsigned int pref_window: 브리지가 선호 메모리 윈도우를 가지고 있는지 여부.
<br>unsigned int is_busmaster: 버스 마스터인지 여부.
<br>unsigned int no_msi: MSI 사용 불가 여부.
<br>unsigned int msi_enabled: MSI 사용 여부.
<br>unsigned int msix_enabled: MSI-X 사용 여부.
<br><br>
<br>u16 ats_cap: ATS Capability 오프셋.
<br>u16 pasid_cap: PASID Capability 오프셋.
<br>unsigned int ats_enabled: ATS(Address Translation Service) 활성화 여부.
<br>unsigned int pasid_enabled: PASID(Process Address Space ID) 활성화 여부.
<br><br>
<br>u16 acs_cap: ACS(Access Control Services) Capability 오프셋.
<br>unsigned int pasid_no_tlp: PASID가 TLP(TLP Prefix) 없이 작동하는지 여부.
<br>unsigned int eetlp_prefix_path: E2E TLP Prefix 지원 여부.
<br>pci_channel_state_t error_state: 현재 연결 상태.
<br>u32 saved_config_space[16]: 절전 시 구성 공간 저장.
<br>struct hlist_head saved_cap_space: 저장된 capability 공간.
<br><br>
<br>void *sysdata: 시스템 특화 확장을 위한 훅.
<br>struct proc_dir_entry *procent: /proc/bus/pci의 장치 엔트리.
<br>struct device_node *of_node: 장치 트리 노드.
<br>struct fwnode_handle *fwnode: 펌웨어 장치 노드.
<br>int numa_node: NUMA 노드.
<br><br>
<br>const char *driver_override: 강제 매칭을 위한 드라이버 이름.
<br>unsigned long priv_flags: PCI 드라이버를 위한 개인 플래그.
<br>u8 reset_methods[PCI_NUM_RESET_METHODS]: PCI 리셋 메서드 우선순위.
<br><br>struct pci_dev는 리눅스 커널에서 PCI 장치를 나타내는 핵심 데이터 구조체로, PCI 장치의 다양한 속성, 상태, 버스 및 드라이버와의 연계, DMA 설정 등을 관리한다. 이 구조체를 통해 커널은 PCI 장치를 효율적으로 관리하고 제어할 수 있으며, PCI 장치의 특성과 요구에 맞게 다양한 기능을 제공할 수 있다.]]></description><link>encyclopedia-of-networksystem/struct/include-linux/pci_dev.html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Struct/include-linux/pci_dev.md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[sk_buff]]></title><description><![CDATA[ 
 <br>/**
 * DOC: Basic sk_buff geometry
 *
 * struct sk_buff itself is a metadata structure and does not hold any packet
 * data. All the data is held in associated buffers.
 *
 * &amp;sk_buff.head points to the main "head" buffer. The head buffer is divided
 * into two parts:
 *
 *  - data buffer, containing headers and sometimes payload;
 *    this is the part of the skb operated on by the common helpers
 *    such as skb_put() or skb_pull();
 *  - shared info (struct skb_shared_info) which holds an array of pointers
 *    to read-only data in the (page, offset, length) format.
 *
 * Optionally &amp;skb_shared_info.frag_list may point to another skb.
 *
 * Basic diagram may look like this::
 *
 *                                  ---------------
 *                                 | sk_buff       |
 *                                  ---------------
 *     ,---------------------------  + head
 *    /          ,-----------------  + data
 *   /          /      ,-----------  + tail
 *  |          |      |            , + end
 *  |          |      |           |
 *  v          v      v           v
 *   -----------------------------------------------
 *  | headroom | data |  tailroom | skb_shared_info |
 *   -----------------------------------------------
 *                                 + [page frag]
 *                                 + [page frag]
 *                                 + [page frag]
 *                                 + [page frag]       ---------
 *                                 + frag_list    --&gt; | sk_buff |
 *                                                     ---------
 *
 */
 
/**
 *	struct sk_buff - socket buffer
 *	@next: Next buffer in list
 *	@prev: Previous buffer in list
 *	@tstamp: Time we arrived/left
 *	@skb_mstamp_ns: (aka @tstamp) earliest departure time; start point
 *		for retransmit timer
 *	@rbnode: RB tree node, alternative to next/prev for netem/tcp
 *	@list: queue head
 *	@ll_node: anchor in an llist (eg socket defer_list)
 *	@sk: Socket we are owned by
 *	@dev: Device we arrived on/are leaving by
 *	@dev_scratch: (aka @dev) alternate use of @dev when @dev would be %NULL
 *	@cb: Control buffer. Free for use by every layer. Put private vars here
 *	@_skb_refdst: destination entry (with norefcount bit)
 *	@len: Length of actual data
 *	@data_len: Data length
 *	@mac_len: Length of link layer header
 *	@hdr_len: writable header length of cloned skb
 *	@csum: Checksum (must include start/offset pair)
 *	@csum_start: Offset from skb-&gt;head where checksumming should start
 *	@csum_offset: Offset from csum_start where checksum should be stored
 *	@priority: Packet queueing priority
 *	@ignore_df: allow local fragmentation
 *	@cloned: Head may be cloned (check refcnt to be sure)
 *	@ip_summed: Driver fed us an IP checksum
 *	@nohdr: Payload reference only, must not modify header
 *	@pkt_type: Packet class
 *	@fclone: skbuff clone status
 *	@ipvs_property: skbuff is owned by ipvs
 *	@inner_protocol_type: whether the inner protocol is
 *		ENCAP_TYPE_ETHER or ENCAP_TYPE_IPPROTO
 *	@remcsum_offload: remote checksum offload is enabled
 *	@offload_fwd_mark: Packet was L2-forwarded in hardware
 *	@offload_l3_fwd_mark: Packet was L3-forwarded in hardware
 *	@tc_skip_classify: do not classify packet. set by IFB device
 *	@tc_at_ingress: used within tc_classify to distinguish in/egress
 *	@redirected: packet was redirected by packet classifier
 *	@from_ingress: packet was redirected from the ingress path
 *	@nf_skip_egress: packet shall skip nf egress - see netfilter_netdev.h
 *	@peeked: this packet has been seen already, so stats have been
 *		done for it, don't do them again
 *	@nf_trace: netfilter packet trace flag
 *	@protocol: Packet protocol from driver
 *	@destructor: Destruct function
 *	@tcp_tsorted_anchor: list structure for TCP (tp-&gt;tsorted_sent_queue)
 *	@_sk_redir: socket redirection information for skmsg
 *	@_nfct: Associated connection, if any (with nfctinfo bits)
 *	@skb_iif: ifindex of device we arrived on
 *	@tc_index: Traffic control index
 *	@hash: the packet hash
 *	@queue_mapping: Queue mapping for multiqueue devices
 *	@head_frag: skb was allocated from page fragments,
 *		not allocated by kmalloc() or vmalloc().
 *	@pfmemalloc: skbuff was allocated from PFMEMALLOC reserves
 *	@pp_recycle: mark the packet for recycling instead of freeing (implies
 *		page_pool support on driver)
 *	@active_extensions: active extensions (skb_ext_id types)
 *	@ndisc_nodetype: router type (from link layer)
 *	@ooo_okay: allow the mapping of a socket to a queue to be changed
 *	@l4_hash: indicate hash is a canonical 4-tuple hash over transport
 *		ports.
 *	@sw_hash: indicates hash was computed in software stack
 *	@wifi_acked_valid: wifi_acked was set
 *	@wifi_acked: whether frame was acked on wifi or not
 *	@no_fcs:  Request NIC to treat last 4 bytes as Ethernet FCS
 *	@encapsulation: indicates the inner headers in the skbuff are valid
 *	@encap_hdr_csum: software checksum is needed
 *	@csum_valid: checksum is already valid
 *	@csum_not_inet: use CRC32c to resolve CHECKSUM_PARTIAL
 *	@csum_complete_sw: checksum was completed by software
 *	@csum_level: indicates the number of consecutive checksums found in
 *		the packet minus one that have been verified as
 *		CHECKSUM_UNNECESSARY (max 3)
 *	@dst_pending_confirm: need to confirm neighbour
 *	@decrypted: Decrypted SKB
 *	@slow_gro: state present at GRO time, slower prepare step required
 *	@mono_delivery_time: When set, skb-&gt;tstamp has the
 *		delivery_time in mono clock base (i.e. EDT).  Otherwise, the
 *		skb-&gt;tstamp has the (rcv) timestamp at ingress and
 *		delivery_time at egress.
 *	@napi_id: id of the NAPI struct this skb came from
 *	@sender_cpu: (aka @napi_id) source CPU in XPS
 *	@alloc_cpu: CPU which did the skb allocation.
 *	@secmark: security marking
 *	@mark: Generic packet mark
 *	@reserved_tailroom: (aka @mark) number of bytes of free space available
 *		at the tail of an sk_buff
 *	@vlan_all: vlan fields (proto &amp; tci)
 *	@vlan_proto: vlan encapsulation protocol
 *	@vlan_tci: vlan tag control information
 *	@inner_protocol: Protocol (encapsulation)
 *	@inner_ipproto: (aka @inner_protocol) stores ipproto when
 *		skb-&gt;inner_protocol_type == ENCAP_TYPE_IPPROTO;
 *	@inner_transport_header: Inner transport layer header (encapsulation)
 *	@inner_network_header: Network layer header (encapsulation)
 *	@inner_mac_header: Link layer header (encapsulation)
 *	@transport_header: Transport layer header
 *	@network_header: Network layer header
 *	@mac_header: Link layer header
 *	@kcov_handle: KCOV remote handle for remote coverage collection
 *	@tail: Tail pointer
 *	@end: End pointer
 *	@head: Head of buffer
 *	@data: Data head pointer
 *	@truesize: Buffer size
 *	@users: User count - see {datagram,tcp}.c
 *	@extensions: allocated extensions, valid if active_extensions is nonzero
 */
<br>struct sk_buff {
	union {
		struct {
			/* These two members must be first to match sk_buff_head. */
			struct sk_buff		*next;
			struct sk_buff		*prev;

			union {
				struct net_device	*dev;
				/* Some protocols might use this space to store information,
				 * while device pointer would be NULL.
				 * UDP receive path is one user.
				 */
				unsigned long		dev_scratch;
			};
		};
		struct rb_node		rbnode; /* used in netem, ip4 defrag, and tcp stack */
		struct list_head	list;
		struct llist_node	ll_node;
	};

	struct sock		*sk;

	union {
		ktime_t		tstamp;
		u64		skb_mstamp_ns; /* earliest departure time */
	};
	/*
	 * This is the control buffer. It is free to use for every
	 * layer. Please put your private variables there. If you
	 * want to keep them across layers you have to do a skb_clone()
	 * first. This is owned by whoever has the skb queued ATM.
	 */
	char			cb[48] __aligned(8);

	union {
		struct {
			unsigned long	_skb_refdst;
			void		(*destructor)(struct sk_buff *skb);
		};
		struct list_head	tcp_tsorted_anchor;
#ifdef CONFIG_NET_SOCK_MSG
		unsigned long		_sk_redir;
#endif
	};

#if defined(CONFIG_NF_CONNTRACK) || defined(CONFIG_NF_CONNTRACK_MODULE)
	unsigned long		 _nfct;
#endif
	unsigned int		len,
				data_len;
	__u16			mac_len,
				hdr_len;

	/* Following fields are _not_ copied in __copy_skb_header()
	 * Note that queue_mapping is here mostly to fill a hole.
	 */
	__u16			queue_mapping;

/* if you move cloned around you also must adapt those constants */
#ifdef __BIG_ENDIAN_BITFIELD
#define CLONED_MASK	(1 &lt;&lt; 7)
#else
#define CLONED_MASK	1
#endif
#define CLONED_OFFSET		offsetof(struct sk_buff, __cloned_offset)

	/* private: */
	__u8			__cloned_offset[0];
	/* public: */
	__u8			cloned:1,
				nohdr:1,
				fclone:2,
				peeked:1,
				head_frag:1,
				pfmemalloc:1,
				pp_recycle:1; /* page_pool recycle indicator */
#ifdef CONFIG_SKB_EXTENSIONS
	__u8			active_extensions;
#endif

	/* Fields enclosed in headers group are copied
	 * using a single memcpy() in __copy_skb_header()
	 */
	struct_group(headers,

	/* private: */
	__u8			__pkt_type_offset[0];
	/* public: */
	__u8			pkt_type:3; /* see PKT_TYPE_MAX */
	__u8			ignore_df:1;
	__u8			dst_pending_confirm:1;
	__u8			ip_summed:2;
	__u8			ooo_okay:1;

	/* private: */
	__u8			__mono_tc_offset[0];
	/* public: */
	__u8			mono_delivery_time:1;	/* See SKB_MONO_DELIVERY_TIME_MASK */
#ifdef CONFIG_NET_XGRESS
	__u8			tc_at_ingress:1;	/* See TC_AT_INGRESS_MASK */
	__u8			tc_skip_classify:1;
#endif
	__u8			remcsum_offload:1;
	__u8			csum_complete_sw:1;
	__u8			csum_level:2;
	__u8			inner_protocol_type:1;

	__u8			l4_hash:1;
	__u8			sw_hash:1;
#ifdef CONFIG_WIRELESS
	__u8			wifi_acked_valid:1;
	__u8			wifi_acked:1;
#endif
	__u8			no_fcs:1;
	/* Indicates the inner headers are valid in the skbuff. */
	__u8			encapsulation:1;
	__u8			encap_hdr_csum:1;
	__u8			csum_valid:1;
#ifdef CONFIG_IPV6_NDISC_NODETYPE
	__u8			ndisc_nodetype:2;
#endif

#if IS_ENABLED(CONFIG_IP_VS)
	__u8			ipvs_property:1;
#endif
#if IS_ENABLED(CONFIG_NETFILTER_XT_TARGET_TRACE) || IS_ENABLED(CONFIG_NF_TABLES)
	__u8			nf_trace:1;
#endif
#ifdef CONFIG_NET_SWITCHDEV
	__u8			offload_fwd_mark:1;
	__u8			offload_l3_fwd_mark:1;
#endif
	__u8			redirected:1;
#ifdef CONFIG_NET_REDIRECT
	__u8			from_ingress:1;
#endif
#ifdef CONFIG_NETFILTER_SKIP_EGRESS
	__u8			nf_skip_egress:1;
#endif
#ifdef CONFIG_TLS_DEVICE
	__u8			decrypted:1;
#endif
	__u8			slow_gro:1;
#if IS_ENABLED(CONFIG_IP_SCTP)
	__u8			csum_not_inet:1;
#endif

#if defined(CONFIG_NET_SCHED) || defined(CONFIG_NET_XGRESS)
	__u16			tc_index;	/* traffic control index */
#endif

	u16			alloc_cpu;

	union {
		__wsum		csum;
		struct {
			__u16	csum_start;
			__u16	csum_offset;
		};
	};
	__u32			priority;
	int			skb_iif;
	__u32			hash;
	union {
		u32		vlan_all;
		struct {
			__be16	vlan_proto;
			__u16	vlan_tci;
		};
	};
#if defined(CONFIG_NET_RX_BUSY_POLL) || defined(CONFIG_XPS)
	union {
		unsigned int	napi_id;
		unsigned int	sender_cpu;
	};
#endif
#ifdef CONFIG_NETWORK_SECMARK
	__u32		secmark;
#endif

	union {
		__u32		mark;
		__u32		reserved_tailroom;
	};

	union {
		__be16		inner_protocol;
		__u8		inner_ipproto;
	};

	__u16			inner_transport_header;
	__u16			inner_network_header;
	__u16			inner_mac_header;

	__be16			protocol;
	__u16			transport_header;
	__u16			network_header;
	__u16			mac_header;

#ifdef CONFIG_KCOV
	u64			kcov_handle;
#endif

	); /* end headers group */

	/* These elements must be at the end, see alloc_skb() for details.  */
	sk_buff_data_t		tail;
	sk_buff_data_t		end;
	unsigned char		*head,
				*data;
	unsigned int		truesize;
	refcount_t		users;

#ifdef CONFIG_SKB_EXTENSIONS
	/* only usable after checking -&gt;active_extensions != 0 */
	struct skb_ext		*extensions;
#endif
};
<br><a rel="noopener nofollow" class="external-link" href="https://pr0gr4m.github.io/linux/kernel/sk_buff/" target="_blank">https://pr0gr4m.github.io/linux/kernel/sk_buff/</a><br>
<br>unsigned char *data : 데이터의 head 포인터
<br>unsigned char *head : 버퍼의 head 포인터
<br>sk_buff_data_t tail : 데이터의 tail 포인터
<br>sk_buff_data_t end : 버퍼의 end 포인터로 tail이 end를 초과할 수 없다.
<br>struct net_device *dev : sk_buff와 연관된 네트워크 인터페이스 장치를 나타내는 net_device 객체이다.
<br>sturct sock *sk : sk_buff를 소유한 socket이다. 포워딩되는 패킷의 sk는 NULL이다.
<br>ktime_t tstamp : 패킷의 도착 타임스탬프. 다음과 같은 헬퍼 함수가 있다.

<br>ktime_t skb_get_ktime(const struct sk_buff *skb)&nbsp;: skb의 tstamp를 반환한다.
<br>void skb_get_timestamp(const struct sk_buff *skb, struct __kernel_old_timeval *stamp)&nbsp;: skb로부터 timestamp를 구하는 legacy 함수이다.
<br>void skb_get_new_timestamp(const struct sk_buff *skb, struct __kernel_sock_timeval *stamp)&nbsp;: struct timespec64 구조체에 skb의 timestamp를 저장한다.
<br>void skb_get_new_timestampns(const struct sk_buff *skb, struct __kernel_sock_timeval *stamp)&nbsp;: skb_get_new_timestamp의 nanosecond 버전
<br>void __net_timestamp(struct sk_buff *skb)&nbsp;: skb 구조체에 ktime_get_real() 함수의 반환 값으로 tstamp를 설정한다.


<br>char cb[48] : 여러 계층에서 자유롭게 사용할 수 있는 control buffer이다. 외부에 공개하지 않을 정보를 저장하는데 사용한다.
<br>unsigned long _skb_refdst : 목적지 항목(dst_entry)의 주소이다. dst_entry 구조체는 특정 목적지에 대한 라우팅 항목을 나타내며, 각 송수신 패킷은 라우팅 테이블에서 탐색을 수행한다. 다음과 같은 헬퍼 함수가 있다.

<br>void skb_dst_set(struct sk_buff *skb, struct dst_entry *dst)&nbsp;: skb에 dst를 설정한다. 이 때 참조를 dst에서 취하고 dst_release() 함수를 통해 release 한다고 가정한다.
<br>void skb_dst_set_noref(struct sk_buff *skb, struct dst_entry *dst)&nbsp;: skb에 dst를 설정한다. 이 때 dst에서 참조를 취하지 않는다고 가정한다. refdst_drop 함수에서 dst에 대해 dst_release() 함수를 호출하지 않을 것이다.


<br>void (*destructor) (struct sk_buff *skb) : kfree_skb() 함수를 호출하여 skb 객체를 해제할 때 호출되는 콜백 함수
<br>unsigned int len : 패킷 바이트의 전체 길이
<br>unsigned int data_len : 데이터의 길이
<br>__u16 mac_len : MAC 헤더의 길이
<br>__u16 hdr_len : clone된 skb의 writable한 헤더 길이
<br>__u8 pkt_type:3 : 패킷의 클래스다. 이더넷의 경우 헤더의 목적지 MAC 주소에 의해 좌우되며, 주로 eth_type_trans() 함수로 결정한다.

<br>본인 호스트 : PACKET_HOST
<br>브로드캐스트 : PACKET_BROADCAST
<br>멀티캐스트 : PACKET_MULTICAST
<br>다른 호스트 : PACKET_OTHERHOST
<br>루프백 : PAKCET_LOOPBACK


<br>__wsum csum : 체크섬
<br>__u32 priority : 패킷의 큐 우선순위.
<br>__u8 cloned:1 : 패킷이 clone 함수로 복제되면 해당 필드는 cloned 객체와 원본 객체 모두 1로 설정된다. 데이터 영역은 복제본과 원본 객체가 공유한다.
<br>__u8 peeked:1 : 이미 확인되어 통계 작업이 이루어졌는지에 대한 플래그. 설정되었다면 통계 작업을 다시 수행하지 않는다.
<br>__be16 protocol : 드라이버에 의해 설정된 패킷의 protocol. 이더넷이라면 eth_type_trans() 함수에 의해 rx 경로에서 ETH_P_IP로 초기화된다.
<br>__u32 hash : 패킷의 hash로 IP 헤더의 출발지, 목적지 주소와 전송 헤더의 포트에 따라 계산된다. SMP로 작동 시 동일 flow의 패킷이 같은 CPU에서 처리되는 것을 보장하는 데 사용하여 캐시 미스를 줄인다.`
<br>__u32 mark : skb를 객체 식별을 위한 mark이다. 예를 들어 iptables 명령으로 다음과 같이 설정할 수 있다.

<br>iptables -A PREROUTING -t mangle -i eth1 -j MARK --set-mark 0x1234&nbsp;: 탐색을 수행하기 전 eth1의 수신 트래픽을 대상으로 모든 skb 객체의 mark 필드에 0x1234를 할당한다.


<br>__u16 transport_header : transport layer (L4) 헤더이다. 헬퍼 함수는 다음과 같다.

<br>unsigned char *skb_transport_header(const struct sk_buff *skb)&nbsp;: skb의 transport header를 반환한다.
<br>bool skb_transport_header_was_set(const struct sk_buff *skb)&nbsp;: skb의 transport header가 세팅되어 있다면 1을 반환한다.


<br>__u16 network_header : network layer (L3) 헤더이다. 헬퍼 함수는 다음과 같다.

<br>unsigned char *skb_network_header(const struct sk_buff *skb)&nbsp;: skb의 network header를 반환한다.
<br>void skb_reset_network_header(struct sk_buff *skb)&nbsp;: skb의 network header를 다시 계산한다.
<br>void skb_set_network_header(strut sk_buff *skb, const int offset)&nbsp;: skb의 network header를 offset을 이용하여 세팅한다.


<br>__u16 mac_header : datalink layer (L2) 헤더이다. 헬퍼 함수는 다음과 같다.

<br>unsigned char *skb_mac_header(const struct sk_buff *skb)&nbsp;: skb의 datalink header를 반환한다.
<br>int skb_mac_header_was_set(const struct sk_buff *skb)&nbsp;: skb의 datalink header가 세팅되어 있다면 1을 반환한다.
<br>int skb_mac_offset(const struct sk_buff *skb)&nbsp;: skb의 datalink header의 offset을 반환한다.
<br>u32 skb_mac_header_len(const struct sk_buff *skb)&nbsp;: skb의 datalink header의 길이를 반환한다.


<br>unsigned int truesize : skb에 할당된 전체 메모리 크기
<br>refcount_t users : 해당 skb 객체의 참조 카운터이다. 1로 초기화되며, skb_get() 함수로 증가하고, kfree_skb() 함수나 consume_skb() 함수로 감소한다. 값이 0에 도달하면 skb 객체가 해제된다. 헬퍼 함수는 다음과 같다.

<br>struct sk_buff *skb_get(struct sk_buff *skb)&nbsp;: 참조 카운터를 1 증가시킨다.
<br>int skb_shared(const struct sk_buff *skb)&nbsp;: 참조 카운터가 1이 아니면 true를 반환한다.
<br>sk_buff *skb_share_check(struct sk_buff *skb, gfp_t pri)&nbsp;: 버퍼가 공유되지 않으면 원래 버퍼가 반환된다. 버퍼가 공유되면 버퍼는 복제되고, 이 전 복사본은 참조를 폐기하여 새로운 복제본은 단일 참조로 반환된다. 인터럽트 컨텍스트나 스핀락에서 호출되면 pri 매개변수는 GFP_ATOMIC이어야 한다.
<br>void consume_skb(struct sk_buff *skb)&nbsp;: kfree_skb 함수를 호출한다.
<br>bool skb_unref(struct sk_buff *skb)&nbsp;: skb의 레퍼런스 카운터를 1 감소하고, 감소한 카운터가 0이라면 true를 아니라면 false를 반환한다.
<br>void kfree_skb(struct sk_buff *skb)&nbsp;: skb_unref를 호출하여 레퍼런스 카운터를 감소시키고, 카운터가 0이라면 skb 객체를 해제한다.


]]></description><link>encyclopedia-of-networksystem/struct/include-linux/sk_buff.html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Struct/include-linux/sk_buff.md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[softnet_data]]></title><description><![CDATA[ 
 <br>struct softnet_data {
	struct list_head poll_list;
	struct sk_buff_head process_queue;
	  
	/* stats */
	unsigned int processed;
	unsigned int time_squeeze;
#ifdef CONFIG_RPS
	struct softnet_data *rps_ipi_list;
#endif
  
	bool in_net_rx_action;
	bool in_napi_threaded_poll;
  
#ifdef CONFIG_NET_FLOW_LIMIT
	struct sd_flow_limit __rcu *flow_limit;
#endif
	struct Qdisc *output_queue;
	struct Qdisc **output_queue_tailp;
	struct sk_buff *completion_queue;
#ifdef CONFIG_XFRM_OFFLOAD
	struct sk_buff_head xfrm_backlog;
#endif
	/* written and read only by owning cpu: */
	struct {
		u16 recursion;
		u8 more;
#ifdef CONFIG_NET_EGRESS
		u8 skip_txqueue;
#endif
	} xmit;
#ifdef CONFIG_RPS
	/* input_queue_head should be written by cpu owning this struct,
	* and only read by other cpus. Worth using a cache line.
	*/
	unsigned int input_queue_head ____cacheline_aligned_in_smp;
	  
	/* Elements below can be accessed between CPUs for RPS/RFS */
	call_single_data_t csd ____cacheline_aligned_in_smp;
	struct softnet_data *rps_ipi_next;
	unsigned int cpu;
	unsigned int input_queue_tail;
#endif
	unsigned int received_rps;
	unsigned int dropped;
	struct sk_buff_head input_pkt_queue;
	struct napi_struct backlog;
	  
	/* Another possibly contended cache line */
	spinlock_t defer_lock ____cacheline_aligned_in_smp;
	int defer_count;
	int defer_ipi_scheduled;
	struct sk_buff *defer_list;
	call_single_data_t defer_csd;
};
<br>
설명으로 Incoming packets are placed on per-CPU queues라고 써져 있다.<br>
여기서 말하는 per-CPU queues가 softnet_data의 poll_list이다.
<br>/*
* Device drivers call our routines to queue packets here. We empty the
* queue in the local softnet handler.
*/
  
DEFINE_PER_CPU_ALIGNED(struct softnet_data, softnet_data);
EXPORT_PER_CPU_SYMBOL(softnet_data);
<br>
위의 코드는 /net/core/dev.c의 420번째 줄이다. 여기서 각 cpu마다 해당 타입의 변수를 선언하게 되며, 심볼 또한 전역적으로 쓸 수 있도록 Export하게 된다.<br>
<a data-tooltip-position="top" aria-label="Excalidraw/RPS-IPI 관계도.md > ^JGjyIN93" data-href="Excalidraw/RPS-IPI 관계도.md#^JGjyIN93" href="excalidraw/rps-ipi-관계도.html#^JGjyIN93" class="internal-link" target="_self" rel="noopener nofollow">softnet_data 도식화</a>
<br>
다음은 net_dev_init()함수이다. 이 함수는 OS 부팅시에 실행 되며, 앞서 NET_RX_SOFTIRQ enum값이 net_rx_action()함수와 매핑되는 함수이기도 하다.
<br>	/*
	* Initialise the packet receive queues.
	*/
	  
	for_each_possible_cpu(i) {
		struct work_struct *flush = per_cpu_ptr(&amp;flush_works, i);
		struct softnet_data *sd = &amp;per_cpu(softnet_data, i);
		  
		INIT_WORK(flush, flush_backlog);
		  
		skb_queue_head_init(&amp;sd-&gt;input_pkt_queue);
		skb_queue_head_init(&amp;sd-&gt;process_queue);
#ifdef CONFIG_XFRM_OFFLOAD
		skb_queue_head_init(&amp;sd-&gt;xfrm_backlog);
#endif
		INIT_LIST_HEAD(&amp;sd-&gt;poll_list);
		sd-&gt;output_queue_tailp = &amp;sd-&gt;output_queue;
#ifdef CONFIG_RPS
		INIT_CSD(&amp;sd-&gt;csd, rps_trigger_softirq, sd);
		sd-&gt;cpu = i;
#endif
		INIT_CSD(&amp;sd-&gt;defer_csd, trigger_rx_softirq, sd);
		spin_lock_init(&amp;sd-&gt;defer_lock);
		  
		init_gro_hash(&amp;sd-&gt;backlog);
		sd-&gt;backlog.poll = process_backlog;
		sd-&gt;backlog.weight = weight_p;
		  
		if (net_page_pool_create(i))
			goto out;
	}
	  
dev_boot_phase = 0;
<br>
위는 net_dev_init()함수의 중간부분이다. 여기서 softnet_data가 초기화되는 것을 알 수 있다. softnet_data의 input_pkt_queue, process_queue를 skb_queue_head타입의 리스트로써 prev, next를 우선 자신을 가르키도록 초기화해준다. 또한, poll_list를 list_head타입으로써 초기화 하게 된다.
그리고 RPS가 설정되어 있다면, rps_trigger_softirq함수를 csd의 func멤버에 매핑하여, 만약 다른 CPU를 IPI를 통해 인터럽트를 일으켰을 때 실행할 hardware IRQ에 사용될 함수를 설정하게 된다.
마지막으로 napi를 초기화하게 되는데, 여기서 이름은 backlog이다. poll함수를 process_backlog함수로 매핑하게 되고, weight은 한 번에 폴링 할 개수가 된다.

process_backlog함수를 간단하게 살펴보면, 우선 net_rx_action()이 끝나는 것을 기다릴 필요 없이 해당 CPU에 ipi_list, 즉 ipi를 할 리스트가 존재한다면, net_rps_action_and_irq_enale()함수를 바로 실행시키게 되고, 이후 process_queue에 담겨있는 skb를 하나씩 꺼내서 __netif_receive_skb()함수를 통해 네트워크 스택으로 올려주게 된다. 여기서 특징은 복잡한 RPS, GRO 등의 과정을 전혀 거치지 않는다는 점이다.<br>
물론 첫 번째 실행에서는 process_queue가 비어있을 것이다. enqueue_to_backlog에서는 input_pkt_queue에 삽입하기 때문이다.<br>
위의 상위 스택으로 넘기는 과정 이후에 input_pkt_queue가 비어있는지 검사하게 되는데, 비워질 때 까지 skb_queue_splice_tail_init()함수를 통해 해당 큐를 process_queue에 옮기는 과정이 반복된다. 따라서 input_pkt_queue가 처리되게 된다. 

]]></description><link>encyclopedia-of-networksystem/struct/include-linux/softnet_data.html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Struct/include-linux/softnet_data.md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[napi_gro_cb]]></title><description><![CDATA[ 
 <br>struct napi_gro_cb {
	union {
		struct {
			/* Virtual address of skb_shinfo(skb)-&gt;frags[0].page + offset. */
			void	*frag0;

			/* Length of frag0. */
			unsigned int frag0_len;
		};

		struct {
			/* used in skb_gro_receive() slow path */
			struct sk_buff *last;

			/* jiffies when first packet was created/queued */
			unsigned long age;
		};
	};

	/* This indicates where we are processing relative to skb-&gt;data. */
	int	data_offset;

	/* This is non-zero if the packet cannot be merged with the new skb. */
	u16	flush;

	/* Save the IP ID here and check when we get to the transport layer */
	u16	flush_id;

	/* Number of segments aggregated. */
	u16	count;

	/* Used in ipv6_gro_receive() and foo-over-udp and esp-in-udp */
	u16	proto;

/* Used in napi_gro_cb::free */
#define NAPI_GRO_FREE             1
#define NAPI_GRO_FREE_STOLEN_HEAD 2
	/* portion of the cb set to zero at every gro iteration */
	struct_group(zeroed,

		/* Start offset for remote checksum offload */
		u16	gro_remcsum_start;

		/* This is non-zero if the packet may be of the same flow. */
		u8	same_flow:1;

		/* Used in tunnel GRO receive */
		u8	encap_mark:1;

		/* GRO checksum is valid */
		u8	csum_valid:1;

		/* Number of checksums via CHECKSUM_UNNECESSARY */
		u8	csum_cnt:3;

		/* Free the skb? */
		u8	free:2;

		/* Used in foo-over-udp, set in udp[46]_gro_receive */
		u8	is_ipv6:1;

		/* Used in GRE, set in fou/gue_gro_receive */
		u8	is_fou:1;

		/* Used to determine if flush_id can be ignored */
		u8	is_atomic:1;

		/* Number of gro_receive callbacks this packet already went through */
		u8 recursion_counter:4;

		/* GRO is done by frag_list pointer chaining. */
		u8	is_flist:1;
	);

	/* used to support CHECKSUM_COMPLETE for tunneling protocols */
	__wsum	csum;

	/* L3 offsets */
	union {
		struct {
			u16 network_offset;
			u16 inner_network_offset;
		};
		u16 network_offsets[2];
	};
};
<br>
다해서 48byte 크기의 구조체가 됨. (8byte long, pointer 기준)
<br>frag0 관련 필드<br>
<br>frag0: skb_shinfo(skb)-&gt;frags[0].page + offset의 가상 주소입니다. 이는 첫 번째 조각(fragment)의 시작 주소를 가리킨다.
<br>frag0_len: 첫 번째 조각의 길이이다.
<br>last와 age<br>
<br>last: 느린 경로에서 마지막 skb를 가리킨다.
<br>age: 첫 번째 패킷이 생성되거나 큐잉된 시점의 jiffies 값이다.
<br>데이터 오프셋과 플래그<br>
<br>data_offset: 현재 데이터 처리 위치를 나타낸다. 이는 skb-&gt;data와의 상대적 위치이다.
<br>flush: 패킷이 새로운 skb와 병합될 수 없는 경우 0이 아닌 값을 가진다.
<br>flush_id: IP ID 값을 저장하고, 트랜스포트 레이어에서 확인할 때 사용한다.
<br>count: 병합된 세그먼트의 수를 나타낸다.
<br>proto: 사용된 프로토콜을 나타낸다. 예를 들어, IPv6, foo-over-udp, esp-in-udp 등이 있다.
<br>zeroed 그룹<br>
<br>gro_remcsum_start: 원격 체크섬 오프로드의 시작 오프셋이다.
<br>same_flow: 동일한 흐름인지 여부를 나타낸다.
<br>encap_mark: 터널링 GRO 수신에 사용된다.
<br>csum_valid: GRO 체크섬이 유효한지 여부를 나타낸다.
<br>csum_cnt: CHECKSUM_UNNECESSARY를 통해 검증된 체크섬의 수이다.
<br>free: skb를 해제할지 여부를 나타낸다.
<br>is_ipv6: IPv6를 사용 중인지 여부를 나타낸다.
<br>is_fou: GRE에서 사용되며, fou/gue_gro_receive에서 설정된다.
<br>is_atomic: flush_id를 무시할 수 있는지 여부를 나타낸다.
<br>recursion_counter: 이미 실행된 gro_receive 콜백의 수이다.
<br>is_flist: frag_list 포인터 체이닝을 통해 GRO가 수행되는지 여부를 나타낸다.
<br>기타 필드<br>
<br>csum: 터널링 프로토콜을 위한 CHECKSUM_COMPLETE를 지원하는 데 사용된다.
<br>network_offsets: 네트워크 오프셋(내부 및 외부)을 나타낸다.
<br>이 구조체는 주로 네트워크 패킷의 병합과 관련된 다양한 상태와 데이터를 관리하는 데 사용되며, 네트워크 성능 최적화에 중요한 역할을 한다. 이를 통해 효율적인 네트워크 패킷 처리를 지원한다.]]></description><link>encyclopedia-of-networksystem/struct/include-net/napi_gro_cb.html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Struct/include-net/napi_gro_cb.md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[netdev_rx_queue]]></title><description><![CDATA[ 
 <br>/* This structure contains an instance of an RX queue. */
struct netdev_rx_queue {
	struct xdp_rxq_info		xdp_rxq;
#ifdef CONFIG_RPS
	struct rps_map __rcu		*rps_map;
	struct rps_dev_flow_table __rcu	*rps_flow_table;
#endif
	struct kobject			kobj;
	struct net_device		*dev;
	netdevice_tracker		dev_tracker;

#ifdef CONFIG_XDP_SOCKETS
	struct xsk_buff_pool            *pool;
#endif
	/* NAPI instance for the queue
	 * Readers and writers must hold RTNL
	 */
	struct napi_struct		*napi;
} ____cacheline_aligned_in_smp;
]]></description><link>encyclopedia-of-networksystem/struct/include-net/netdev_rx_queue.html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Struct/include-net/netdev_rx_queue.md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[rps_dev_flow_table]]></title><description><![CDATA[ 
 <br> * The rps_dev_flow_table structure contains a table of flow mappings.
 */
struct rps_dev_flow_table {
	unsigned int		mask;
	struct rcu_head		rcu;
	struct rps_dev_flow	flows[];
};
]]></description><link>encyclopedia-of-networksystem/struct/include-net/rps_dev_flow_table.html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Struct/include-net/rps_dev_flow_table.md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[rps_map]]></title><description><![CDATA[ 
 <br> * This structure holds an RPS map which can be of variable length.  The
 * map is an array of CPUs.
 */
struct rps_map {
	unsigned int	len;
	struct rcu_head	rcu;
	u16		cpus[];
};
]]></description><link>encyclopedia-of-networksystem/struct/include-net/rps_map.html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Struct/include-net/rps_map.md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[sock]]></title><description><![CDATA[ 
 <br>struct sock {
	/*
	* Now struct inet_timewait_sock also uses sock_common, so please just
	* don't add nothing before this first member (__sk_common) --acme
	*/
	struct sock_common __sk_common;
#define sk_node __sk_common.skc_node
#define sk_nulls_node __sk_common.skc_nulls_node
#define sk_refcnt __sk_common.skc_refcnt
#define sk_tx_queue_mapping __sk_common.skc_tx_queue_mapping
#ifdef CONFIG_SOCK_RX_QUEUE_MAPPING
#define sk_rx_queue_mapping __sk_common.skc_rx_queue_mapping
#endif
  
#define sk_dontcopy_begin __sk_common.skc_dontcopy_begin
#define sk_dontcopy_end __sk_common.skc_dontcopy_end
#define sk_hash __sk_common.skc_hash
#define sk_portpair __sk_common.skc_portpair
#define sk_num __sk_common.skc_num
#define sk_dport __sk_common.skc_dport
#define sk_addrpair __sk_common.skc_addrpair
#define sk_daddr __sk_common.skc_daddr
#define sk_rcv_saddr __sk_common.skc_rcv_saddr
#define sk_family __sk_common.skc_family
#define sk_state __sk_common.skc_state
#define sk_reuse __sk_common.skc_reuse
#define sk_reuseport __sk_common.skc_reuseport
#define sk_ipv6only __sk_common.skc_ipv6only
#define sk_net_refcnt __sk_common.skc_net_refcnt
#define sk_bound_dev_if __sk_common.skc_bound_dev_if
#define sk_bind_node __sk_common.skc_bind_node
#define sk_prot __sk_common.skc_prot
#define sk_net __sk_common.skc_net
#define sk_v6_daddr __sk_common.skc_v6_daddr
#define sk_v6_rcv_saddr __sk_common.skc_v6_rcv_saddr
#define sk_cookie __sk_common.skc_cookie
#define sk_incoming_cpu __sk_common.skc_incoming_cpu
#define sk_flags __sk_common.skc_flags
#define sk_rxhash __sk_common.skc_rxhash
	  
	__cacheline_group_begin(sock_write_rx);
	  
	atomic_t sk_drops;
	__s32 sk_peek_off;
	struct sk_buff_head sk_error_queue;
	struct sk_buff_head sk_receive_queue;
	/*
	* The backlog queue is special, it is always used with
	* the per-socket spinlock held and requires low latency
	* access. Therefore we special case it's implementation.
	* Note : rmem_alloc is in this structure to fill a hole
	* on 64bit arches, not because its logically part of
	* backlog.
	*/
	struct {
		atomic_t rmem_alloc;
		int len;
		struct sk_buff *head;
		struct sk_buff *tail;
	} sk_backlog;
#define sk_rmem_alloc sk_backlog.rmem_alloc
  
	__cacheline_group_end(sock_write_rx);
	
	__cacheline_group_begin(sock_read_rx);
	/* early demux fields */
	struct dst_entry __rcu *sk_rx_dst;
	int         sk_rx_dst_ifindex;
	u32         sk_rx_dst_cookie;
	  
#ifdef CONFIG_NET_RX_BUSY_POLL
	unsigned int sk_ll_usec;
	unsigned int sk_napi_id;
	u16 sk_busy_poll_budget;
	u8 sk_prefer_busy_poll;
#endif
	u8 sk_userlocks;
	int sk_rcvbuf;
  
	struct sk_filter __rcu *sk_filter;
	union {
		struct socket_wq __rcu *sk_wq;
		/* private: */
		struct socket_wq *sk_wq_raw;
		/* public: */
	};
  
	void (*sk_data_ready)(struct sock *sk);
	long sk_rcvtimeo;
	int sk_rcvlowat;
	__cacheline_group_end(sock_read_rx);
	  
	__cacheline_group_begin(sock_read_rxtx);
	int sk_err;
	struct socket *sk_socket;
	struct mem_cgroup *sk_memcg;
#ifdef CONFIG_XFRM
	struct xfrm_policy __rcu *sk_policy[2];
#endif
	__cacheline_group_end(sock_read_rxtx);
	  
	__cacheline_group_begin(sock_write_rxtx);
	socket_lock_t sk_lock;
	u32 sk_reserved_mem;
	int sk_forward_alloc;
	u32 sk_tsflags;
	__cacheline_group_end(sock_write_rxtx);
	  
	__cacheline_group_begin(sock_write_tx);
	int sk_write_pending;
	atomic_t sk_omem_alloc;
	int sk_sndbuf;
	  
	int sk_wmem_queued;
	refcount_t sk_wmem_alloc;
	unsigned long sk_tsq_flags;
	union {
		struct sk_buff *sk_send_head;
		struct rb_root tcp_rtx_queue;
	};
	struct sk_buff_head sk_write_queue;
	u32 sk_dst_pending_confirm;
	u32 sk_pacing_status; /* see enum sk_pacing */
	struct page_frag sk_frag;
	struct timer_list sk_timer;
	  
	unsigned long sk_pacing_rate; /* bytes per second */
	atomic_t sk_zckey;
	atomic_t sk_tskey;
	__cacheline_group_end(sock_write_tx);
	  
	__cacheline_group_begin(sock_read_tx);
	unsigned long sk_max_pacing_rate;
	long sk_sndtimeo;
	u32 sk_priority;
	u32 sk_mark;
	struct dst_entry __rcu *sk_dst_cache;
	netdev_features_t sk_route_caps;
#ifdef CONFIG_SOCK_VALIDATE_XMIT
	struct sk_buff* (*sk_validate_xmit_skb)(struct sock *sk,
							struct net_device *dev,
							struct sk_buff *skb);
#endif
u16 sk_gso_type;
u16 sk_gso_max_segs;
unsigned int sk_gso_max_size;
gfp_t sk_allocation;
u32 sk_txhash;
u8 sk_pacing_shift;
bool sk_use_task_frag;
__cacheline_group_end(sock_read_tx);
  
/*
* Because of non atomicity rules, all
* changes are protected by socket lock.
*/
	u8 sk_gso_disabled : 1,
	sk_kern_sock : 1,
	sk_no_check_tx : 1,
	sk_no_check_rx : 1;
	u8 sk_shutdown;
	u16 sk_type;
	u16 sk_protocol;
	unsigned long sk_lingertime;
	struct proto *sk_prot_creator;
	rwlock_t sk_callback_lock;
	int sk_err_soft;
	u32 sk_ack_backlog;
	u32 sk_max_ack_backlog;
	kuid_t sk_uid;
	spinlock_t sk_peer_lock;
	int sk_bind_phc;
	struct pid *sk_peer_pid;
	const struct cred *sk_peer_cred;
	  
	ktime_t sk_stamp;
#if BITS_PER_LONG==32
	seqlock_t sk_stamp_seq;
#endif
	int sk_disconnects;
	  
	u8 sk_txrehash;
	u8 sk_clockid;
	u8 sk_txtime_deadline_mode : 1,
	sk_txtime_report_errors : 1,
	sk_txtime_unused : 6;
	  
	void *sk_user_data;
#ifdef CONFIG_SECURITY
	void *sk_security;
#endif
	struct sock_cgroup_data sk_cgrp_data;
	void (*sk_state_change)(struct sock *sk);
	void (*sk_write_space)(struct sock *sk);
	void (*sk_error_report)(struct sock *sk);
	int (*sk_backlog_rcv)(struct sock *sk,
							struct sk_buff *skb);
	void (*sk_destruct)(struct sock *sk);
	struct sock_reuseport __rcu *sk_reuseport_cb;
#ifdef CONFIG_BPF_SYSCALL
	struct bpf_local_storage __rcu *sk_bpf_storage;
#endif
	struct rcu_head sk_rcu;
	netns_tracker ns_tracker;
};
]]></description><link>encyclopedia-of-networksystem/struct/include-net/sock.html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Struct/include-net/sock.md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[net_hotdata]]></title><description><![CDATA[ 
 <br>struct net_hotdata net_hotdata __cacheline_aligned = {

.offload_base = LIST_HEAD_INIT(net_hotdata.offload_base),

.ptype_all = LIST_HEAD_INIT(net_hotdata.ptype_all),

.gro_normal_batch = 8,

  

.netdev_budget = 300,

/* Must be at least 2 jiffes to guarantee 1 jiffy timeout */

.netdev_budget_usecs = 2 * USEC_PER_SEC / HZ,

  

.tstamp_prequeue = 1,

.max_backlog = 1000,

.dev_tx_weight = 64,

.dev_rx_weight = 64,

};

EXPORT_SYMBOL(net_hotdata);
<br>
net_hotdata를 초기화하는 코드이다. 여기서 각종 설정 값들이 적용되게 된다. gro_normal_batch값을 따라가다보니 닿게 되었다. 여기서는 8로 셋팅되어 있다.
]]></description><link>encyclopedia-of-networksystem/struct/net-core/net_hotdata.html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Struct/net-core/net_hotdata.md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[net-core]]></title><description><![CDATA[ 
 ]]></description><link>encyclopedia-of-networksystem/struct/net-core/net-core.html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/Struct/net-core/net-core.md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item><item><title><![CDATA[README]]></title><description><![CDATA[ 
 <br>현재 NetworkSystemEncyclopedia 폴더로 노션에 있던 기존 자료들을 옮기는 작업 중에 있습니다. 이로 인해 폴더 내에 많은 내용들이 삭제되거나 수정될 수 있기 때문에, 작업이 끝나기 전까지는 다른 폴더에다가 저장해주시면 감사하겠습니다! ㅎㅎ]]></description><link>encyclopedia-of-networksystem/readme.html</link><guid isPermaLink="false">Encyclopedia of NetworkSystem/README.md</guid><pubDate>Thu, 12 Sep 2024 03:26:43 GMT</pubDate></item></channel></rss>